"""batch_prompting.py

Generates and queries LLM apis for batch prompting jobs based on grammar evaluation.
"""

import dataclasses
import datetime as dt
import json
import logging
import pathlib
from typing import Any

import dotenv
import fire
import openai
import pandas as pd
import pyrootutils

import formal_gym.grammar as fg_grammar
import formal_gym.utils.utils as fg_utils

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%Y-%d-%m %H:%M:%S",
    level=logging.INFO,
)

log = fg_utils.get_logger(__name__)

PROJECT_ROOT = path = pyrootutils.find_root(
    search_from=__file__, indicator=".project-root"
)

dotenv.load_dotenv(PROJECT_ROOT / ".env")


@dataclasses.dataclass(frozen=True)
class ChatCompletionResponse:
    user_prompt: str
    metadata: dict[str, Any]
    system_prompt: str = "You are a helpful assistant."
    max_tokens: int = 1024

    # TODO: validate this
    def to_anthropic_json(self, model: str) -> str:
        return json.dumps(
            {
                "method": "POST",
                "url": "/v1/messages",
                "body": {
                    "model": model,
                    "messages": [{"role": "user", "content": self.user_prompt}],
                    "max_tokens": self.max_tokens,
                },
            }
        )

    # TODO: validate this
    def to_openai_json(self, model: str) -> str:
        return json.dumps(
            {
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": self.user_prompt},
                    ],
                    "max_tokens": self.max_tokens,
                },
            }
        )

    def to_openai_batched_json(self, model: str, custom_id: str) -> str:
        return json.dumps(
            {
                "custom_id": custom_id,
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": self.user_prompt},
                    ],
                    "max_tokens": self.max_tokens,
                    "metadata": self.metadata,
                    "store": True if self.metadata else False,
                },
            }
        )


def create_prompt(grammar_str: str, sample: str) -> str:
    """Creates a prompt for the LLM based on the grammar and sample."""

    return f"""You will be presented with a context-free grammar in Chomsky normal form and a string which may or may not be in the language defined by the given grammar. Your job is to determine whether or not the grammar generates the provided string. You can use any reasoning strategy you like, but you must end your response with either 'Yes' (if the string is generated by the grammar) or 'No' (if it isn't.)\n\nGrammar: ```{grammar_str}```\n\nString: `{sample}`.\n\nRemember, end your response with either 'Yes' or 'No'."""  # noqa: E501


def generate_batch_jsonl(
    grammar_file: pathlib.Path | str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data",
    n_samples: int = 1000,
    save_sample_files: bool = False,
    model: str = "gpt-4o-mini",
):
    if isinstance(grammar_file, str):
        grammar_file = pathlib.Path(grammar_file)

    grammar_file_name = grammar_file.stem
    grammar_path = data_path / grammar_file
    pos_sample_name = f"{grammar_file_name}_positive_{n_samples}.txt"
    neg_sample_name = f"{grammar_file_name}_negative_{n_samples}.txt"
    pos_sample_path = data_path / pos_sample_name
    neg_sample_path = data_path / neg_sample_name

    g = fg_grammar.Grammar.from_grammar(grammar_path)

    log.info(f"Generating {n_samples} positive samples from {grammar_file}")
    pos_samples = list(g.generate(n_samples=n_samples, sep=" "))
    if save_sample_files or not pos_sample_path.is_file():
        log.info(f"Writing positive examples to {pos_sample_path}")
        with open(pos_sample_path, "w") as f:
            for gen in pos_samples:
                f.write(f"{gen}\n")

    log.info(f"Generating {n_samples} negative samples from {grammar_file}")
    neg_samples = list(
        g.generate_negative_samples_matching_lengths(
            positive_sample_path=pos_sample_path,
        )
    )
    if save_sample_files or not neg_sample_path.is_file():
        log.info(f"Writing negative examples to {neg_sample_path}")
        with open(neg_sample_path, "w") as f:
            for gen in neg_samples:
                f.write(f"{gen}\n")

    pos_df = pd.DataFrame(pos_samples, columns=["sample"])
    neg_df = pd.DataFrame(neg_samples, columns=["sample"])

    pos_df["sample_type"] = "positive"
    neg_df["sample_type"] = "negative"

    with open(grammar_path, "r") as f:
        grammar_str = f.read()

    sample_df = pd.concat([pos_df, neg_df], ignore_index=True)

    sample_df["prompt"] = sample_df["sample"].apply(
        lambda s: create_prompt(grammar_str=grammar_str, sample=s)
    )

    sample_df[f"{model}_batched_json"] = sample_df.apply(
        lambda row: ChatCompletionResponse(
            user_prompt=row["prompt"],
            metadata={
                "sample_type": row["sample_type"],
                "sample": row["sample"],
                "grammar_file": grammar_file_name,
                "model": model,
            },
        ).to_openai_batched_json(model=model, custom_id=f"request-{row.name}"),
        axis=1,
    )

    batch_jsonl_filename = f"{grammar_file_name}_{model}_batched_{len(sample_df)}.jsonl"
    batch_jsonl_path = data_path / batch_jsonl_filename
    log.info(f"Writing batch job to {batch_jsonl_path}")
    with open(batch_jsonl_path, "w") as f:
        for j in sample_df[f"{model}_batched_json"]:
            f.write(f"{j}\n")


def load_prompt_tsv(
    tsv_file: pathlib.Path | str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data",
):
    if isinstance(tsv_file, str):
        tsv_file = pathlib.Path(tsv_file)

    tst_path = data_path / tsv_file

    log.info(f"Loading prompts from {tst_path}")
    df = pd.read_csv(tst_path, sep="\t")
    return df


def upload_batch_job(
    batch_file: pathlib.Path | str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data",
):
    if isinstance(batch_file, str):
        batch_file = pathlib.Path(batch_file)

    batch_jsonl_path = data_path / batch_file
    log.info(f"Uploading batch job from {batch_jsonl_path}")

    client = openai.OpenAI()
    batch_input_file = client.files.create(
        file=open(batch_jsonl_path, "rb"),
        purpose="batch",
    )

    log.info(f"Batch input file created: {batch_input_file}")

    batch_obj = client.batches.create(
        input_file_id=batch_input_file.id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
        metadata={"description": "Batch job for grammar evaluation."},
    )

    log.info(f"Batch job created: {batch_obj}")

    print(batch_input_file)

    # write info to log file in `logs`
    timestamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = PROJECT_ROOT / "logs" / f"batch_job_{timestamp}.log"
    with open(log_file_path, "w") as f:
        f.write(f"{batch_input_file}\n\n")
        f.write(f"{batch_obj}")


def get_batch_status(batch_id: str):
    client = openai.OpenAI()
    batch = client.batches.retrieve(batch_id)
    log.info(batch)


def get_batch_error(batch_id: str):
    client = openai.OpenAI()
    batch = client.batches.retrieve(batch_id)
    error_file = client.files.content(batch.error_file_id)
    print(error_file.text)


def get_batch_results(
    batch_id: str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data" / "completions",
):
    client = openai.OpenAI()
    batch_results = client.batches.retrieve(batch_id)
    log.info(batch_results)

    if batch_results.status == "completed":
        log.info("Batch job succeeded.")

        input_file = client.files.content(batch_results.input_file_id)
        input_path = data_path / f"{batch_id}_inputs.jsonl"
        output_file = client.files.content(batch_results.output_file_id)
        output_path = data_path / f"{batch_id}_results.jsonl"

        # write results to file
        log.info(f"Writing batch inputs to {input_path}")
        with open(input_path, "w") as f:
            f.write(input_file.text)
        log.info(f"Writing batch results to {output_path}")
        with open(output_path, "w") as f:
            f.write(output_file.text)
    else:
        log.warning("Batch job not completed yet.")


if __name__ == "__main__":
    fire.Fire()
