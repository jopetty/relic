"""batch_prompting.py

Generates and queries LLM apis for batch prompting jobs based on grammar evaluation.
"""

import logging
import pathlib

import dotenv
import formal_gym.grammar as fg_grammar
import formal_gym.utils.utils as fg_utils
import openai
import pandas as pd
import pyrootutils

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%Y-%d-%m %H:%M:%S",
    level=logging.INFO,
)

log = fg_utils.get_logger(__name__)

PROJECT_ROOT = path = pyrootutils.find_root(
    search_from=__file__, indicator=".project-root"
)

# load dotenv
dotenv.load_dotenv(PROJECT_ROOT / ".env")


def generate_batch_job(
    grammar_file: pathlib.Path | str,
    n_samples: int = 300,
):
    if isinstance(grammar_file, str):
        grammar_file = pathlib.Path(grammar_file)

    grammar_file_name = grammar_file.stem
    grammar_file_parent = grammar_file.parent
    pos_sample_name = f"{grammar_file_name}_positive_{n_samples}.txt"
    neg_sample_name = f"{grammar_file_name}_negative_{n_samples}.txt"
    pos_sample_path = grammar_file_parent / pos_sample_name
    neg_sample_path = grammar_file_parent / neg_sample_name

    log.info(f"Generating {n_samples} samples from {grammar_file}")

    g = fg_grammar.Grammar.from_grammar(grammar_file)
    pos_samples = list(g.generate(n_samples=n_samples, sep=" "))

    log.info(f"Writing positive examples to {pos_sample_path}")
    with open(pos_sample_path, "w") as f:
        for gen in pos_samples:
            f.write(f"{gen}\n")

    log.info(f"Generating {n_samples} negative samples from {grammar_file}")
    neg_samples = list(
        g.generate_negative_samples_matching_lengths(
            positive_sample_path=pos_sample_path,
        )
    )

    log.info(f"Writing negative examples to {neg_sample_path}")
    with open(neg_sample_path, "w") as f:
        for gen in neg_samples:
            f.write(f"{gen}\n")

    pos_df = pd.DataFrame(pos_samples, columns=["sample"])
    neg_df = pd.DataFrame(neg_samples, columns=["sample"])

    pos_df["sample_type"] = "positive"
    neg_df["sample_type"] = "negative"

    with open(grammar_file, "r") as f:
        grammar_str = f.read()

    prompt = """You will be presented with a context-free grammar in Chomsky normal form and a string which may or may not be in the language defined by the given grammar. Your job is to determine whether or not the grammar generates the provided string. You can use any reasoning strategy you like, but you must end your response with either 'Yes' (if the string is generated by the grammar) or 'No' (if it isn't.)

Grammar: ```
{grammar_str}
```

String: `{sample}`
"""  # noqa: E501

    sample_df = pd.concat([pos_df, neg_df], ignore_index=True)

    sample_df["prompt"] = sample_df.apply(
        lambda row: prompt.format(grammar_str=grammar_str, sample=row["sample"]),
        axis=1,
    )

    batch_jsonl_prefix = '''{"custom_id": "'''

    batch_jsonl_infix = '''", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o-mini", "messages": [{"role": "system", "content": "You are a helpful assistant."},{"role": "user", "content": "'''  # noqa: E501

    batch_jsonl_suffix = """"}],"max_tokens": 1000}}"""

    sample_df["json"] = sample_df.apply(
        lambda row: batch_jsonl_prefix
        + f"request-{row.name}"
        + batch_jsonl_infix
        + row["prompt"]
        + batch_jsonl_suffix,
        axis=1,
    )

    # write to .jsonl file
    batch_jsonl_name = f"{grammar_file_name}_batch_{2*n_samples}.jsonl"
    batch_jsonl_path = grammar_file_parent / batch_jsonl_name
    log.info(f"Writing batch job to {batch_jsonl_path}")
    with open(batch_jsonl_path, "w") as f:
        for json in sample_df["json"]:
            f.write(f"{json.replace('\n', '\\n')}\n")

    client = openai.OpenAI()
    batch_input_file = client.files.create(
        file=open(batch_jsonl_path, "rb"), purpose="batch"
    )

    print(batch_input_file)

    batch_input_file_id = batch_input_file.id

    batch = client.batches.create(
        input_file_id=batch_input_file_id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
        metadata={
            "description": f"Batch job for {grammar_file_name}",
        },
    )

    return {
        "batch_job_file": batch_jsonl_path,
        "positive_samples": pos_sample_path,
        "negative_samples": neg_sample_path,
        "batch_file_id": batch.id,
        "batch": batch,
    }


def upload_batch_job(
    batch_job_file: pathlib.Path | str,
):
    if isinstance(batch_job_file, str):
        batch_job_file = pathlib.Path(batch_job_file)

    log.info(f"Uploading batch job from {batch_job_file}")

    client = openai.OpenAI()

    batch_input_file = client.files.create(
        file=open(batch_job_file, "rb"),
    )

    print(batch_input_file)


if __name__ == "__main__":
    # fire.Fire(generate_batch_job)

    client = openai.OpenAI()
    print(client.batches.retrieve("batch_6712ee6977188190a2317ae1a56bb8bd"))

    file_response = client.files.content("file-azrKT9uDBlYGj36eqQ29M1OW")
    print(file_response.text)

    with open("data/batch_file_response.jsonl", "w") as f:
        f.write(file_response.text)
