"""batch_prompting.py

Generates and queries LLM apis for batch prompting jobs based on grammar evaluation.
"""

import dataclasses
import datetime as dt
import json
import logging
import pathlib
import random

import anthropic
import dotenv
import fire
import openai
import pandas as pd
import pyrootutils

import formal_gym.grammar as fg_grammar
import formal_gym.utils.utils as fg_utils

# Type aliases
ClaudeMessageCreateParamsNonStreaming = (
    anthropic.types.beta.message_create_params.MessageCreateParamsNonStreaming
)
ClaudeRequest = anthropic.types.beta.messages.batch_create_params.Request

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%Y-%d-%m %H:%M:%S",
    level=logging.INFO,
)

log = fg_utils.get_logger(__name__)

PROJECT_ROOT = path = pyrootutils.find_root(
    search_from=__file__, indicator=".project-root"
)

dotenv.load_dotenv(PROJECT_ROOT / ".env")


@dataclasses.dataclass(frozen=True)
class ChatCompletionResponse:
    user_prompt: str
    metadata: dict[str, str]
    system_prompt: str = "You are a helpful assistant."
    max_tokens: int = 1024

    # TODO: validate this
    def to_anthropic_json(self, model: str) -> str:
        return json.dumps(
            {
                "method": "POST",
                "url": "/v1/messages",
                "body": {
                    "model": model,
                    "messages": [{"role": "user", "content": self.user_prompt}],
                    "max_tokens": self.max_tokens,
                },
            }
        )

    # TODO: validate this
    def to_openai_json(self, model: str) -> str:
        return json.dumps(
            {
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": self.user_prompt},
                    ],
                    "max_tokens": self.max_tokens,
                },
            }
        )

    def to_anthropic_request(self, model: str, custom_id: str) -> ClaudeRequest:
        return ClaudeRequest(
            custom_id=custom_id,
            params=ClaudeMessageCreateParamsNonStreaming(
                model=model,
                max_tokens=self.max_tokens,
                messages=[{"role": "user", "content": self.user_prompt}],
            ),
        )

    def to_openai_batched_json(self, model: str, custom_id: str) -> str:
        return json.dumps(
            {
                "custom_id": custom_id,
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": self.user_prompt},
                    ],
                    "max_tokens": self.max_tokens,
                    "metadata": self.metadata,
                    "store": True if self.metadata else False,
                },
            }
        )


def create_prompt(grammar_str: str, sample: str, shots: dict[str, list[str]]) -> str:
    """Creates a prompt for the LLM based on the grammar and sample."""

    prefix = f"""You will be presented with a context-free grammar in Chomsky normal form and a string which may or may not be in the language defined by the given grammar. Your job is to determine whether or not the grammar generates the provided string. You can use any reasoning strategy you like, but you must end your response with either 'Yes' (if the string is generated by the grammar) or 'No' (if it isn't.)\n\nGrammar: ```{grammar_str}\n\n"""  # noqa: E501

    if shots["positive"]:
        few_shot = """Here are some examples of some positive and negative samples from the provided grammar:\n"""  # noqa: E501
        for ds in shots["positive"]:
            few_shot += f"- `{ds}` -> Yes\n"
        for ds in shots["negative"]:
            few_shot += f"- `{ds}` -> No\n"
        few_shot += "\n"
    else:
        few_shot = ""

    question = f"""Here is the string you need to evaluate:\n\nString: `{sample}`.\n\nRemember, end your response with either 'Yes' or 'No'."""  # noqa: E501

    return prefix + few_shot + question


def _get_batch_df(
    grammar_file: pathlib.Path | str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data",
    n_samples: int = 1000,
    save_sample_files: bool = False,
    model: str = "gpt-4o-mini",
    n_shots: int = 0,
) -> pd.DataFrame:
    if isinstance(grammar_file, str):
        grammar_file = pathlib.Path(grammar_file)

    grammar_file_name = grammar_file.stem
    grammar_path = data_path / grammar_file
    pos_sample_name = f"{grammar_file_name}_positive_{n_samples}.txt"
    neg_sample_name = f"{grammar_file_name}_negative_{n_samples}.txt"
    pos_sample_path = data_path / pos_sample_name
    neg_sample_path = data_path / neg_sample_name

    g = fg_grammar.Grammar.from_grammar(grammar_path)

    log.info(f"Generating {n_samples} positive samples from {grammar_file}")
    pos_samples = list(set(g.generate(n_samples=n_samples, sep=" ")))
    if save_sample_files or not pos_sample_path.is_file():
        log.info(f"Writing positive examples to {pos_sample_path}")
        with open(pos_sample_path, "w") as f:
            for gen in pos_samples:
                f.write(f"{gen}\n")

    log.info(f"Generating {n_samples} negative samples from {grammar_file}")
    neg_samples = list(
        set(
            g.generate_negative_samples_matching_lengths(
                positive_sample_path=pos_sample_path,
            )
        )
    )
    if save_sample_files or not neg_sample_path.is_file():
        log.info(f"Writing negative examples to {neg_sample_path}")
        with open(neg_sample_path, "w") as f:
            for gen in neg_samples:
                f.write(f"{gen}\n")

    demo_samples = {
        "positive": [],
        "negative": [],
    }

    if n_shots > 0:
        demo_samples["positive"] = random.choices(pos_samples, k=n_shots)
        demo_samples["negative"] = random.choices(neg_samples, k=n_shots)
        pos_samples = [s for s in pos_samples if s not in demo_samples["positive"]]
        neg_samples = [s for s in neg_samples if s not in demo_samples["negative"]]

    pos_df = pd.DataFrame(pos_samples, columns=["sample"])
    neg_df = pd.DataFrame(neg_samples, columns=["sample"])

    pos_df["sample_type"] = "positive"
    neg_df["sample_type"] = "negative"

    with open(grammar_path, "r") as f:
        grammar_str = f.read()

    sample_df = pd.concat([pos_df, neg_df], ignore_index=True)

    sample_df["prompt"] = sample_df["sample"].apply(
        lambda s: create_prompt(grammar_str=grammar_str, sample=s, shots=demo_samples)
    )

    return sample_df


def generate_claude_batch(
    grammar_file: pathlib.Path | str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data",
    n_samples: int = 1000,
    save_sample_files: bool = False,
    model: str = "claude-3-5-sonnet-20241022",
    n_shots: int = 0,
):
    if isinstance(grammar_file, str):
        grammar_file = pathlib.Path(grammar_file)

    sample_df = _get_batch_df(
        grammar_file=grammar_file,
        data_path=data_path,
        n_samples=n_samples,
        save_sample_files=save_sample_files,
        model=model,
        n_shots=n_shots,
    )

    sample_df[f"{model}_request"] = sample_df.apply(
        lambda row: ChatCompletionResponse(
            user_prompt=row["prompt"],
            metadata={
                "sample_type": row["sample_type"],
                "sample": row["sample"],
                "grammar_file": grammar_file.stem,
                "model": model,
                "n_shots": str(2 * n_shots),
            },
        ).to_anthropic_request(model=model, custom_id=f"request-{row.name}"),
        axis=1,
    )

    requets = sample_df[f"{model}_request"].tolist()
    print(requets[0])
    print(sample_df["sample"].iloc[0])
    print(sample_df["sample_type"].iloc[0])


def generate_batch_jsonl(
    grammar_file: pathlib.Path | str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data",
    n_samples: int = 1000,
    save_sample_files: bool = False,
    model: str = "gpt-4o-mini",
    n_shots: int = 0,
):
    if isinstance(grammar_file, str):
        grammar_file = pathlib.Path(grammar_file)

    grammar_file_name = grammar_file.stem
    sample_df = _get_batch_df(
        grammar_file=grammar_file,
        data_path=data_path,
        n_samples=n_samples,
        save_sample_files=save_sample_files,
        model=model,
        n_shots=n_shots,
    )

    sample_df[f"{model}_batched_json"] = sample_df.apply(
        lambda row: ChatCompletionResponse(
            user_prompt=row["prompt"],
            metadata={
                "sample_type": row["sample_type"],
                "sample": row["sample"],
                "grammar_file": grammar_file_name,
                "model": model,
                "n_shots": str(2 * n_shots),
            },
        ).to_openai_batched_json(model=model, custom_id=f"request-{row.name}"),
        axis=1,
    )

    batch_jsonl_filename = f"{grammar_file_name}_{model}_batched_{2*n_shots}-shot_{len(sample_df)}-samples.jsonl"
    batch_jsonl_path = data_path / batch_jsonl_filename
    log.info(f"Writing batch job to {batch_jsonl_path}")
    with open(batch_jsonl_path, "w") as f:
        for j in sample_df[f"{model}_batched_json"]:
            f.write(f"{j}\n")


def upload_batch_job(
    batch_file: pathlib.Path | str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data",
):
    if isinstance(batch_file, str):
        batch_file = pathlib.Path(batch_file)

    batch_jsonl_path = data_path / batch_file
    log.info(f"Uploading batch job from {batch_jsonl_path}")

    client = openai.OpenAI()
    batch_input_file = client.files.create(
        file=open(batch_jsonl_path, "rb"),
        purpose="batch",
    )

    log.info(f"Batch input file created: {batch_input_file}")

    batch_obj = client.batches.create(
        input_file_id=batch_input_file.id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
        metadata={"description": "Batch job for grammar evaluation."},
    )

    log.info(f"Batch job created: {batch_obj}")

    timestamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = PROJECT_ROOT / "logs" / f"batch_job_{timestamp}.log"
    with open(log_file_path, "w") as f:
        f.write(f"{batch_input_file}\n\n")
        f.write(f"{batch_obj}")


def get_batch_status(batch_id: str):
    client = openai.OpenAI()
    batch = client.batches.retrieve(batch_id)
    log.info(batch)


def get_batch_error(batch_id: str):
    client = openai.OpenAI()
    batch = client.batches.retrieve(batch_id)
    error_file = client.files.content(batch.error_file_id)
    print(error_file.text)


def get_batch_results(
    batch_id: str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data" / "completions",
):
    client = openai.OpenAI()
    batch_results = client.batches.retrieve(batch_id)
    log.info(batch_results)

    if batch_results.status == "completed":
        log.info("Batch job succeeded.")

        input_file = client.files.content(batch_results.input_file_id)
        input_path = data_path / f"{batch_id}_inputs.jsonl"
        output_file = client.files.content(batch_results.output_file_id)
        output_path = data_path / f"{batch_id}_results.jsonl"

        # @TODO: write error file if it exists

        log.info(f"Writing batch inputs to {input_path}")
        with open(input_path, "w") as f:
            f.write(input_file.text)
        log.info(f"Writing batch results to {output_path}")
        with open(output_path, "w") as f:
            f.write(output_file.text)
    else:
        log.warning("Batch job not completed yet.")


if __name__ == "__main__":
    fire.Fire()
