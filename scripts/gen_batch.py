"""batch_prompting.py

Generates and queries LLM apis for batch prompting jobs based on grammar evaluation.
"""

import dataclasses
import datetime as dt
import json
import logging
import pathlib
import random

import anthropic
import dotenv
import fire
import openai
import pandas as pd
import pyrootutils
import tqdm

import formal_gym.grammar as fg_grammar
import formal_gym.utils.utils as fg_utils

logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%Y-%d-%m %H:%M:%S",
    level=logging.INFO,
)

log = fg_utils.get_logger(__name__)

PROJECT_ROOT = path = pyrootutils.find_root(
    search_from=__file__, indicator=".project-root"
)

dotenv.load_dotenv(PROJECT_ROOT / ".env")


@dataclasses.dataclass(frozen=True)
class ChatCompletionResponse:
    user_prompt: str
    metadata: dict[str, str]
    system_prompt: str = "You are a helpful assistant."
    max_tokens: int = 1024

    def to_openai_batched_json(self, model: str, custom_id: str) -> str:
        return json.dumps(
            {
                "custom_id": custom_id,
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": self.user_prompt},
                    ],
                    "max_tokens": self.max_tokens,
                    "metadata": self.metadata,
                    "store": True if self.metadata else False,
                },
            }
        )


def create_prompt(grammar_str: str, sample: str, shots: dict[str, list[str]]) -> str:
    """Creates a prompt for the LLM based on the grammar and sample."""

    assert sample not in shots["positive"]
    assert sample not in shots["negative"]

    prefix = f"""You will be presented with a context-free grammar in Chomsky normal form and a string which may or may not be in the language defined by the given grammar. Your job is to determine whether or not the grammar generates the provided string. You can use any reasoning strategy you like, but you must end your response with either 'Yes' (if the string is generated by the grammar) or 'No' (if it isn't.)\n\nGrammar: ```{grammar_str}\n\n"""  # noqa: E501

    if shots["positive"]:
        few_shot = """Here are some examples of some positive and negative samples from the provided grammar. The order in which the samples are listed is irrelevant. Labeled samples:\n"""  # noqa: E501
        for ds in shots["positive"]:
            few_shot += f"- `{ds}` -> Yes\n"
        for ds in shots["negative"]:
            few_shot += f"- `{ds}` -> No\n"
        few_shot += "\n"
    else:
        few_shot = ""

    question = f"""Here is the string you need to evaluate:\n\nString: `{sample}`.\n\nRemember, end your response with either 'Yes' or 'No'."""  # noqa: E501

    return prefix + few_shot + question


def generate_batch_jsonl(
    samples_file: pathlib.Path | str,
    samples_dir: pathlib.Path | str = PROJECT_ROOT / "data" / "samples",
    model: str = "gpt-4o-mini",
    n_shots: int = 0,
):
    if isinstance(samples_file, str):
        samples_file = pathlib.Path(samples_file)

    grammar_base = samples_file.parent.name + ".cfg"
    grammar_file = PROJECT_ROOT / "data" / "grammars" / grammar_base
    grammar_file_name = grammar_file.stem
    with open(grammar_file, "r") as f:
        grammar_str = f.read()

    samples_path = samples_dir / samples_file
    samples_df = pd.read_csv(samples_path)

    pos_samples = samples_df[samples_df["type"] == "positive"].reset_index(drop=True)
    neg_samples = samples_df[samples_df["type"] == "negative"].reset_index(drop=True)

    samples_df["prompt"] = ""

    for idx, row in tqdm.tqdm(samples_df.iterrows(), total=len(samples_df)):
        if row["type"] == "positive":
            id_type = "positive"
            od_type = "negative"
            id_samples = pos_samples
            od_samples = neg_samples
        else:
            id_type = "negative"
            od_type = "positive"
            id_samples = neg_samples
            od_samples = pos_samples

        in_domain_idxs = [i for i in id_samples.index.values if i != idx]
        max_ood = max(od_samples.index.values)
        possible_idxs = [i for i in in_domain_idxs if i < max_ood]

        alt_ilocs = random.choices(possible_idxs, k=n_shots)
        id_alts = []
        od_alts = []
        for i in alt_ilocs:
            id_alts.append(id_samples.iloc[i]["sample"])
            od_alts.append(od_samples.iloc[i]["sample"])
        alt_samples = {
            id_type: id_alts,
            od_type: od_alts,
        }

        samples_df.at[idx, "prompt"] = create_prompt(
            grammar_str=grammar_str,
            sample=row["sample"],
            shots=alt_samples,
        )

    samples_df = samples_df.rename(columns={"type": "sample_type"})

    samples_df[f"{model}_batched_json"] = samples_df.apply(
        lambda row: ChatCompletionResponse(
            user_prompt=row["prompt"],
            metadata={
                "sample_type": row["sample_type"],
                "sample": row["sample"],
                "grammar_file": grammar_file_name,
                "model": model,
                "n_shots": str(2 * n_shots),
            },
        ).to_openai_batched_json(model=model, custom_id=f"request-{row.name}"),
        axis=1,
    )

    batch_jsonl_filename = f"{samples_file.stem}_{model}_batched_{2*n_shots}-shot.jsonl"
    batch_jsonl_path = samples_dir / samples_file.parent.name / batch_jsonl_filename
    log.info(f"Writing batch job to {batch_jsonl_path}")
    with open(batch_jsonl_path, "w") as f:
        for j in samples_df[f"{model}_batched_json"]:
            f.write(f"{j}\n")


def upload_batch_job(
    batch_file: pathlib.Path | str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data",
):
    if isinstance(batch_file, str):
        batch_file = pathlib.Path(batch_file)

    batch_jsonl_path = data_path / batch_file
    log.info(f"Uploading batch job from {batch_jsonl_path}")

    client = openai.OpenAI()
    batch_input_file = client.files.create(
        file=open(batch_jsonl_path, "rb"),
        purpose="batch",
    )

    log.info(f"Batch input file created: {batch_input_file}")

    batch_obj = client.batches.create(
        input_file_id=batch_input_file.id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
        metadata={"description": "Batch job for grammar evaluation."},
    )

    log.info(f"Batch job created: {batch_obj}")

    timestamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = PROJECT_ROOT / "logs" / f"batch_job_{timestamp}.log"
    with open(log_file_path, "w") as f:
        f.write(f"{batch_input_file}\n\n")
        f.write(f"{batch_obj}")


def get_batch_status(batch_id: str):
    client = openai.OpenAI()
    batch = client.batches.retrieve(batch_id)
    log.info(batch)


def get_batch_error(batch_id: str):
    client = openai.OpenAI()
    batch = client.batches.retrieve(batch_id)
    error_file = client.files.content(batch.error_file_id)
    print(error_file.text)


def get_batch_results(
    batch_id: str,
    data_path: pathlib.Path | str = PROJECT_ROOT / "data" / "completions",
):
    client = openai.OpenAI()
    batch_results = client.batches.retrieve(batch_id)
    log.info(batch_results)

    if batch_results.status == "completed":
        log.info("Batch job succeeded.")

        input_file = client.files.content(batch_results.input_file_id)
        input_path = data_path / f"{batch_id}_inputs.jsonl"
        output_file = client.files.content(batch_results.output_file_id)
        output_path = data_path / f"{batch_id}_results.jsonl"

        # @TODO: write error file if it exists

        log.info(f"Writing batch inputs to {input_path}")
        with open(input_path, "w") as f:
            f.write(input_file.text)
        log.info(f"Writing batch results to {output_path}")
        with open(output_path, "w") as f:
            f.write(output_file.text)
    else:
        log.warning("Batch job not completed yet.")


if __name__ == "__main__":
    fire.Fire()
