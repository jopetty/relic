{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Ngrams & Dependency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import seaborn as sns\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.parse import ViterbiParser\n",
    "\n",
    "from formal_gym import grammar as fg_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")\n",
    "\n",
    "grammars_dir = PROJECT_ROOT / \"data\" / \"grammars\"\n",
    "grammar_stats_filename = \"grammar_stats.json\"\n",
    "samples_stats_filename = \"filtered_samples_stats.json\"\n",
    "\n",
    "grammar_dirs = [\n",
    "    f\n",
    "    for f in grammars_dir.iterdir()\n",
    "    if (f.is_dir())\n",
    "    and (f / grammar_stats_filename).exists()\n",
    "    and (f / samples_stats_filename).exists()\n",
    "]\n",
    "\n",
    "grammar_stats = []\n",
    "for g in grammar_dirs:\n",
    "    g_stats = json.load(open(g / grammar_stats_filename))\n",
    "    s_stats = json.load(open(g / samples_stats_filename))\n",
    "    merged = {**g_stats, **s_stats}\n",
    "    grammar_stats.append(merged)\n",
    "grammar_stats_df = pd.DataFrame(grammar_stats)\n",
    "\n",
    "grammar_stats_df = (\n",
    "    grammar_stats_df[\n",
    "        (grammar_stats_df.coverage > 0.98)\n",
    "        & (grammar_stats_df.n_terminals <= 100)\n",
    "        & (grammar_stats_df.n_nonterminals <= 100)\n",
    "        & (grammar_stats_df.n_lexical_productions <= 100)\n",
    "        & (grammar_stats_df.n_nonlexical_productions <= 100)\n",
    "    ]\n",
    "    .drop([\"median_positive_parses\", \"mean_positive_parses\"], axis=1)\n",
    "    .sort_values(by=\"grammar_name\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# grammar_stats_df[\"grammar\"] = grammar_stats_df[\"grammar_name\"].apply(\n",
    "#     lambda x: fg_grammar.Grammar.from_file(grammars_dir / x / f\"{x}.cfg\")\n",
    "# )\n",
    "\n",
    "grammar_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_g_name = grammar_stats_df[\"grammar_name\"].iloc[0]\n",
    "test_g_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grammar = fg_grammar.Grammar.from_file(\n",
    "    grammars_dir / test_g_name / f\"{test_g_name}.cfg\"\n",
    ")\n",
    "\n",
    "test_grammar.as_pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(\n",
    "    grammar: str,\n",
    "    n: int = 1,\n",
    ") -> dict[tuple[str, ...], float]:\n",
    "    counts = defaultdict(int)\n",
    "    vocab = set()\n",
    "\n",
    "    with open(grammars_dir / grammar / \"filtered_positive_samples.txt\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(\" \")\n",
    "            vocab.update(tokens)\n",
    "            if len(tokens) < n:\n",
    "                continue\n",
    "            for i in range(len(tokens) - n + 1):\n",
    "                ngram = tuple(tokens[i : i + n])\n",
    "                counts[ngram] += 1\n",
    "\n",
    "    V = len(vocab)\n",
    "    total = sum(counts.values()) + V\n",
    "\n",
    "    def get_prob(ngram: tuple[str, ...]) -> float:\n",
    "        return (counts.get(ngram, 0) + 1) / total\n",
    "\n",
    "    return get_prob, V\n",
    "\n",
    "\n",
    "def string_logprob(s: str, get_prob, n: int) -> float:\n",
    "    tokens = s.strip().split(\" \")\n",
    "\n",
    "    if len(tokens) < n:\n",
    "        return float(\"-inf\")\n",
    "\n",
    "    logp = 0.0\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[i : i + n])\n",
    "        logp += math.log(get_prob(ngram))\n",
    "\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"t14 t18 t4 t20 t14 t24 t12\"\n",
    "N = 4\n",
    "get_p, V = get_ngram(test_g_name, n=N)\n",
    "test_string_logprob = string_logprob(test_string, get_p, N)\n",
    "\n",
    "test_string_logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nltk_ngram(grammar: str, n: int = 1) -> nltk.lm.MLE:\n",
    "    # open the file and read lines\n",
    "    with open(grammars_dir / grammar / \"filtered_positive_samples.txt\") as f:\n",
    "        lines = f.readlines()\n",
    "    # tokenize the lines\n",
    "    tokenized_lines = [line.strip().split(\" \") for line in lines]\n",
    "\n",
    "    train, vocab = padded_everygram_pipeline(2, tokenized_lines)\n",
    "    lm = nltk.lm.models.Laplace(n)\n",
    "    lm.fit(train, vocab)\n",
    "    return lm, vocab\n",
    "\n",
    "\n",
    "def get_ngram_string_logprob(s: str, lm: nltk.lm.MLE) -> float:\n",
    "    tokens = s.strip().split(\" \")\n",
    "\n",
    "    logp = 0.0\n",
    "    for i in range(len(tokens)):\n",
    "        context = tuple(tokens[max(0, i - lm.order + 1) : i])\n",
    "        logp += lm.logscore(tokens[i], context)\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, vocab = get_nltk_ngram(test_g_name, n=1)\n",
    "get_ngram_string_logprob(test_string, lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcfg_string_logprob(s: str, grammar: str) -> float:\n",
    "    grammar_object = fg_grammar.Grammar.from_file(\n",
    "        grammars_dir / grammar / f\"{grammar}.cfg\"\n",
    "    )\n",
    "\n",
    "    parser = ViterbiParser(grammar_object.as_pcfg)\n",
    "    trees = list(parser.parse(s.strip().split(\" \")))\n",
    "\n",
    "    if not trees:\n",
    "        return float(\"-inf\")\n",
    "\n",
    "    total_prob = sum(tree.prob() for tree in trees)\n",
    "    log_prob = math.log(total_prob) if total_prob > 0 else float(\"-inf\")\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pcfg_string_logprob(test_string, test_g_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(grammar: str, ns: list[int] = [1, 2, 3, 4, 5, 6]) -> float:\n",
    "    strings_file = grammars_dir / grammar / \"filtered_positive_samples.txt\"\n",
    "    with open(strings_file) as f:\n",
    "        strings = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    ngrams = {}\n",
    "    for n in ns:\n",
    "        ngrams[n], _ = get_nltk_ngram(grammar, n=n)\n",
    "\n",
    "    print(\"Calculating PCFG log probabilities...\")\n",
    "    p_logprobs = [get_pcfg_string_logprob(s, grammar) for s in strings]\n",
    "\n",
    "    qs_logprobs = {}\n",
    "    for n in ns:\n",
    "        print(f\"Calculating {n}-gram log probabilities...\")\n",
    "        qs_logprobs[n] = [get_ngram_string_logprob(s, ngrams[n]) for s in strings]\n",
    "\n",
    "    kl_divs = {}\n",
    "    for n in ns:\n",
    "        kl_divs[n] = sum(\n",
    "            math.exp(p) * (p - q)\n",
    "            for q, p in zip(qs_logprobs[n], p_logprobs)\n",
    "            if p > float(\"-inf\")\n",
    "        ) / len(strings)\n",
    "\n",
    "    return kl_divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = \"grammar_20250319112222_327647\"\n",
    "kl_divs = kl_divergence(g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divs_fmt = []\n",
    "for n, div in kl_divs.items():\n",
    "    kl_divs_fmt.append({\"n\": n, \"kl_divergence\": div})\n",
    "kl_divs_df = pd.DataFrame(kl_divs_fmt)\n",
    "\n",
    "sns.lineplot(\n",
    "    data=kl_divs_df,\n",
    "    x=\"n\",\n",
    "    y=\"kl_divergence\",\n",
    "    marker=\"o\",\n",
    "    markersize=8,\n",
    "    linewidth=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
