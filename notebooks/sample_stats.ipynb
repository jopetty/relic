{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar and Sample Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURES_DIR = PROJECT_ROOT / \"notebooks\" / \"figures\"\n",
    "\n",
    "PAPER_WIDTH_IN = 5.5\n",
    "\n",
    "\n",
    "def darken(\n",
    "    color: str\n",
    "    | tuple[float, float, float]\n",
    "    | dict[str, Any]\n",
    "    | sns.palettes._ColorPalette,\n",
    "    by: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Darken a color by provided amount.\n",
    "    \"\"\"\n",
    "\n",
    "    def _darken_color(c: str | tuple[float, float, float], by: float):\n",
    "        by = min(max(0, by), 1)\n",
    "        pct_darken = 1 - by\n",
    "\n",
    "        if isinstance(c, str):\n",
    "            c = sns.color_palette([c])[0]\n",
    "\n",
    "        for c_i in c:\n",
    "            if c_i > 1:\n",
    "                c_i /= 255\n",
    "        c_hls = colorsys.rgb_to_hls(c[0], c[1], c[2])\n",
    "        # Darken the color by reducing the lightness\n",
    "\n",
    "        c_hls = (\n",
    "            c_hls[0],  # hue\n",
    "            c_hls[1] * pct_darken,  # lightness\n",
    "            c_hls[2],  # saturation\n",
    "        )\n",
    "        # Convert back to RGB\n",
    "        c_rgb = colorsys.hls_to_rgb(c_hls[0], c_hls[1], c_hls[2])\n",
    "        return c_rgb\n",
    "\n",
    "    if isinstance(color, dict):\n",
    "        # If color is a dictionary, assume it's a palette\n",
    "        # and darken each color in the palette\n",
    "        return {k: _darken_color(v, by) for k, v in color.items()}\n",
    "    elif isinstance(color, sns.palettes._ColorPalette):\n",
    "        colors = [_darken_color(c, by) for c in color]\n",
    "        return sns.palettes._ColorPalette(colors)\n",
    "    else:\n",
    "        return _darken_color(color, by)\n",
    "\n",
    "\n",
    "# For heatmaps, correlations, -1 to 1 scales, etc\n",
    "CMAP_HEATMAP = \"vlag\"\n",
    "\n",
    "# For any plots where color differentiates sample type\n",
    "PALETTE_SAMPLE_TYPE = {\n",
    "    \"positive\": darken(\"#ffcc66\"),\n",
    "    \"negative\": \"#5c5cff\",\n",
    "    \"unknown\": \"#000000\",\n",
    "}\n",
    "\n",
    "# For any plots where color differentiates model\n",
    "PALETTE_MODEL = darken(\n",
    "    {\n",
    "        # \"gpt-4.1-nano\": sns.color_palette(\"rocket_r\", n_colors=3)[0],\n",
    "        # \"gpt-4.1-mini\": sns.color_palette(\"rocket_r\", n_colors=3)[1],\n",
    "        # \"gpt-4.1\": sns.color_palette(\"rocket_r\", n_colors=3)[2],\n",
    "        \"gpt-4.1-nano\": sns.cubehelix_palette(start=0.5, rot=-0.5, n_colors=4)[0],\n",
    "        \"gpt-4.1-mini\": sns.cubehelix_palette(start=0.5, rot=-0.5, n_colors=4)[1],\n",
    "        \"gpt-4.1\": sns.cubehelix_palette(start=0.5, rot=-0.5, n_colors=4)[2],\n",
    "        # \"o4-mini\": sns.color_palette(\"crest\", n_colors=2)[0],\n",
    "        # \"o3\": sns.color_palette(\"crest\", n_colors=2)[1],\n",
    "        \"o4-mini\": sns.color_palette(\"YlOrBr\", n_colors=2)[0],\n",
    "        \"o3\": sns.color_palette(\"YlOrBr\", n_colors=2)[1],\n",
    "        \"gemma-3-1b\": sns.cubehelix_palette(n_colors=5)[0],\n",
    "        \"gemma-3-4b\": sns.cubehelix_palette(n_colors=5)[1],\n",
    "        \"gemma-3-12b\": sns.cubehelix_palette(n_colors=5)[2],\n",
    "        \"gemma-3-27b\": sns.cubehelix_palette(n_colors=5)[3],\n",
    "    }\n",
    ")\n",
    "\n",
    "PALETTE_SCORE = {\n",
    "    \"Weighted F1\": sns.color_palette(\"terrain\", n_colors=2, desat=0.8)[0],\n",
    "    \"Macro F1\": sns.color_palette(\"terrain\", n_colors=2, desat=0.8)[1],\n",
    "}\n",
    "\n",
    "PALETTES = {\n",
    "    \"model\": PALETTE_MODEL,\n",
    "    \"sample_type\": PALETTE_SAMPLE_TYPE,\n",
    "    \"score\": PALETTE_SCORE,\n",
    "}\n",
    "\n",
    "# For marking at-chance baselines\n",
    "COLOR_AT_CHANCE = \"#ff0000\"  # Red\n",
    "ALPHA_AT_CHANCE = 0.5\n",
    "\n",
    "# Bar Chart settings\n",
    "BAR_EDGE_COLOR = \"black\"\n",
    "BAR_EDGE_WIDTH = 0.8\n",
    "\n",
    "\n",
    "def display_palette(palette: dict[str, Any] | sns.palettes._ColorPalette):\n",
    "    if isinstance(palette, sns.palettes._ColorPalette):\n",
    "        display(palette)\n",
    "    else:\n",
    "        colors = list(palette.values())\n",
    "        display(sns.color_palette(colors))\n",
    "\n",
    "\n",
    "def filter_by_alpha(\n",
    "    keys: list[str],\n",
    "    ax,\n",
    "    palette: dict[str, Any] | None = None,\n",
    "    alpha=0.3,\n",
    "    highlight: str | list[str] | None = None,\n",
    "):\n",
    "    alphas = defaultdict(lambda: alpha)\n",
    "    if highlight is not None:\n",
    "        if isinstance(highlight, str):\n",
    "            highlight = [highlight]\n",
    "        for h in highlight:\n",
    "            alphas[h] = 1\n",
    "\n",
    "    if palette is None:\n",
    "        # look through all the palettes in PALETTE; find one whose keys match\n",
    "        # the keys passed in; if found, use that palette; otherwise, throw an error\n",
    "        for palette_name, palette_dict in PALETTES.items():\n",
    "            if all(k in palette_dict for k in keys):\n",
    "                palette = palette_dict\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"No matching palette found for keys: {keys}\")\n",
    "\n",
    "    face_colors = ax.collections[0].get_facecolors()\n",
    "    face_colors[:, 3] = alpha\n",
    "    for key in keys:\n",
    "        key_color = palette[key]\n",
    "\n",
    "        # Get indices of face_colors whose first 3 values match the model color\n",
    "        indices = [\n",
    "            i\n",
    "            for i, color in enumerate(face_colors)\n",
    "            if (color[0], color[1], color[2]) == key_color[:3]\n",
    "        ]\n",
    "        for i in indices:\n",
    "            face_colors[i][3] = alphas[key]\n",
    "    ax.collections[0].set_facecolor(face_colors)\n",
    "\n",
    "\n",
    "def legend_format(\n",
    "    ax: mpl.axes._axes.Axes | sns.FacetGrid,\n",
    "    keys: list[str] | None = None,\n",
    "    title: str | None = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if isinstance(ax, sns.FacetGrid):\n",
    "        fg = ax\n",
    "        ax = fg.ax\n",
    "\n",
    "        _ = fg.legend.remove()\n",
    "\n",
    "    # Legend Formatting\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    if keys is not None:\n",
    "        if \"gpt-4.1\" in keys:\n",
    "            spacing_locs = [3, 6]\n",
    "        else:\n",
    "            ValueError(f\"Unsure how to space legend for {keys=}\")\n",
    "\n",
    "        spacer = mpatches.Patch(alpha=0, linewidth=0)\n",
    "        for sloc in spacing_locs:\n",
    "            handles.insert(sloc, spacer)\n",
    "            labels.insert(sloc, \"\")\n",
    "\n",
    "    _ = ax.legend(handles, labels)\n",
    "    _ = ax.get_legend().set_frame_on(False)\n",
    "\n",
    "    if title is not None:\n",
    "        _ = ax.get_legend().set_title(title)\n",
    "\n",
    "    if \"loc\" not in kwargs:\n",
    "        kwargs[\"loc\"] = \"upper left\"\n",
    "    if \"bbox_to_anchor\" not in kwargs:\n",
    "        kwargs[\"bbox_to_anchor\"] = (1, 1)\n",
    "    _ = sns.move_legend(ax, **kwargs)\n",
    "\n",
    "\n",
    "# MARK: Printing, experimentation, etc\n",
    "display(sns.color_palette(\"terrain\", n_colors=2, desat=0.8))\n",
    "display(darken(sns.color_palette(\"terrain\", n_colors=2, desat=0.8), by=0.2))\n",
    "\n",
    "display_palette(PALETTE_MODEL)\n",
    "display_palette(PALETTE_SAMPLE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cyclic(g_string: str) -> bool:\n",
    "    \"\"\"Check for cycles in the graph string.\"\"\"\n",
    "\n",
    "    productions = {}\n",
    "    for line in g_string.strip().split(\"\\n\"):\n",
    "        match = re.match(r\"(\\w+)\\s*->\\s*(\\w+)\\s+(\\w+)\", line.strip())\n",
    "        if match:\n",
    "            lhs = match.group(1)\n",
    "            rhs1 = match.group(2)\n",
    "            rhs2 = match.group(3)\n",
    "            productions.setdefault(lhs, []).extend([rhs1, rhs2])\n",
    "\n",
    "    visited = set()\n",
    "    recursion_stack = set()\n",
    "\n",
    "    def check_cycle(node):\n",
    "        visited.add(node)\n",
    "        recursion_stack.add(node)\n",
    "\n",
    "        for neighbor in productions.get(node, []):\n",
    "            if neighbor not in visited:\n",
    "                if check_cycle(neighbor):\n",
    "                    return True\n",
    "            elif neighbor in recursion_stack:\n",
    "                return True\n",
    "        recursion_stack.remove(node)\n",
    "        return False\n",
    "\n",
    "    if \"S\" in productions:\n",
    "        return check_cycle(\"S\")\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules1 = \"\"\"\n",
    "S -> B C\n",
    "C -> D E\n",
    "D -> F G\n",
    "F -> C H\n",
    "\"\"\"\n",
    "\n",
    "rules2 = \"\"\"\n",
    "S -> A B\n",
    "A -> C D\n",
    "C -> E F\n",
    "\"\"\"\n",
    "\n",
    "rules3 = \"\"\"\n",
    "S -> A B\n",
    "A -> B C\n",
    "B -> C A\n",
    "\"\"\"\n",
    "\n",
    "rules4 = \"\"\"\n",
    "S -> A B\n",
    "A -> C D\n",
    "B -> E F\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Rules 1 have cycles: {is_cyclic(rules1)}\")\n",
    "print(f\"Rules 2 have cycles: {is_cyclic(rules2)}\")\n",
    "print(f\"Rules 3 have cycles: {is_cyclic(rules3)}\")\n",
    "print(f\"Rules 4 have cycles: {is_cyclic(rules4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")\n",
    "\n",
    "grammars_dir = PROJECT_ROOT / \"data\" / \"grammars\"\n",
    "grammar_stats_filename = \"grammar_stats.json\"\n",
    "samples_stats_filename = \"filtered_samples_stats.json\"\n",
    "\n",
    "grammars = [\n",
    "    f\n",
    "    for f in grammars_dir.iterdir()\n",
    "    if (f.is_dir())\n",
    "    and (f / grammar_stats_filename).exists()\n",
    "    and (f / samples_stats_filename).exists()\n",
    "]\n",
    "\n",
    "stats = []\n",
    "for g in grammars:\n",
    "    g_stats = json.load(open(g / grammar_stats_filename))\n",
    "    s_stats = json.load(open(g / samples_stats_filename))\n",
    "\n",
    "    # Check to see if grammar is cyclic\n",
    "    g_file = g / f\"{g.name}.cfg\"\n",
    "    g_str = open(g_file).read()\n",
    "    g_stats[\"is_cyclic\"] = is_cyclic(g_str)\n",
    "\n",
    "    merged = {**g_stats, **s_stats}\n",
    "    stats.append(merged)\n",
    "stats_df = pd.DataFrame(stats)\n",
    "\n",
    "# Filter grammars to only keep those with at least 90% coverage of positive & negative\n",
    "# samples to ensure we aren't testing models on languages which can't generate strings\n",
    "# of the relevant lengths.\n",
    "good_stats_df = (\n",
    "    stats_df[stats_df.coverage > 0.8]\n",
    "    .sort_values(by=\"grammar_name\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "good_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 3))\n",
    "gs = gridspec.GridSpec(1, 4)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "hparams = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "for ax, hparam in zip(axes, hparams):\n",
    "    sns.histplot(\n",
    "        data=good_stats_df,\n",
    "        x=hparam,\n",
    "        binwidth=100,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(hparam)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "fig.suptitle(\"Compression ratio vs. Grammar HParams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_terminals\"\n",
    "x_tasks = [\"n_nonterminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonterminals\"\n",
    "x_tasks = [\"n_terminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_lexical_productions\"\n",
    "x_tasks = [\"n_terminals\", \"n_nonterminals\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonlexical_productions\"\n",
    "x_tasks = [\"n_terminals\", \"n_nonterminals\", \"n_lexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_corr = good_stats_df[\n",
    "    [\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "    ]\n",
    "].corr()\n",
    "hyp_mask = np.triu(np.ones_like(hyp_corr, dtype=bool))\n",
    "\n",
    "_ = sns.heatmap(\n",
    "    hyp_corr,\n",
    "    # mask=hyp_mask,\n",
    "    annot=True,\n",
    "    cmap=\"vlag_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we have a set of grammars whose hyperparameters are not too correlated, we do grid searches for grammars for each hp between 1 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_stats_small = good_stats_df = (\n",
    "    stats_df[\n",
    "        (stats_df.coverage > 0.5)\n",
    "        & (stats_df.n_terminals < 100)\n",
    "        & (stats_df.n_nonterminals < 100)\n",
    "        & (stats_df.n_lexical_productions < 100)\n",
    "        & (stats_df.n_nonlexical_productions < 100)\n",
    "    ]\n",
    "    .sort_values(by=\"grammar_name\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "good_stats_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "_ = sns.histplot(\n",
    "    data=good_stats_small,\n",
    "    x=\"coverage\",\n",
    "    hue=\"is_cyclic\",\n",
    "    binwidth=0.01,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"% Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 3))\n",
    "gs = gridspec.GridSpec(1, 4)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "hparams = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "for ax, hparam in zip(axes, hparams):\n",
    "    sns.histplot(\n",
    "        data=good_stats_small,\n",
    "        x=hparam,\n",
    "        binwidth=10,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(hparam)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\"Compression ratio vs. Grammar HParams\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"coverage\",\n",
    "        palette=sns.color_palette(\"ch:s=.25,rot=-.25\", as_cmap=True),\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\"Compression ratio vs. Grammar HParams\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_terminals\"\n",
    "x_tasks = [\"n_nonterminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small, x=x_tasks[i], y=y_task, ax=ax, hue=\"is_cyclic\"\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(f\"n_terminalals vs. other HParams (n={len(good_stats_small)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonterminals\"\n",
    "x_tasks = [\"n_terminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_lexical_productions\"\n",
    "x_tasks = [\"n_terminals\", \"n_nonterminals\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonlexical_productions\"\n",
    "x_tasks = [\"n_terminals\", \"n_nonterminals\", \"n_lexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_corr_small = good_stats_small[\n",
    "    [\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "    ]\n",
    "].corr()\n",
    "hyp_mask_small = np.triu(np.ones_like(hyp_corr_small, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "_ = sns.heatmap(\n",
    "    hyp_corr_small,\n",
    "    mask=hyp_mask_small,\n",
    "    annot=True,\n",
    "    cmap=\"vlag_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_title(f\"Correlation of HParams ≤ 100 (n={len(good_stats_small)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_regions(df: pd.DataFrame, cols: list[str], n_bins=5):\n",
    "    discretized_cols = []\n",
    "    for col in cols:\n",
    "        bins = np.linspace(df[col].min(), df[col].max(), n_bins + 1)\n",
    "        discretized_col = pd.cut(df[col], bins=bins, labels=False, include_lowest=True)\n",
    "        discretized_cols.append(discretized_col)\n",
    "\n",
    "    df_discrete = pd.DataFrame(dict(zip(cols, discretized_cols)))\n",
    "    region_counts = df_discrete.value_counts().sort_values(ascending=False)\n",
    "    threshold = region_counts.mean() * 2\n",
    "    overrepresented_regions = region_counts[region_counts > threshold].index.tolist()\n",
    "    return df_discrete, overrepresented_regions\n",
    "\n",
    "\n",
    "def subsample(df, cols, target_max_corr=0.1, grid_bins=5, removal_fraction=0.1):\n",
    "    \"\"\"Iteratively subsamples dataframe to lower the pairwise correlation\"\"\"\n",
    "\n",
    "    df_subsampled = df.copy()\n",
    "    while True:\n",
    "        corr_matrix = df_subsampled[cols].corr()\n",
    "        max_abs_corr = np.abs(\n",
    "            corr_matrix.values[np.triu_indices_from(corr_matrix, k=1)]\n",
    "        ).max()\n",
    "        if max_abs_corr <= target_max_corr:\n",
    "            break\n",
    "\n",
    "        df_discrete, dense_regions = get_dense_regions(\n",
    "            df_subsampled, cols=cols, n_bins=grid_bins\n",
    "        )\n",
    "        if not dense_regions:\n",
    "            break\n",
    "\n",
    "        most_correlated_pair = None\n",
    "        max_corr = -1\n",
    "        for i in range(len(cols)):\n",
    "            for j in range(i + 1, len(cols)):\n",
    "                if np.abs(corr_matrix.loc[cols[i], cols[j]]) > max_corr:\n",
    "                    max_corr = np.abs(corr_matrix.loc[cols[i], cols[j]])\n",
    "                    most_correlated_pair = (cols[i], cols[j])\n",
    "\n",
    "        if most_correlated_pair:\n",
    "            col1, col2 = most_correlated_pair\n",
    "            indices_to_remove = []\n",
    "            for region in dense_regions:\n",
    "                region_filter = True\n",
    "                for i, val in enumerate(region):\n",
    "                    region_filter &= df_discrete.iloc[:, i] == val\n",
    "                region_indices = df_subsampled[region_filter].index.tolist()\n",
    "                if region_indices:\n",
    "                    sub_df = df_subsampled.loc[region_indices]\n",
    "                    if not sub_df.empty:\n",
    "                        sub_df[\"correlation_contribution\"] = (\n",
    "                            sub_df[col1] - sub_df[col1].mean()\n",
    "                        ) * (sub_df[col2] - sub_df[col2].mean())\n",
    "                        indices_to_remove.extend(\n",
    "                            sub_df.sort_values(\n",
    "                                by=\"correlation_contribution\", ascending=False\n",
    "                            )\n",
    "                            .head(int(len(sub_df) * removal_fraction))\n",
    "                            .index.tolist()\n",
    "                        )\n",
    "\n",
    "            if indices_to_remove:\n",
    "                df_subsampled = df_subsampled.drop(\n",
    "                    index=list(set(indices_to_remove))\n",
    "                ).reset_index(drop=True)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        if df_subsampled.empty:\n",
    "            break\n",
    "\n",
    "    return df_subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "good_stats_small_subsampled = subsample(\n",
    "    good_stats_small,\n",
    "    cols=corr_cols,\n",
    "    target_max_corr=0.0,\n",
    "    grid_bins=4,\n",
    "    removal_fraction=0.99,\n",
    ")\n",
    "\n",
    "len(good_stats_small_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "_ = sns.histplot(\n",
    "    data=good_stats_small_subsampled,\n",
    "    x=\"coverage\",\n",
    "    hue=\"is_cyclic\",\n",
    "    binwidth=0.01,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"% Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small_subsampled,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\"Compression ratio vs. Grammar HParams\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small_subsampled,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"coverage\",\n",
    "        palette=sns.color_palette(\"ch:s=.25,rot=-.25\", as_cmap=True),\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\"Compression ratio vs. Grammar HParams\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_corr_small_sub = good_stats_small_subsampled[\n",
    "    [\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "    ]\n",
    "].corr()\n",
    "hyp_mask_small_sub = np.triu(np.ones_like(hyp_corr_small_sub, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "_ = sns.heatmap(\n",
    "    hyp_corr_small_sub,\n",
    "    mask=hyp_mask_small_sub,\n",
    "    annot=True,\n",
    "    cmap=\"vlag_r\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_title(\n",
    "    f\"Correlation of HParams ≤ 100, Subsampled  (n={len(good_stats_small_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0, sharex=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0, sharex=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_terminals\"\n",
    "x_tasks = [\"n_nonterminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "a_max = 0\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small_subsampled, x=x_tasks[i], y=y_task, ax=ax, hue=\"is_cyclic\"\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    if ax.get_xlim()[1] > a_max:\n",
    "        a_max = ax.get_xlim()[1]\n",
    "    if ax.get_ylim()[1] > a_max:\n",
    "        a_max = ax.get_ylim()[1]\n",
    "\n",
    "axes[0].set_xlim(0, a_max)\n",
    "axes[0].set_ylim(0, a_max)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"n_terminalals vs. other HParams (Subsampled, n={len(good_stats_small_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonterminals\"\n",
    "x_tasks = [\"n_terminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "a_max = 0\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small_subsampled, x=x_tasks[i], y=y_task, ax=ax, hue=\"is_cyclic\"\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "    if ax.get_xlim()[1] > a_max:\n",
    "        a_max = ax.get_xlim()[1]\n",
    "    if ax.get_ylim()[1] > a_max:\n",
    "        a_max = ax.get_ylim()[1]\n",
    "\n",
    "axes[0].set_xlim(0, a_max)\n",
    "axes[0].set_ylim(0, a_max)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"n_nonterminals vs. other HParams (Subsampled, n={len(good_stats_small_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_subsample(\n",
    "    df: pd.DataFrame, end_points: int, cols: list[str], seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    data = df[cols].to_numpy()\n",
    "    data_scaled = (data - data.min(axis=0)) / (\n",
    "        data.max(axis=0) - data.min(axis=0) + 1e-8\n",
    "    )\n",
    "\n",
    "    # start with a random subset\n",
    "    current_idx = np.random.choice(len(df), size=end_points, replace=False)\n",
    "\n",
    "    def mean_pairwise_dist(subset: np.ndarray) -> float:\n",
    "        diffs = subset[:, np.newaxis, :] - subset[np.newaxis, :, :]  # (n, n, d)\n",
    "        dists = np.sqrt(np.sum(diffs**2, axis=-1))  # (n, n)\n",
    "        return np.sum(dists) / (len(subset) * (len(subset) - 1) + 1e-8)\n",
    "\n",
    "    def objective(indices: np.ndarray):\n",
    "        subset = data_scaled[indices]\n",
    "        corr = np.corrcoef(subset, rowvar=False)\n",
    "        corr[np.isnan(corr)] = 0\n",
    "        off_diag_corr = np.sum(np.abs(corr - np.eye(len(cols))))\n",
    "        # coverage term: maximize mean pairwise distance\n",
    "        mean_dist = mean_pairwise_dist(subset)\n",
    "        return off_diag_corr - mean_dist  # lower is better\n",
    "\n",
    "    # greedy local search\n",
    "    for _ in range(1000):\n",
    "        i = np.random.randint(end_points)\n",
    "        j = np.random.randint(len(df))\n",
    "        if j in current_idx:\n",
    "            continue\n",
    "        new_idx = current_idx.copy()\n",
    "        new_idx[i] = j\n",
    "        if objective(new_idx) < objective(current_idx):\n",
    "            current_idx = new_idx\n",
    "\n",
    "    return df.iloc[np.unique(current_idx)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "df_new_subsampled = new_subsample(\n",
    "    good_stats_small,\n",
    "    end_points=99,\n",
    "    cols=corr_cols,\n",
    ")\n",
    "\n",
    "len(df_new_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hpy_corr_small = df_new_subsampled[corr_cols].corr()\n",
    "new_hpy_corr_small = new_hpy_corr_small.rename(\n",
    "    {\n",
    "        \"n_terminals\": r\"$n_\\text{term}$\",\n",
    "        \"n_nonterminals\": r\"$n_\\text{nonterm}$\",\n",
    "        \"n_lexical_productions\": r\"$n_\\text{lex}$\",\n",
    "        \"n_nonlexical_productions\": r\"$n_\\text{nonlex}$\",\n",
    "    },\n",
    "    axis=1,\n",
    ").rename(\n",
    "    {\n",
    "        \"n_terminals\": r\"$n_\\text{term}$\",\n",
    "        \"n_nonterminals\": r\"$n_\\text{nonterm}$\",\n",
    "        \"n_lexical_productions\": r\"$n_\\text{lex}$\",\n",
    "        \"n_nonlexical_productions\": r\"$n_\\text{nonlex}$\",\n",
    "    },\n",
    "    axis=0,\n",
    ")\n",
    "new_hpy_corr_mask = np.triu(np.ones_like(new_hpy_corr_small, dtype=bool))\n",
    "\n",
    "with sns.plotting_context(\"notebook\"):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "    _ = sns.heatmap(\n",
    "        new_hpy_corr_small,\n",
    "        mask=new_hpy_corr_mask,\n",
    "        annot=True,\n",
    "        cmap=CMAP_HEATMAP,\n",
    "        center=0,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        fmt=\".2f\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    _ = ax.set_title(\"Correlation of G100 HParams\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"g100_corr.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0, sharex=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0, sharex=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_terminals\"\n",
    "x_tasks = [\"n_nonterminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "a_max = 0\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=df_new_subsampled, x=x_tasks[i], y=y_task, ax=ax, hue=\"is_cyclic\"\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    if ax.get_xlim()[1] > a_max:\n",
    "        a_max = ax.get_xlim()[1]\n",
    "    if ax.get_ylim()[1] > a_max:\n",
    "        a_max = ax.get_ylim()[1]\n",
    "\n",
    "axes[0].set_xlim(0, a_max)\n",
    "axes[0].set_ylim(0, a_max)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"n_terminalals vs. other HParams (Subsampled, n={len(df_new_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=df_new_subsampled,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.legend_.remove()\n",
    "\n",
    "_ = fig.suptitle(f\"Compression ratio vs. Grammar HParams (n={len(df_new_subsampled)})\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "_ = sns.histplot(\n",
    "    data=df_new_subsampled,\n",
    "    x=\"coverage\",\n",
    "    hue=\"is_cyclic\",\n",
    "    binwidth=0.01,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"% Coverage\")\n",
    "_ = ax.set_title(f\"Coverage Histogram (n={len(df_new_subsampled)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_stats_large = good_stats_df = (\n",
    "    stats_df[\n",
    "        (stats_df.coverage > 0.5)\n",
    "        & (\n",
    "            (stats_df.n_terminals > 100)\n",
    "            | (stats_df.n_nonterminals > 100)\n",
    "            | (stats_df.n_lexical_productions > 100)\n",
    "            | (stats_df.n_nonlexical_productions > 100)\n",
    "        )\n",
    "        & (stats_df.n_nonterminals < 10000)\n",
    "        & (stats_df.n_terminals < 400)\n",
    "        & (stats_df.n_lexical_productions < 10000)\n",
    "        & (stats_df.n_nonlexical_productions < 10000)\n",
    "    ]\n",
    "    .sort_values(by=\"grammar_name\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "good_stats_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "_ = sns.histplot(\n",
    "    data=good_stats_large,\n",
    "    x=\"coverage\",\n",
    "    hue=\"is_cyclic\",\n",
    "    binwidth=0.01,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"% Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_large,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\"Compression ratio vs. Grammar HParams\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "df_large_subsampled = new_subsample(\n",
    "    good_stats_large,\n",
    "    end_points=99,\n",
    "    cols=corr_cols,\n",
    ")\n",
    "\n",
    "len(df_large_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hpy_corr_large = df_large_subsampled[corr_cols].corr()\n",
    "\n",
    "new_hpy_corr_large = new_hpy_corr_large.rename(\n",
    "    {\n",
    "        \"n_terminals\": r\"$n_\\text{term}$\",\n",
    "        \"n_nonterminals\": r\"$n_\\text{nonterm}$\",\n",
    "        \"n_lexical_productions\": r\"$n_\\text{lex}$\",\n",
    "        \"n_nonlexical_productions\": r\"$n_\\text{nonlex}$\",\n",
    "    },\n",
    "    axis=1,\n",
    ").rename(\n",
    "    {\n",
    "        \"n_terminals\": r\"$n_\\text{term}$\",\n",
    "        \"n_nonterminals\": r\"$n_\\text{nonterm}$\",\n",
    "        \"n_lexical_productions\": r\"$n_\\text{lex}$\",\n",
    "        \"n_nonlexical_productions\": r\"$n_\\text{nonlex}$\",\n",
    "    },\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "new_hpy_corr_mask = np.triu(np.ones_like(new_hpy_corr_large, dtype=bool))\n",
    "\n",
    "with sns.plotting_context(\"notebook\"):\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    _ = sns.heatmap(\n",
    "        new_hpy_corr_large,\n",
    "        mask=new_hpy_corr_mask,\n",
    "        annot=True,\n",
    "        cmap=CMAP_HEATMAP,\n",
    "        center=0,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        fmt=\".2f\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    _ = ax.set_title(\"Correlation of G400 HParams\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"g400_corr.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"notebook\", font_scale=0.8):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(PAPER_WIDTH_IN, 2.8))\n",
    "    cbar_ax = fig.add_axes([0.92, 0.01, 0.04, 0.8])\n",
    "\n",
    "    heatmap_args = {\n",
    "        \"mask\": new_hpy_corr_mask,\n",
    "        \"annot\": True,\n",
    "        \"cmap\": CMAP_HEATMAP,\n",
    "        \"center\": 0,\n",
    "        \"vmin\": -1,\n",
    "        \"vmax\": 1,\n",
    "        \"fmt\": \".2f\",\n",
    "    }\n",
    "\n",
    "    sns.heatmap(\n",
    "        data=new_hpy_corr_small,\n",
    "        ax=ax1,\n",
    "        **heatmap_args,\n",
    "        cbar=False,\n",
    "    )\n",
    "    sns.heatmap(\n",
    "        data=new_hpy_corr_large,\n",
    "        ax=ax2,\n",
    "        cbar_ax=cbar_ax,\n",
    "        cbar=True,\n",
    "        **heatmap_args,\n",
    "    )\n",
    "\n",
    "    ax1.set_title(\"Correlation of G100 HParams\")\n",
    "    ax2.set_title(\"Correlation of G400 HParams\")\n",
    "\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"g_corr.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hpy_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0, sharex=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0, sharex=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_terminals\"\n",
    "x_tasks = [\"n_nonterminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "a_max = 0\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=df_large_subsampled, x=x_tasks[i], y=y_task, ax=ax, hue=\"is_cyclic\"\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    if ax.get_xlim()[1] > a_max:\n",
    "        a_max = ax.get_xlim()[1]\n",
    "    if ax.get_ylim()[1] > a_max:\n",
    "        a_max = ax.get_ylim()[1]\n",
    "\n",
    "axes[0].set_xlim(0, a_max)\n",
    "axes[0].set_ylim(0, a_max)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"n_terminalals vs. other HParams (Subsampled, n={len(df_large_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=df_large_subsampled,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.legend_.remove()\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"Compression ratio vs. Grammar HParams (n={len(df_large_subsampled)})\"\n",
    ")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "_ = sns.histplot(\n",
    "    data=df_large_subsampled,\n",
    "    x=\"coverage\",\n",
    "    hue=\"is_cyclic\",\n",
    "    binwidth=0.01,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"% Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large_subsampled[\"grammar_name\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
