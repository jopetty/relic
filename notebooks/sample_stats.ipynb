{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar and Sample Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cyclic(g_string: str) -> bool:\n",
    "    \"\"\"Check for cycles in the graph string.\"\"\"\n",
    "\n",
    "    productions = {}\n",
    "    for line in g_string.strip().split(\"\\n\"):\n",
    "        match = re.match(r\"(\\w+)\\s*->\\s*(\\w+)\\s+(\\w+)\", line.strip())\n",
    "        if match:\n",
    "            lhs = match.group(1)\n",
    "            rhs1 = match.group(2)\n",
    "            rhs2 = match.group(3)\n",
    "            productions.setdefault(lhs, []).extend([rhs1, rhs2])\n",
    "\n",
    "    visited = set()\n",
    "    recursion_stack = set()\n",
    "\n",
    "    def check_cycle(node):\n",
    "        visited.add(node)\n",
    "        recursion_stack.add(node)\n",
    "\n",
    "        for neighbor in productions.get(node, []):\n",
    "            if neighbor not in visited:\n",
    "                if check_cycle(neighbor):\n",
    "                    return True\n",
    "            elif neighbor in recursion_stack:\n",
    "                return True\n",
    "        recursion_stack.remove(node)\n",
    "        return False\n",
    "\n",
    "    if \"S\" in productions:\n",
    "        return check_cycle(\"S\")\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules1 = \"\"\"\n",
    "S -> B C\n",
    "C -> D E\n",
    "D -> F G\n",
    "F -> C H\n",
    "\"\"\"\n",
    "\n",
    "rules2 = \"\"\"\n",
    "S -> A B\n",
    "A -> C D\n",
    "C -> E F\n",
    "\"\"\"\n",
    "\n",
    "rules3 = \"\"\"\n",
    "S -> A B\n",
    "A -> B C\n",
    "B -> C A\n",
    "\"\"\"\n",
    "\n",
    "rules4 = \"\"\"\n",
    "S -> A B\n",
    "A -> C D\n",
    "B -> E F\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Rules 1 have cycles: {is_cyclic(rules1)}\")\n",
    "print(f\"Rules 2 have cycles: {is_cyclic(rules2)}\")\n",
    "print(f\"Rules 3 have cycles: {is_cyclic(rules3)}\")\n",
    "print(f\"Rules 4 have cycles: {is_cyclic(rules4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")\n",
    "\n",
    "grammars_dir = PROJECT_ROOT / \"data\" / \"grammars\"\n",
    "grammar_stats_filename = \"grammar_stats.json\"\n",
    "samples_stats_filename = \"filtered_samples_stats.json\"\n",
    "\n",
    "grammars = [\n",
    "    f\n",
    "    for f in grammars_dir.iterdir()\n",
    "    if (f.is_dir())\n",
    "    and (f / grammar_stats_filename).exists()\n",
    "    and (f / samples_stats_filename).exists()\n",
    "]\n",
    "\n",
    "stats = []\n",
    "for g in grammars:\n",
    "    g_stats = json.load(open(g / grammar_stats_filename))\n",
    "    s_stats = json.load(open(g / samples_stats_filename))\n",
    "\n",
    "    # Check to see if grammar is cyclic\n",
    "    g_file = g / f\"{g.name}.cfg\"\n",
    "    g_str = open(g_file).read()\n",
    "    g_stats[\"is_cyclic\"] = is_cyclic(g_str)\n",
    "\n",
    "    merged = {**g_stats, **s_stats}\n",
    "    stats.append(merged)\n",
    "stats_df = pd.DataFrame(stats)\n",
    "\n",
    "# Filter grammars to only keep those with at least 90% coverage of positive & negative\n",
    "# samples to ensure we aren't testing models on languages which can't generate strings\n",
    "# of the relevant lengths.\n",
    "good_stats_df = (\n",
    "    stats_df[stats_df.coverage > 0.8]\n",
    "    .sort_values(by=\"grammar_name\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "good_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 3))\n",
    "gs = gridspec.GridSpec(1, 4)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "hparams = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "for ax, hparam in zip(axes, hparams):\n",
    "    sns.histplot(\n",
    "        data=good_stats_df,\n",
    "        x=hparam,\n",
    "        binwidth=100,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(hparam)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "fig.suptitle(\"Compression ratio vs. Grammar HParams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_terminals\"\n",
    "x_tasks = [\"n_nonterminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonterminals\"\n",
    "x_tasks = [\"n_terminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_lexical_productions\"\n",
    "x_tasks = [\"n_terminals\", \"n_nonterminals\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonlexical_productions\"\n",
    "x_tasks = [\"n_terminals\", \"n_nonterminals\", \"n_lexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_df,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_corr = good_stats_df[\n",
    "    [\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "    ]\n",
    "].corr()\n",
    "hyp_mask = np.triu(np.ones_like(hyp_corr, dtype=bool))\n",
    "\n",
    "_ = sns.heatmap(\n",
    "    hyp_corr,\n",
    "    # mask=hyp_mask,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we have a set of grammars whose hyperparameters are not too correlated, we do grid searches for grammars for each hp between 1 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_stats_small = good_stats_df = (\n",
    "    stats_df[\n",
    "        (stats_df.coverage > 0.5)\n",
    "        & (stats_df.n_terminals < 100)\n",
    "        & (stats_df.n_nonterminals < 100)\n",
    "        & (stats_df.n_lexical_productions < 100)\n",
    "        & (stats_df.n_nonlexical_productions < 100)\n",
    "    ]\n",
    "    .sort_values(by=\"grammar_name\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "good_stats_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "_ = sns.histplot(\n",
    "    data=good_stats_small,\n",
    "    x=\"coverage\",\n",
    "    hue=\"is_cyclic\",\n",
    "    binwidth=0.01,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"% Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 3))\n",
    "gs = gridspec.GridSpec(1, 4)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "hparams = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "for ax, hparam in zip(axes, hparams):\n",
    "    sns.histplot(\n",
    "        data=good_stats_small,\n",
    "        x=hparam,\n",
    "        binwidth=10,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(hparam)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\"Compression ratio vs. Grammar HParams\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"coverage\",\n",
    "        palette=sns.color_palette(\"ch:s=.25,rot=-.25\", as_cmap=True),\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\"Compression ratio vs. Grammar HParams\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_terminals\"\n",
    "x_tasks = [\"n_nonterminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small, x=x_tasks[i], y=y_task, ax=ax, hue=\"is_cyclic\"\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(f\"n_terminalals vs. other HParams (n={len(good_stats_small)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonterminals\"\n",
    "x_tasks = [\"n_terminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_lexical_productions\"\n",
    "x_tasks = [\"n_terminals\", \"n_nonterminals\", \"n_nonlexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonlexical_productions\"\n",
    "x_tasks = [\"n_terminals\", \"n_nonterminals\", \"n_lexical_productions\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_corr_small = good_stats_small[\n",
    "    [\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "    ]\n",
    "].corr()\n",
    "hyp_mask_small = np.triu(np.ones_like(hyp_corr_small, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "_ = sns.heatmap(\n",
    "    hyp_corr_small,\n",
    "    mask=hyp_mask_small,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_title(f\"Correlation of HParams ≤ 100 (n={len(good_stats_small)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_regions(df: pd.DataFrame, cols: list[str], n_bins=5):\n",
    "    discretized_cols = []\n",
    "    for col in cols:\n",
    "        bins = np.linspace(df[col].min(), df[col].max(), n_bins + 1)\n",
    "        discretized_col = pd.cut(df[col], bins=bins, labels=False, include_lowest=True)\n",
    "        discretized_cols.append(discretized_col)\n",
    "\n",
    "    df_discrete = pd.DataFrame(dict(zip(cols, discretized_cols)))\n",
    "    region_counts = df_discrete.value_counts().sort_values(ascending=False)\n",
    "    threshold = region_counts.mean() * 2\n",
    "    overrepresented_regions = region_counts[region_counts > threshold].index.tolist()\n",
    "    return df_discrete, overrepresented_regions\n",
    "\n",
    "\n",
    "def subsample(df, cols, target_max_corr=0.1, grid_bins=5, removal_fraction=0.1):\n",
    "    \"\"\"Iteratively subsamples dataframe to lower the pairwise correlation\"\"\"\n",
    "\n",
    "    df_subsampled = df.copy()\n",
    "    while True:\n",
    "        corr_matrix = df_subsampled[cols].corr()\n",
    "        max_abs_corr = np.abs(\n",
    "            corr_matrix.values[np.triu_indices_from(corr_matrix, k=1)]\n",
    "        ).max()\n",
    "        if max_abs_corr <= target_max_corr:\n",
    "            break\n",
    "\n",
    "        df_discrete, dense_regions = get_dense_regions(\n",
    "            df_subsampled, cols=cols, n_bins=grid_bins\n",
    "        )\n",
    "        if not dense_regions:\n",
    "            break\n",
    "\n",
    "        most_correlated_pair = None\n",
    "        max_corr = -1\n",
    "        for i in range(len(cols)):\n",
    "            for j in range(i + 1, len(cols)):\n",
    "                if np.abs(corr_matrix.loc[cols[i], cols[j]]) > max_corr:\n",
    "                    max_corr = np.abs(corr_matrix.loc[cols[i], cols[j]])\n",
    "                    most_correlated_pair = (cols[i], cols[j])\n",
    "\n",
    "        if most_correlated_pair:\n",
    "            col1, col2 = most_correlated_pair\n",
    "            indices_to_remove = []\n",
    "            for region in dense_regions:\n",
    "                region_filter = True\n",
    "                for i, val in enumerate(region):\n",
    "                    region_filter &= df_discrete.iloc[:, i] == val\n",
    "                region_indices = df_subsampled[region_filter].index.tolist()\n",
    "                if region_indices:\n",
    "                    sub_df = df_subsampled.loc[region_indices]\n",
    "                    if not sub_df.empty:\n",
    "                        sub_df[\"correlation_contribution\"] = (\n",
    "                            sub_df[col1] - sub_df[col1].mean()\n",
    "                        ) * (sub_df[col2] - sub_df[col2].mean())\n",
    "                        indices_to_remove.extend(\n",
    "                            sub_df.sort_values(\n",
    "                                by=\"correlation_contribution\", ascending=False\n",
    "                            )\n",
    "                            .head(int(len(sub_df) * removal_fraction))\n",
    "                            .index.tolist()\n",
    "                        )\n",
    "\n",
    "            if indices_to_remove:\n",
    "                df_subsampled = df_subsampled.drop(\n",
    "                    index=list(set(indices_to_remove))\n",
    "                ).reset_index(drop=True)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        if df_subsampled.empty:\n",
    "            break\n",
    "\n",
    "    return df_subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "good_stats_small_subsampled = subsample(\n",
    "    good_stats_small,\n",
    "    cols=corr_cols,\n",
    "    target_max_corr=0.0,\n",
    "    grid_bins=4,\n",
    "    removal_fraction=0.99,\n",
    ")\n",
    "\n",
    "len(good_stats_small_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "_ = sns.histplot(\n",
    "    data=good_stats_small_subsampled,\n",
    "    x=\"coverage\",\n",
    "    hue=\"is_cyclic\",\n",
    "    binwidth=0.01,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"% Coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small_subsampled,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\"Compression ratio vs. Grammar HParams\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small_subsampled,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"coverage\",\n",
    "        palette=sns.color_palette(\"ch:s=.25,rot=-.25\", as_cmap=True),\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\"Compression ratio vs. Grammar HParams\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_corr_small_sub = good_stats_small_subsampled[\n",
    "    [\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "    ]\n",
    "].corr()\n",
    "hyp_mask_small_sub = np.triu(np.ones_like(hyp_corr_small_sub, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "_ = sns.heatmap(\n",
    "    hyp_corr_small_sub,\n",
    "    mask=hyp_mask_small_sub,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_title(\n",
    "    f\"Correlation of HParams ≤ 100, Subsampled  (n={len(good_stats_small_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0, sharex=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0, sharex=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_terminals\"\n",
    "x_tasks = [\"n_nonterminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "a_max = 0\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small_subsampled, x=x_tasks[i], y=y_task, ax=ax, hue=\"is_cyclic\"\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    if ax.get_xlim()[1] > a_max:\n",
    "        a_max = ax.get_xlim()[1]\n",
    "    if ax.get_ylim()[1] > a_max:\n",
    "        a_max = ax.get_ylim()[1]\n",
    "\n",
    "axes[0].set_xlim(0, a_max)\n",
    "axes[0].set_ylim(0, a_max)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"n_terminalals vs. other HParams (Subsampled, n={len(good_stats_small_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_nonterminals\"\n",
    "x_tasks = [\"n_terminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "a_max = 0\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=good_stats_small_subsampled, x=x_tasks[i], y=y_task, ax=ax, hue=\"is_cyclic\"\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "    if ax.get_xlim()[1] > a_max:\n",
    "        a_max = ax.get_xlim()[1]\n",
    "    if ax.get_ylim()[1] > a_max:\n",
    "        a_max = ax.get_ylim()[1]\n",
    "\n",
    "axes[0].set_xlim(0, a_max)\n",
    "axes[0].set_ylim(0, a_max)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"n_nonterminals vs. other HParams (Subsampled, n={len(good_stats_small_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_subsample(\n",
    "    df: pd.DataFrame, end_points: int, cols: list[str], seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    data = df[cols].to_numpy()\n",
    "    data_scaled = (data - data.min(axis=0)) / (\n",
    "        data.max(axis=0) - data.min(axis=0) + 1e-8\n",
    "    )\n",
    "\n",
    "    # start with a random subset\n",
    "    current_idx = np.random.choice(len(df), size=end_points, replace=False)\n",
    "\n",
    "    def mean_pairwise_dist(subset: np.ndarray) -> float:\n",
    "        diffs = subset[:, np.newaxis, :] - subset[np.newaxis, :, :]  # (n, n, d)\n",
    "        dists = np.sqrt(np.sum(diffs**2, axis=-1))  # (n, n)\n",
    "        return np.sum(dists) / (len(subset) * (len(subset) - 1) + 1e-8)\n",
    "\n",
    "    def objective(indices: np.ndarray):\n",
    "        subset = data_scaled[indices]\n",
    "        corr = np.corrcoef(subset, rowvar=False)\n",
    "        corr[np.isnan(corr)] = 0\n",
    "        off_diag_corr = np.sum(np.abs(corr - np.eye(len(cols))))\n",
    "        # coverage term: maximize mean pairwise distance\n",
    "        mean_dist = mean_pairwise_dist(subset)\n",
    "        return off_diag_corr - mean_dist  # lower is better\n",
    "\n",
    "    # greedy local search\n",
    "    for _ in range(1000):\n",
    "        i = np.random.randint(end_points)\n",
    "        j = np.random.randint(len(df))\n",
    "        if j in current_idx:\n",
    "            continue\n",
    "        new_idx = current_idx.copy()\n",
    "        new_idx[i] = j\n",
    "        if objective(new_idx) < objective(current_idx):\n",
    "            current_idx = new_idx\n",
    "\n",
    "    return df.iloc[np.unique(current_idx)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "df_new_subsampled = new_subsample(\n",
    "    good_stats_small,\n",
    "    end_points=100,\n",
    "    cols=corr_cols,\n",
    ")\n",
    "\n",
    "len(df_new_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hpy_corr = df_new_subsampled[corr_cols].corr()\n",
    "new_hpy_corr_mask = np.triu(np.ones_like(new_hpy_corr, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "_ = sns.heatmap(\n",
    "    new_hpy_corr,\n",
    "    mask=new_hpy_corr_mask,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    fmt=\".2f\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_title(\n",
    "    f\"Correlation of HParams ≤ 100, Subsampled  (n={len(df_new_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3.3))\n",
    "gs = gridspec.GridSpec(1, 3, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0, sharex=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0, sharex=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "y_task = \"n_terminals\"\n",
    "x_tasks = [\"n_nonterminals\", \"n_lexical_productions\", \"n_nonlexical_productions\"]\n",
    "\n",
    "a_max = 0\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=df_new_subsampled, x=x_tasks[i], y=y_task, ax=ax, hue=\"is_cyclic\"\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    if ax.get_xlim()[1] > a_max:\n",
    "        a_max = ax.get_xlim()[1]\n",
    "    if ax.get_ylim()[1] > a_max:\n",
    "        a_max = ax.get_ylim()[1]\n",
    "\n",
    "axes[0].set_xlim(0, a_max)\n",
    "axes[0].set_ylim(0, a_max)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "_ = fig.suptitle(\n",
    "    f\"n_terminalals vs. other HParams (Subsampled, n={len(df_new_subsampled)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2.5))\n",
    "gs = gridspec.GridSpec(1, 4, wspace=0.1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1], sharey=ax0)\n",
    "ax2 = fig.add_subplot(gs[0, 2], sharey=ax0)\n",
    "ax3 = fig.add_subplot(gs[0, 3], sharey=ax0)\n",
    "\n",
    "axes = [ax0, ax1, ax2, ax3]\n",
    "\n",
    "y_task = \"compression_ratio\"\n",
    "x_tasks = [\n",
    "    \"n_terminals\",\n",
    "    \"n_nonterminals\",\n",
    "    \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.scatterplot(\n",
    "        data=df_new_subsampled,\n",
    "        x=x_tasks[i],\n",
    "        y=y_task,\n",
    "        ax=ax,\n",
    "        hue=\"is_cyclic\",\n",
    "    )\n",
    "    ax.set_xlabel(x_tasks[i])\n",
    "    ax.set_ylabel(y_task)\n",
    "\n",
    "for ax in axes[1:]:\n",
    "    plt.setp(ax.get_yticklabels(), visible=False)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.legend_.remove()\n",
    "\n",
    "_ = fig.suptitle(f\"Compression ratio vs. Grammar HParams (n={len(df_new_subsampled)})\")\n",
    "_ = axes[0].set_ylabel(\"Compression Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "_ = sns.histplot(\n",
    "    data=df_new_subsampled,\n",
    "    x=\"coverage\",\n",
    "    hue=\"is_cyclic\",\n",
    "    binwidth=0.01,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xlabel(\"% Coverage\")\n",
    "_ = ax.set_title(f\"Coverage Histogram (n={len(df_new_subsampled)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
