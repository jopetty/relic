{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import lark\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import seaborn as sns\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "from formal_gym import grammar as fg_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")\n",
    "\n",
    "# grammar_name = \"grammar_20250312172959_597104\"\n",
    "grammar_name = \"grammar_20250319112222_631725\"\n",
    "grammar_path = PROJECT_ROOT / \"data\" / \"grammars\" / grammar_name / f\"{grammar_name}.cfg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = fg_grammar.Grammar.from_file(grammar_path)\n",
    "grammar.as_cfg.productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grammar.terminals)\n",
    "\n",
    "import random\n",
    "\n",
    "new_sample = \" \".join(random.choices(list(grammar.terminals), k=10))\n",
    "print(new_sample)\n",
    "\n",
    "print(grammar.test_sample(new_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg_to_lark(grammar: fg_grammar.Grammar) -> str:\n",
    "    out_str = \"\"\n",
    "    for production in grammar.as_cfg.productions():\n",
    "        lhs = production.lhs()\n",
    "        if str(lhs) == \"S\":\n",
    "            lhs = \"start\"\n",
    "        rhs_pre = production.rhs()\n",
    "        rhs = []\n",
    "        for r in rhs_pre:\n",
    "            if isinstance(r, str):\n",
    "                rhs.append(f'\"{r}\"')\n",
    "            elif isinstance(r, nltk.grammar.Nonterminal):\n",
    "                rhs.append(f\"{r}\")\n",
    "        rhs = \" \".join(sym for sym in rhs)\n",
    "        out_str += f\"{lhs} : {rhs}\\n\"\n",
    "    return out_str\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def convert_cfg_to_ebnf(grammar: fg_grammar.Grammar) -> str:\n",
    "    rules = defaultdict(set)\n",
    "\n",
    "    for production in grammar.as_cfg.productions():\n",
    "        rules[production.lhs()].add(production.rhs())\n",
    "\n",
    "    lark_rules = []\n",
    "    for lhs, rhs_set in rules.items():\n",
    "        rhs_rules = []\n",
    "        for rhs in rhs_set:\n",
    "            rhs_syms = []\n",
    "            for s in rhs:\n",
    "                if isinstance(s, str):\n",
    "                    rhs_syms.append(f'\"{s}\"')\n",
    "                elif isinstance(s, nltk.grammar.Nonterminal):\n",
    "                    rhs_syms.append(f\"{str(s).lower()}\")\n",
    "            rhs_string = \" \".join(rhs_syms)\n",
    "            rhs_rules.append(rhs_string)\n",
    "        print(rhs_rules)\n",
    "        lark_rhs = \" | \".join(s for s in list(rhs_rules))\n",
    "        if str(lhs) == \"S\":\n",
    "            lhs = \"start\"\n",
    "        lark_rules.append(f\"{str(lhs).lower()} : {lark_rhs}\")\n",
    "\n",
    "    g_dir = \"%import common.WS_INLINE\\n%ignore WS_INLINE\"\n",
    "    return \"\\n\".join(lark_rules) + \"\\n\" + g_dir\n",
    "\n",
    "\n",
    "lark_g = convert_cfg_to_ebnf(grammar)\n",
    "print(lark_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lark_parser = lark.Lark(lark_g, ambiguity=\"explicit\")\n",
    "lark_parser.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_parser = nltk.ChartParser(grammar.as_cfg)\n",
    "recdescent_parser = nltk.RecursiveDescentParser(grammar.as_cfg)\n",
    "shift_reduce_parser = nltk.ShiftReduceParser(grammar.as_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_sample = \"t0 t0 t0 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3 t3\".split(\" \")\n",
    "sample_with_parse = grammar.generate_tree()\n",
    "positive_sample = sample_with_parse[\"string\"].split(\" \")\n",
    "print(sample_with_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_tree = lark_parser.parse(sample_with_parse[\"string\"])\n",
    "len(parse_tree.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_with_lark(sample: str):\n",
    "#     try:\n",
    "#         return lark_parser.parse(sample)\n",
    "#     except lark.exceptions.LarkError as e:\n",
    "#         # print(f\"Error parsing sample: {sample}\")\n",
    "#         # print(e)\n",
    "#         return None\n",
    "\n",
    "# parse_with_lark(\"t0 t0 t0 t3\")\n",
    "# parse_with_lark(\"t3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Token, Tree\n",
    "\n",
    "\n",
    "def clean_tree(tree, in_string: str) -> str:\n",
    "    clean_parses = []\n",
    "\n",
    "    for parse_tree in tree.children:\n",
    "        labels = in_string.split()  # tokens in original order\n",
    "\n",
    "        def recurse(node):\n",
    "            children = \" \".join(recurse(child) for child in node.children)\n",
    "            if children == \"\":\n",
    "                children = labels.pop(0)\n",
    "            node_label = node.data.upper()\n",
    "            if node_label == \"START\":\n",
    "                node_label = \"S\"\n",
    "            return f\"({node_label} {children})\"\n",
    "\n",
    "        clean_parses.append(recurse(parse_tree))\n",
    "\n",
    "    return clean_parses\n",
    "\n",
    "\n",
    "print(clean_tree(parse_tree, sample_with_parse[\"string\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_parses = chart_parser.parse(positive_sample)\n",
    "for p in c_parses:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_parses = shift_reduce_parser.parse(positive_sample)\n",
    "\n",
    "for p in sr_parses:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_parses = recdescent_parser.parse(positive_sample)\n",
    "\n",
    "for p in rd_parses:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_parses = chart_parser.parse(positive_sample)\n",
    "\n",
    "n = 0\n",
    "for p in c_parses:\n",
    "    n += 1\n",
    "    print(p)\n",
    "\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
