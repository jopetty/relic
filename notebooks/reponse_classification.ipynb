{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Response Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sk_metrics\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "from openai import OpenAI\n",
    "\n",
    "from formal_gym import prompt as fg_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")\n",
    "\n",
    "load_dotenv(os.path.join(PROJECT_ROOT, \".env\"))\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Plotting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURES_DIR = PROJECT_ROOT / \"notebooks\" / \"figures\"\n",
    "\n",
    "PAPER_WIDTH_IN = 5.5\n",
    "\n",
    "rcs = {\n",
    "    \"font.size\": 10.0,\n",
    "    \"axes.labelsize\": \"small\",\n",
    "    \"axes.titlesize\": \"small\",\n",
    "    \"xtick.labelsize\": \"x-small\",\n",
    "    \"ytick.labelsize\": \"x-small\",\n",
    "}\n",
    "\n",
    "\n",
    "def darken(\n",
    "    color: str\n",
    "    | tuple[float, float, float]\n",
    "    | dict[str, Any]\n",
    "    | sns.palettes._ColorPalette,\n",
    "    by: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Darken a color by provided amount.\n",
    "    \"\"\"\n",
    "\n",
    "    def _darken_color(c: str | tuple[float, float, float], by: float):\n",
    "        by = min(max(0, by), 1)\n",
    "        pct_darken = 1 - by\n",
    "\n",
    "        if isinstance(c, str):\n",
    "            c = sns.color_palette([c])[0]\n",
    "\n",
    "        for c_i in c:\n",
    "            if c_i > 1:\n",
    "                c_i /= 255\n",
    "        c_hls = colorsys.rgb_to_hls(c[0], c[1], c[2])\n",
    "        # Darken the color by reducing the lightness\n",
    "\n",
    "        c_hls = (\n",
    "            c_hls[0],  # hue\n",
    "            c_hls[1] * pct_darken,  # lightness\n",
    "            c_hls[2],  # saturation\n",
    "        )\n",
    "        # Convert back to RGB\n",
    "        c_rgb = colorsys.hls_to_rgb(c_hls[0], c_hls[1], c_hls[2])\n",
    "        return c_rgb\n",
    "\n",
    "    if isinstance(color, dict):\n",
    "        # If color is a dictionary, assume it's a palette\n",
    "        # and darken each color in the palette\n",
    "        return {k: _darken_color(v, by) for k, v in color.items()}\n",
    "    elif isinstance(color, sns.palettes._ColorPalette):\n",
    "        colors = [_darken_color(c, by) for c in color]\n",
    "        return sns.palettes._ColorPalette(colors)\n",
    "    else:\n",
    "        return _darken_color(color, by)\n",
    "\n",
    "\n",
    "# For heatmaps, correlations, -1 to 1 scales, etc\n",
    "CMAP_HEATMAP = \"vlag\"\n",
    "\n",
    "# For any plots where color differentiates sample type\n",
    "PALETTE_SAMPLE_TYPE = {\n",
    "    \"positive\": darken(\"#ffcc66\"),\n",
    "    \"negative\": \"#5c5cff\",\n",
    "    \"unknown\": \"#C41E3A\",\n",
    "}\n",
    "\n",
    "# For any plots where color differentiates model\n",
    "PALETTE_MODEL = darken(\n",
    "    {\n",
    "        \"gpt-4.1-nano\": sns.cubehelix_palette(start=0.5, rot=-0.5, n_colors=4)[0],\n",
    "        \"gpt-4.1-mini\": sns.cubehelix_palette(start=0.5, rot=-0.5, n_colors=4)[1],\n",
    "        \"gpt-4.1\": sns.cubehelix_palette(start=0.5, rot=-0.5, n_colors=4)[2],\n",
    "        \"o4-mini\": sns.color_palette(\"YlOrBr\", n_colors=2)[0],\n",
    "        \"o3\": sns.color_palette(\"YlOrBr\", n_colors=2)[1],\n",
    "        \"gemma-3-1b\": sns.cubehelix_palette(n_colors=5)[0],\n",
    "        \"gemma-3-4b\": sns.cubehelix_palette(n_colors=5)[1],\n",
    "        \"gemma-3-12b\": sns.cubehelix_palette(n_colors=5)[2],\n",
    "        \"gemma-3-27b\": sns.cubehelix_palette(n_colors=5)[3],\n",
    "        \"DSR1-7B\": sns.color_palette(\"cool\", n_colors=2)[0],\n",
    "    }\n",
    ")\n",
    "\n",
    "PALETTE_SCORE = {\n",
    "    \"Weighted F1\": sns.color_palette(\"terrain\", n_colors=2, desat=0.8)[0],\n",
    "    \"Macro F1\": sns.color_palette(\"terrain\", n_colors=2, desat=0.8)[1],\n",
    "}\n",
    "\n",
    "PALETTE_STRAGETY = {\n",
    "    \"rule-based\": \"#942822\",\n",
    "    \"heuristic\": \"#FFE7CE\",\n",
    "    \"code\": sns.color_palette(\"gist_earth\", n_colors=4)[2],\n",
    "    \"unknown\": sns.color_palette(\"gist_earth\", n_colors=4)[3],\n",
    "}\n",
    "\n",
    "PALETTES = {\n",
    "    \"model\": PALETTE_MODEL,\n",
    "    \"sample_type\": PALETTE_SAMPLE_TYPE,\n",
    "    \"score\": PALETTE_SCORE,\n",
    "    \"strategy\": PALETTE_STRAGETY,\n",
    "}\n",
    "\n",
    "# For marking at-chance baselines\n",
    "COLOR_AT_CHANCE = \"#ff0000\"  # Red\n",
    "ALPHA_AT_CHANCE = 0.5\n",
    "\n",
    "# Bar Chart settings\n",
    "BAR_EDGE_COLOR = \"black\"\n",
    "BAR_EDGE_WIDTH = 0.8\n",
    "\n",
    "\n",
    "def display_palette(palette: dict[str, Any] | sns.palettes._ColorPalette):\n",
    "    if isinstance(palette, sns.palettes._ColorPalette):\n",
    "        display(palette)\n",
    "    else:\n",
    "        colors = list(palette.values())\n",
    "        display(sns.color_palette(colors))\n",
    "\n",
    "\n",
    "def filter_by_alpha(\n",
    "    keys: list[str],\n",
    "    ax,\n",
    "    palette: dict[str, Any] | None = None,\n",
    "    alpha=0.3,\n",
    "    highlight: str | list[str] | None = None,\n",
    "):\n",
    "    alphas = defaultdict(lambda: alpha)\n",
    "    if highlight is not None:\n",
    "        if isinstance(highlight, str):\n",
    "            highlight = [highlight]\n",
    "        for h in highlight:\n",
    "            alphas[h] = 1\n",
    "\n",
    "    if palette is None:\n",
    "        # look through all the palettes in PALETTE; find one whose keys match\n",
    "        # the keys passed in; if found, use that palette; otherwise, throw an error\n",
    "        for palette_name, palette_dict in PALETTES.items():\n",
    "            if all(k in palette_dict for k in keys):\n",
    "                palette = palette_dict\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"No matching palette found for keys: {keys}\")\n",
    "\n",
    "    face_colors = ax.collections[0].get_facecolors()\n",
    "    face_colors[:, 3] = alpha\n",
    "    for key in keys:\n",
    "        key_color = palette[key]\n",
    "\n",
    "        # Get indices of face_colors whose first 3 values match the model color\n",
    "        indices = [\n",
    "            i\n",
    "            for i, color in enumerate(face_colors)\n",
    "            if (color[0], color[1], color[2]) == key_color[:3]\n",
    "        ]\n",
    "        for i in indices:\n",
    "            face_colors[i][3] = alphas[key]\n",
    "    ax.collections[0].set_facecolor(face_colors)\n",
    "\n",
    "\n",
    "def legend_format(\n",
    "    ax: mpl.axes._axes.Axes | sns.FacetGrid,\n",
    "    keys: list[str] | None = None,\n",
    "    title: str | None = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if isinstance(ax, sns.FacetGrid):\n",
    "        fg = ax\n",
    "        ax = fg.ax\n",
    "\n",
    "        _ = fg.legend.remove()\n",
    "\n",
    "    # Legend Formatting\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    if keys is not None:\n",
    "        if \"gpt-4.1\" in keys:\n",
    "            spacing_locs = [3, 6]\n",
    "        else:\n",
    "            ValueError(f\"Unsure how to space legend for {keys=}\")\n",
    "\n",
    "        spacer = mpatches.Patch(alpha=0, linewidth=0)\n",
    "        for sloc in spacing_locs:\n",
    "            handles.insert(sloc, spacer)\n",
    "            labels.insert(sloc, \"\")\n",
    "\n",
    "    _ = ax.legend(handles, labels)\n",
    "    _ = ax.get_legend().set_frame_on(False)\n",
    "\n",
    "    if title is not None:\n",
    "        _ = ax.get_legend().set_title(title)\n",
    "\n",
    "    if \"loc\" not in kwargs:\n",
    "        kwargs[\"loc\"] = \"upper left\"\n",
    "    if \"bbox_to_anchor\" not in kwargs:\n",
    "        kwargs[\"bbox_to_anchor\"] = (1, 1)\n",
    "    _ = sns.move_legend(ax, **kwargs)\n",
    "\n",
    "\n",
    "# MARK: Printing, experimentation, etc\n",
    "display(sns.color_palette(\"terrain\", n_colors=2, desat=0.8))\n",
    "display(darken(sns.color_palette(\"terrain\", n_colors=2, desat=0.8), by=0.2))\n",
    "\n",
    "display_palette(PALETTE_MODEL)\n",
    "display_palette(PALETTE_SAMPLE_TYPE)\n",
    "display_palette(PALETTE_STRAGETY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMARS_DIR = PROJECT_ROOT / \"data\" / \"grammars\"\n",
    "\n",
    "response_df_file = PROJECT_ROOT / \"data\" / \"response_df.feather\"\n",
    "f1_df_file = PROJECT_ROOT / \"data\" / \"f1_df.feather\"\n",
    "acc_df_file = PROJECT_ROOT / \"data\" / \"acc_df.feather\"\n",
    "\n",
    "response_df = None\n",
    "accuracy_df = None\n",
    "f1_df = None\n",
    "\n",
    "if os.path.exists(response_df_file):\n",
    "    print(f\"Loading `response_df` from {response_df_file}\")\n",
    "    response_df = pd.read_feather(\n",
    "        response_df_file,\n",
    "    )\n",
    "\n",
    "if os.path.exists(f1_df_file):\n",
    "    print(f\"Loading `f1_df` from {f1_df_file}\")\n",
    "    f1_df = pd.read_feather(\n",
    "        f1_df_file,\n",
    "    )\n",
    "\n",
    "if os.path.exists(acc_df_file):\n",
    "    print(f\"Loading `accuracy_df` from {acc_df_file}\")\n",
    "    accuracy_df = pd.read_feather(\n",
    "        acc_df_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## DSR1 Overthinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsr1_acc_df = accuracy_df[accuracy_df.model == \"DSR1-7B\"].copy()\n",
    "dsr1_acc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times \"wait\" appears in the model_response column\n",
    "\n",
    "dsr1_acc_df[\"wait_count\"] = dsr1_acc_df[\"model_response\"].str.count(\n",
    "    r\"\\bwait\\b\", flags=re.IGNORECASE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_height = 0.8\n",
    "\n",
    "fig = plt.figure(figsize=(PAPER_WIDTH_IN, fig_height))\n",
    "grid = fig.add_gridspec(1, 2, wspace=0.05, width_ratios=[1.3, 2])\n",
    "\n",
    "with sns.plotting_context(\"paper\", rc=rcs):\n",
    "    for c in range(2):\n",
    "        ax = fig.add_subplot(grid[0, c])\n",
    "        if c == 0:\n",
    "            sns.lineplot(\n",
    "                dsr1_acc_df,\n",
    "                x=\"sample.length\",\n",
    "                y=\"wait_count\",\n",
    "                color=PALETTE_MODEL[\"DSR1-7B\"],\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_xlabel(\"Task Complexity (Sample Length)\", ha=\"left\", x=0.0)\n",
    "            ax.set_ylabel(\"“Wait” Count\")\n",
    "            ax.set_yticks([0, 5, 10, 15, 20])\n",
    "        else:\n",
    "            sns.lineplot(\n",
    "                dsr1_acc_df,\n",
    "                x=\"n_nonlexical_productions\",\n",
    "                y=\"wait_count\",\n",
    "                color=PALETTE_MODEL[\"DSR1-7B\"],\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_ylabel(None)\n",
    "            ax.set_xlabel(\n",
    "                \"Instruction Set Complexity (# of Nonlexical Productions)\",\n",
    "                ha=\"left\",\n",
    "                x=0.0,\n",
    "            )\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xscale(\"log\")\n",
    "            ax.set_xticks([10, 100])\n",
    "            ax.set_xticklabels([10, 100])\n",
    "            ax.set_ylim(0, 20)\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1)\n",
    "    plt.savefig(\n",
    "        FIGURES_DIR / \"dsr1_wait_count.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## GPT-4.1 Strategy Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_df = accuracy_df[\n",
    "    accuracy_df[\"model\"].isin([\"gpt-4.1-nano\", \"gpt-4.1-mini\", \"gpt-4.1\"])\n",
    "].copy()[\n",
    "    [\n",
    "        \"model_response\",\n",
    "        \"sample\",\n",
    "        \"sample.type.ground_truth\",\n",
    "        \"correct\",\n",
    "        \"sample.length\",\n",
    "        \"n_nonlexical_productions\",\n",
    "        \"grammar_file\",\n",
    "        \"model\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "models_to_remove = set(accuracy_df[\"model\"].cat.categories) - set(\n",
    "    [\"gpt-4.1-nano\", \"gpt-4.1-mini\", \"gpt-4.1\"]\n",
    ")\n",
    "\n",
    "gpt_df[\"model\"] = gpt_df[\"model\"].cat.remove_categories(models_to_remove)\n",
    "\n",
    "gpt_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grammar = gpt_df[\"grammar_file\"].unique()[10]\n",
    "print(f\"Test grammar: {test_grammar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpt_acc(grammar_idx: int):\n",
    "    test_grammar = gpt_df[\"grammar_file\"].unique()[grammar_idx]\n",
    "\n",
    "    acc_by_length_df = (\n",
    "        gpt_df[gpt_df[\"grammar_file\"] == test_grammar]\n",
    "        .groupby([\"sample.length\", \"model\"], observed=True)[\"correct\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    with sns.plotting_context(\"paper\", rc=rcs):\n",
    "        fig = plt.figure(figsize=(PAPER_WIDTH_IN, fig_height))\n",
    "        grid = fig.add_gridspec(1, 3, wspace=0.05)\n",
    "\n",
    "        for c, model in enumerate(acc_by_length_df[\"model\"].unique()):\n",
    "            ax = fig.add_subplot(grid[0, c])\n",
    "            sns.lineplot(\n",
    "                acc_by_length_df[acc_by_length_df[\"model\"] == model],\n",
    "                x=\"sample.length\",\n",
    "                y=\"correct\",\n",
    "                color=PALETTE_MODEL[model],\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_title(model)\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "            if c == 0:\n",
    "                ax.set_xlabel(\"Task Complexity (Sample Length)\", ha=\"left\", x=0.0)\n",
    "                ax.set_ylabel(\"Accuracy\")\n",
    "                ax.set_yticks([0, 1])\n",
    "            else:\n",
    "                ax.set_xlabel(None)\n",
    "                ax.set_ylabel(None)\n",
    "                ax.set_yticks([])\n",
    "\n",
    "    return test_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpt_acc(195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR_IDX = 192\n",
    "\n",
    "grammar_name = plot_gpt_acc(GRAMMAR_IDX)\n",
    "# grammar_name = gpt_df[\"grammar_file\"].unique()[GRAMMAR_IDX]\n",
    "print(f\"Grammar name: {grammar_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_192_df = gpt_df[gpt_df[\"grammar_file\"] == grammar_name].reset_index()\n",
    "\n",
    "gpt_192_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_classification_file(grammar_name: str, pct_per_length: float = 0.2):\n",
    "    classification_prompt = \"\"\"You will be presented with a completion from an LLM which was given a context-free grammar and a string of symbols drawn from that grammar's set of terminal symbols and asked to determine whether the string is generated by the grammar or not. Your job is to classify how the LLM attempted to solve the task by binning the completion strategy into one of the following categories:\n",
    "  - `heuristic`: The LLM attempts to solve the task by using heuristics it surmises from the grammar, such as “if the string is long, it is likely generated by the grammar” or “the string only contains terminal symbols present in the grammar, so it’s likely a positive sample”. Count strategies as heuristic if they appeal to the existence of certain production rules but do not rigorously determine that no such derivation exists.\n",
    "  - `rule-based`: The LLM attempts to solve the task by writing out the FULL DERIVATION of the sample from the grammar, or rigorously determining that no such derivation exists. Only count strategies as rule-based if the LLM doesn’t use any guesswork to reach its final conclusion.\n",
    "  - `code`: The LLM attempts to solve the task by writing out a program or algorithm which it claims will solve the task. This includes writing out a program in a programming language, or writing out pseudocode.\n",
    "\n",
    "You can write as much as you want in your answer, but please end your response with the name of the classification you think is most appropriate.\n",
    "\n",
    "Here is the LLM's completion:\n",
    "\n",
    "```\n",
    "{completion}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "    grammar_df = gpt_df[gpt_df[\"grammar_file\"] == grammar_name].reset_index(drop=True)\n",
    "\n",
    "    subsampled_df = (\n",
    "        grammar_df.groupby(\n",
    "            [\"sample.length\", \"sample.type.ground_truth\", \"model\"], observed=True\n",
    "        )[\n",
    "            [\n",
    "                \"model_response\",\n",
    "                \"sample.type.ground_truth\",\n",
    "                \"correct\",\n",
    "                \"sample.length\",\n",
    "                \"grammar_file\",\n",
    "                \"sample\",\n",
    "                \"model\",\n",
    "            ]\n",
    "        ]\n",
    "        .apply(lambda x: x.sample(frac=pct_per_length, random_state=42))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    subsampled_df[\"classification_prompt\"] = subsampled_df[\"model_response\"].map(\n",
    "        lambda x: classification_prompt.format(completion=x)\n",
    "    )\n",
    "\n",
    "    subsampled_df[\"api_request\"] = subsampled_df.apply(\n",
    "        lambda row: fg_prompt.ChatCompletionResponse(\n",
    "            user_prompt=row[\"classification_prompt\"],\n",
    "            metadata={\n",
    "                \"response_model\": row[\"model\"],\n",
    "                \"sample.length\": row[\"sample.length\"],\n",
    "                \"sample.type.ground_truth\": row[\"sample.type.ground_truth\"],\n",
    "                \"sample\": row[\"sample\"],\n",
    "                \"correct\": row[\"correct\"],\n",
    "                \"grammar_file\": row[\"grammar_file\"],\n",
    "            },\n",
    "        ).to_openai_batched_json(\n",
    "            model=\"o4-mini\",\n",
    "            custom_id=str(row.name),\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    classification_filename = f\"{grammar_name}_gpt4.1_classification.jsonl\"\n",
    "    batch_jsonl_path = (\n",
    "        PROJECT_ROOT / \"data\" / \"grammars\" / grammar_name / classification_filename\n",
    "    )\n",
    "    with open(batch_jsonl_path, \"w\") as f:\n",
    "        for j in subsampled_df[\"api_request\"]:\n",
    "            f.write(f\"{j}\\n\")\n",
    "\n",
    "    return classification_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammars_data = []\n",
    "\n",
    "grammar_names = (\n",
    "    gpt_df[\"grammar_file\"].drop_duplicates().sample(frac=0.2, random_state=42)\n",
    ")\n",
    "\n",
    "for g in grammar_names:\n",
    "    grammars_data.append(\n",
    "        {\n",
    "            \"grammar_name\": g,\n",
    "            \"classification_filename\": create_batch_classification_file(g),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_classification_batch(\n",
    "    batch_jsonl_path: str,\n",
    "):\n",
    "    file_response = client.files.create(\n",
    "        file=open(batch_jsonl_path, \"rb\"), purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=file_response.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"batch\": batch,\n",
    "        \"file\": file_response,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, g in enumerate(grammars_data):\n",
    "#     print(f\"Submitting batch {i+1}/{len(grammars_data)}\")\n",
    "\n",
    "#     batch_jsonl_path = (\n",
    "#         PROJECT_ROOT / \"data\" / \"grammars\" / g[\"grammar_name\"] / g[\"classification_filename\"]\n",
    "#     )\n",
    "\n",
    "#     response = submit_classification_batch(\n",
    "#         batch_jsonl_path=batch_jsonl_path\n",
    "#     )\n",
    "#     grammars_data[i][\"response\"] = response\n",
    "#     print(f\"Batch ID: {response['batch'].id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in grammars_data:\n",
    "    g[\"batch_id\"] = g[\"response\"][\"batch\"].id\n",
    "\n",
    "grammars_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_classification_batches(\n",
    "    grammars_data: list[dict[str, Any]],\n",
    "):\n",
    "    for g in grammars_data:\n",
    "        input_filename = g[\"classification_filename\"]\n",
    "        preds_filename = input_filename[:-6] + \"-preds.jsonl\"\n",
    "        output_path = (\n",
    "            PROJECT_ROOT / \"data\" / \"grammars\" / g[\"grammar_name\"] / preds_filename\n",
    "        )\n",
    "        if not output_path.exists():\n",
    "            batch_id = g[\"batch_id\"]\n",
    "\n",
    "            # print(batch_id)\n",
    "\n",
    "            batch = client.batches.retrieve(batch_id)\n",
    "            if batch.status == \"completed\":\n",
    "                results = client.files.content(batch.output_file_id)\n",
    "                with open(output_path, \"w\") as f:\n",
    "                    f.write(results.text)\n",
    "                print(f\"Results saved to {output_path}\")\n",
    "            else:\n",
    "                print(f\"{batch_id} status: {batch.status}\")\n",
    "        else:\n",
    "            print(f\"File {output_path} already exists, skipping download.\")\n",
    "\n",
    "\n",
    "download_classification_batches(grammars_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_response_classifications() -> pd.DataFrame:\n",
    "    GRAMMARS_DIR = PROJECT_ROOT / \"data\" / \"grammars\"\n",
    "    response_file_pattern = \"*gpt4.1_classification-preds.jsonl\"\n",
    "    submission_file_pattern = \"*gpt4.1_classification.jsonl\"\n",
    "    submission_files = list(GRAMMARS_DIR.rglob(submission_file_pattern))\n",
    "    response_files = list(GRAMMARS_DIR.rglob(response_file_pattern))\n",
    "\n",
    "    response_dfs = []\n",
    "    for response_file in response_files:\n",
    "        with open(response_file, \"r\") as f:\n",
    "            cat_df = pd.read_json(f, lines=True)\n",
    "            cat_struct = json.loads(cat_df.to_json(orient=\"records\"))\n",
    "            cat_df = pd.json_normalize(cat_struct)\n",
    "            response_dfs.append(cat_df)\n",
    "\n",
    "    response_df = pd.concat(response_dfs, ignore_index=True)\n",
    "\n",
    "    submission_dfs = []\n",
    "    for submission_file in submission_files:\n",
    "        with open(submission_file, \"r\") as f:\n",
    "            sub_df = pd.read_json(f, lines=True)\n",
    "            sub_struct = json.loads(sub_df.to_json(orient=\"records\"))\n",
    "            sub_df = pd.json_normalize(sub_struct)\n",
    "            submission_dfs.append(sub_df)\n",
    "    submission_df = pd.concat(submission_dfs, ignore_index=True)\n",
    "\n",
    "    classification_df = pd.merge(\n",
    "        submission_df,\n",
    "        response_df,\n",
    "        on=\"custom_id\",\n",
    "        suffixes=(\"_submission\", \"_response\"),\n",
    "    )\n",
    "\n",
    "    classification_df[\"model\"] = classification_df[\"body.metadata.response_model\"]\n",
    "    classification_df[\"sample.length\"] = classification_df[\n",
    "        \"body.metadata.sample.length\"\n",
    "    ]\n",
    "    classification_df[\"sample.type.ground_truth\"] = classification_df[\n",
    "        \"body.metadata.sample.type.ground_truth\"\n",
    "    ]\n",
    "    classification_df[\"sample\"] = classification_df[\"body.metadata.sample\"]\n",
    "    classification_df[\"correct\"] = classification_df[\"body.metadata.correct\"]\n",
    "    classification_df[\"grammar_file\"] = classification_df[\"body.metadata.grammar_file\"]\n",
    "    classification_df[\"prompt\"] = classification_df[\"body.messages\"].apply(\n",
    "        lambda x: x[0][\"content\"]\n",
    "    )\n",
    "    classification_df[\"response\"] = classification_df[\"response.body.choices\"].apply(\n",
    "        lambda x: x[0][\"message\"][\"content\"]\n",
    "    )\n",
    "\n",
    "    classification_df = classification_df[\n",
    "        [\n",
    "            \"custom_id\",\n",
    "            \"model\",\n",
    "            \"sample.length\",\n",
    "            \"sample.type.ground_truth\",\n",
    "            \"sample\",\n",
    "            \"correct\",\n",
    "            \"response\",\n",
    "            \"grammar_file\",\n",
    "            \"prompt\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    strategy_regex = re.compile(\n",
    "        r\"(?:heuristic|rule[-|‐]based|code)\", flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    classification_df[\"strategy\"] = (\n",
    "        classification_df[\"response\"]\n",
    "        .apply(lambda x: strategy_regex.findall(x))\n",
    "        .apply(lambda x: x[-1] if len(x) > 0 else \"unknown\")\n",
    "        .str.lower()\n",
    "    )\n",
    "\n",
    "    classification_df[\"strategy\"] = classification_df[\"strategy\"].map(\n",
    "        {\n",
    "            \"heuristic\": \"heuristic\",\n",
    "            \"rule-based\": \"rule-based\",\n",
    "            \"code\": \"rule-based\",\n",
    "            \"unknown\": \"unknown\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"submissions\": submission_df.dropna(),\n",
    "        \"classifications\": classification_df.dropna(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = load_response_classifications()\n",
    "submission_df = df_dict[\"submissions\"]\n",
    "classification_df = df_dict[\"classifications\"]\n",
    "\n",
    "submission_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"body.messages\"].iloc[0][0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    classification_df[\n",
    "        (classification_df[\"sample.type.ground_truth\"] == \"positive\")\n",
    "        & (classification_df[\"sample.length\"] < 6)\n",
    "    ]\n",
    "    .query(\"model == 'gpt-4.1-mini'\")\n",
    "    .query(\"correct == True\")\n",
    "    .query(\"strategy == 'rule-based'\")[\n",
    "        [\"sample.length\", \"sample\", \"response\", \"prompt\", \"grammar_file\"]\n",
    "    ]\n",
    ").iloc[100][\"prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df.to_feather(\n",
    "    PROJECT_ROOT / \"data\" / \"gpt_classification_df.feather\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_height = 0.8\n",
    "fig = plt.figure(figsize=(PAPER_WIDTH_IN, fig_height))\n",
    "grid = fig.add_gridspec(1, 3, wspace=0.05)\n",
    "\n",
    "with sns.plotting_context(\"paper\", rc=rcs):\n",
    "    for c, model in enumerate(classification_df[\"model\"].unique()):\n",
    "        ax = fig.add_subplot(grid[0, c])\n",
    "        sns.histplot(\n",
    "            classification_df[classification_df[\"model\"] == model],\n",
    "            x=\"sample.length\",\n",
    "            hue=\"strategy\",\n",
    "            palette=PALETTE_STRAGETY,\n",
    "            stat=\"proportion\",\n",
    "            multiple=\"fill\",\n",
    "            bins=50,\n",
    "            ax=ax,\n",
    "            alpha=0.8,\n",
    "            linewidth=0,\n",
    "            legend=False,\n",
    "        )\n",
    "        ax.set_title(model, fontsize=7)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_xticks([1, 50])\n",
    "        ax.get_xticklabels()[0].set_ha(\"left\")\n",
    "        ax.get_xticklabels()[-1].set_ha(\"right\")\n",
    "\n",
    "        if c == 0:\n",
    "            ax.set_xlabel(\"Task Complexity (Sample Length)\", ha=\"left\", x=0.0)\n",
    "            ax.set_ylabel(\"Proportion\\nof Strategies\")\n",
    "            ax.set_yticks([0, 1])\n",
    "        else:\n",
    "            ax.set_xlabel(None)\n",
    "            ax.set_ylabel(None)\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        if c == 0:\n",
    "            ax.text(\n",
    "                0.4,\n",
    "                0.35,\n",
    "                \"rule-based\",\n",
    "                color=PALETTE_STRAGETY[\"rule-based\"],\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=8,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "            ax.text(\n",
    "                0.95,\n",
    "                0.9,\n",
    "                \"heuristic\",\n",
    "                color=\"#ce8669\",\n",
    "                ha=\"right\",\n",
    "                va=\"top\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=8,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    for o in fig.findobj():\n",
    "        o.set_clip_on(False)\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1)\n",
    "\n",
    "    plt.savefig(\n",
    "        FIGURES_DIR / \"gpt_strategy.pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_192_df[\"classification_prompt\"] = gpt_192_df[\"model_response\"].map(\n",
    "    lambda x: classification_prompt + f\"```\\n{x}\\n```\"\n",
    ")\n",
    "\n",
    "gpt_192_df[\"api_request\"] = gpt_192_df.apply(\n",
    "    lambda row: fg_prompt.ChatCompletionResponse(\n",
    "        user_prompt=row[\"classification_prompt\"],\n",
    "        metadata={\"response_model\": row[\"model\"]},\n",
    "    ).to_openai_batched_json(\n",
    "        model=\"o4-mini\",\n",
    "        custom_id=str(row.name),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "display(gpt_192_df.iloc[0][\"api_request\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_filename = f\"{grammar_name}-classification_o4-mini.jsonl\"\n",
    "batch_jsonl_path = PROJECT_ROOT / \"notebooks\" / \"data\" / classification_filename\n",
    "with open(batch_jsonl_path, \"w\") as f:\n",
    "    for j in gpt_192_df[\"api_request\"]:\n",
    "        f.write(f\"{j}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(pathlib.Path(\"data\") / classification_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_response = client.files.create(file=open(batch_jsonl_path, \"rb\"), purpose=\"batch\")\n",
    "\n",
    "file_id = file_response.id\n",
    "\n",
    "batch = client.batches.create(\n",
    "    input_file_id=file_id, endpoint=\"/v1/chat/completions\", completion_window=\"24h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_6822a0de128881908790210c526cf470\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = client.batches.retrieve(\"batch_6822a0de128881908790210c526cf470\")\n",
    "output = client.files.content(batch.output_file_id)\n",
    "\n",
    "\n",
    "output_file_path = (\n",
    "    PROJECT_ROOT / \"notebooks\" / \"data\" / f\"{batch.id}-classification.jsonl\"\n",
    ")\n",
    "with open(output_file_path, \"w\") as f:\n",
    "    f.write(output.text)\n",
    "# client.batches.retrieve(batch.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open output_file_path and explode the jsonl file, then convert to a dataframe\n",
    "with open(output_file_path, \"r\") as f:\n",
    "    cat_df = pd.read_json(f, lines=True)\n",
    "    cat_struct = json.loads(cat_df.to_json(orient=\"records\"))\n",
    "    cat_df = pd.json_normalize(cat_struct)\n",
    "\n",
    "cat_df[\"model_response\"] = cat_df[\"response.body.choices\"].apply(\n",
    "    lambda x: x[0][\"message\"][\"content\"]\n",
    ")\n",
    "cat_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the words \"heuristic\", \"rule-based\", and \"code\" in the model_response column; take the last match, or if none are found, return \"unknown\"\n",
    "strategy_regex = re.compile(r\"\\b(?:heuristic|rule-based|code)\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "r\"(heuristic|rule-based|code)\"\n",
    "cat_df[\"strategy\"] = (\n",
    "    cat_df[\"model_response\"]\n",
    "    .apply(lambda x: strategy_regex.findall(x))\n",
    "    .apply(lambda x: x[-1] if len(x) > 0 else \"unknown\")\n",
    "    .str.lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df[\"custom_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the gpt_192_df with the cat_df[\"strategy\"] on the shared index\n",
    "gpt_192_df[\"strategy\"] = cat_df[\"strategy\"]\n",
    "gpt_192_df[\"strategy\"] = pd.Categorical(\n",
    "    gpt_192_df[\"strategy\"],\n",
    "    categories=[\"heuristic\", \"rule-based\", \"code\", \"unknown\"],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "gpt_192_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gpt_acc(GRAMMAR_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the proportion of each strategy for each model as a function of sample length\n",
    "\n",
    "fig_height = 2\n",
    "fig = plt.figure(figsize=(PAPER_WIDTH_IN, fig_height))\n",
    "grid = fig.add_gridspec(2, 3, wspace=0.05)\n",
    "\n",
    "acc_by_nlp_df = (\n",
    "    gpt_192_df.groupby([\"n_nonlexical_productions\", \"model\"], observed=True)[\"correct\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "with sns.plotting_context(\"paper\", rc=rcs):\n",
    "    for r in range(2):\n",
    "        for c, model in enumerate(gpt_192_df[\"model\"].unique()):\n",
    "            ax = fig.add_subplot(grid[r, c])\n",
    "            if r == 0:\n",
    "                sns.lineplot(\n",
    "                    acc_by_nlp_df[acc_by_nlp_df[\"model\"] == model],\n",
    "                    x=\"n_nonlexical_productions\",\n",
    "                    y=\"correct\",\n",
    "                    color=PALETTE_MODEL[model],\n",
    "                    ax=ax,\n",
    "                )\n",
    "                ax.set_ylim(0, 1)\n",
    "                ax.set_title(model)\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "                ax.set_xlabel(None)\n",
    "                ax.set_xticks([])\n",
    "\n",
    "                if c == 0:\n",
    "                    ax.set_ylabel(\"Accuracy\")\n",
    "                    ax.set_yticks([0, 1])\n",
    "                else:\n",
    "                    ax.set_ylabel(None)\n",
    "                    ax.set_yticks([])\n",
    "            else:\n",
    "                sns.histplot(\n",
    "                    gpt_192_df[gpt_192_df[\"model\"] == model],\n",
    "                    x=\"n_nonlexical_productions\",\n",
    "                    hue=\"strategy\",\n",
    "                    stat=\"proportion\",\n",
    "                    multiple=\"fill\",\n",
    "                    bins=50,\n",
    "                    ax=ax,\n",
    "                    alpha=0.8,\n",
    "                    linewidth=0,\n",
    "                    # legend=False,\n",
    "                )\n",
    "                ax.set_title(None)\n",
    "                ax.spines[\"top\"].set_visible(False)\n",
    "                ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "                if c == 0:\n",
    "                    ax.set_xlabel(\"Grammar Complexity\", ha=\"left\", x=0.0)\n",
    "                    ax.set_ylabel(\"Proportion\\nof Strategies\")\n",
    "                    ax.set_yticks([0, 1])\n",
    "                else:\n",
    "                    ax.set_xlabel(None)\n",
    "                    ax.set_ylabel(None)\n",
    "                    ax.set_yticks([])\n",
    "\n",
    "                if c == 2:\n",
    "                    sns.move_legend(\n",
    "                        ax,\n",
    "                        loc=\"upper left\",\n",
    "                        bbox_to_anchor=(1, 1),\n",
    "                        title=\"Strategy\",\n",
    "                        title_fontsize=8,\n",
    "                        fontsize=8,\n",
    "                    )\n",
    "                else:\n",
    "                    ax.get_legend().remove()\n",
    "\n",
    "    for o in fig.findobj():\n",
    "        o.set_clip_on(False)\n",
    "    plt.subplots_adjust(left=0, bottom=0, right=1, top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the `gpt_192_df` to contain ten instances of `heuristic` and `rule-based`\n",
    "# Responses\n",
    "\n",
    "# gpt_192_df[[\"model_response\", \"strategy\"]]\n",
    "\n",
    "\n",
    "# shuffle the rows of the dataframe\n",
    "(\n",
    "    gpt_192_df[gpt_192_df[\"strategy\"].isin([\"heuristic\", \"rule-based\"])][\n",
    "        [\"model_response\", \"strategy\"]\n",
    "    ]\n",
    "    .groupby(\n",
    "        [\"strategy\"],\n",
    "        observed=True,\n",
    "    )\n",
    "    .apply(lambda x: x.sample(10, random_state=42), include_groups=True)\n",
    "    .reset_index(drop=True)\n",
    "    # shuffle the rows of the dataframe\n",
    "    .sample(frac=1, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ").to_csv(\n",
    "    PROJECT_ROOT / \"notebooks\" / \"data\" / \"gpt_192_subsample.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_192_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
