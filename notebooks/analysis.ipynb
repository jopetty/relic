{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for Prompting Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sk_metrics\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import display\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURES_DIR = PROJECT_ROOT / \"notebooks\" / \"figures\"\n",
    "\n",
    "\n",
    "def darken(\n",
    "    color: str\n",
    "    | tuple[float, float, float]\n",
    "    | dict[str, Any]\n",
    "    | sns.palettes._ColorPalette,\n",
    "    by: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Darken a color by provided amount.\n",
    "    \"\"\"\n",
    "\n",
    "    def _darken_color(c: str | tuple[float, float, float], by: float):\n",
    "        by = min(max(0, by), 1)\n",
    "        pct_darken = 1 - by\n",
    "\n",
    "        if isinstance(c, str):\n",
    "            c = sns.color_palette([c])[0]\n",
    "\n",
    "        for c_i in c:\n",
    "            if c_i > 1:\n",
    "                c_i /= 255\n",
    "        c_hls = colorsys.rgb_to_hls(c[0], c[1], c[2])\n",
    "        # Darken the color by reducing the lightness\n",
    "\n",
    "        c_hls = (\n",
    "            c_hls[0],  # hue\n",
    "            c_hls[1] * pct_darken,  # lightness\n",
    "            c_hls[2],  # saturation\n",
    "        )\n",
    "        # Convert back to RGB\n",
    "        c_rgb = colorsys.hls_to_rgb(c_hls[0], c_hls[1], c_hls[2])\n",
    "        return c_rgb\n",
    "\n",
    "    if isinstance(color, dict):\n",
    "        # If color is a dictionary, assume it's a palette\n",
    "        # and darken each color in the palette\n",
    "        return {k: _darken_color(v, by) for k, v in color.items()}\n",
    "    elif isinstance(color, sns.palettes._ColorPalette):\n",
    "        colors = [_darken_color(c, by) for c in color]\n",
    "        return sns.palettes._ColorPalette(colors)\n",
    "    else:\n",
    "        return _darken_color(color, by)\n",
    "\n",
    "\n",
    "# For heatmaps, correlations, -1 to 1 scales, etc\n",
    "CMAP_HEATMAP = \"vlag\"\n",
    "\n",
    "# For any plots where color differentiates sample type\n",
    "PALETTE_SAMPLE_TYPE = {\n",
    "    \"positive\": darken(\"#ffcc66\"),\n",
    "    \"negative\": \"#5c5cff\",\n",
    "    \"unknown\": \"#000000\",\n",
    "}\n",
    "\n",
    "# For any plots where color differentiates model\n",
    "PALETTE_MODEL = darken(\n",
    "    {\n",
    "        # \"gpt-4.1-nano\": sns.color_palette(\"rocket_r\", n_colors=3)[0],\n",
    "        # \"gpt-4.1-mini\": sns.color_palette(\"rocket_r\", n_colors=3)[1],\n",
    "        # \"gpt-4.1\": sns.color_palette(\"rocket_r\", n_colors=3)[2],\n",
    "        \"gpt-4.1-nano\": sns.cubehelix_palette(start=0.5, rot=-0.5, n_colors=4)[0],\n",
    "        \"gpt-4.1-mini\": sns.cubehelix_palette(start=0.5, rot=-0.5, n_colors=4)[1],\n",
    "        \"gpt-4.1\": sns.cubehelix_palette(start=0.5, rot=-0.5, n_colors=4)[2],\n",
    "        # \"o4-mini\": sns.color_palette(\"crest\", n_colors=2)[0],\n",
    "        # \"o3\": sns.color_palette(\"crest\", n_colors=2)[1],\n",
    "        \"o4-mini\": sns.color_palette(\"YlOrBr\", n_colors=2)[0],\n",
    "        \"o3\": sns.color_palette(\"YlOrBr\", n_colors=2)[1],\n",
    "        \"gemma-3-1b\": sns.cubehelix_palette(n_colors=5)[0],\n",
    "        \"gemma-3-4b\": sns.cubehelix_palette(n_colors=5)[1],\n",
    "        \"gemma-3-12b\": sns.cubehelix_palette(n_colors=5)[2],\n",
    "        \"gemma-3-27b\": sns.cubehelix_palette(n_colors=5)[3],\n",
    "    }\n",
    ")\n",
    "\n",
    "PALETTE_SCORE = {\n",
    "    \"Weighted F1\": sns.color_palette(\"terrain\", n_colors=2, desat=0.8)[0],\n",
    "    \"Macro F1\": sns.color_palette(\"terrain\", n_colors=2, desat=0.8)[1],\n",
    "}\n",
    "\n",
    "PALETTES = {\n",
    "    \"model\": PALETTE_MODEL,\n",
    "    \"sample_type\": PALETTE_SAMPLE_TYPE,\n",
    "    \"score\": PALETTE_SCORE,\n",
    "}\n",
    "\n",
    "# For marking at-chance baselines\n",
    "COLOR_AT_CHANCE = \"#ff0000\"  # Red\n",
    "ALPHA_AT_CHANCE = 0.5\n",
    "\n",
    "# Bar Chart settings\n",
    "BAR_EDGE_COLOR = \"black\"\n",
    "BAR_EDGE_WIDTH = 0.8\n",
    "\n",
    "\n",
    "def display_palette(palette: dict[str, Any] | sns.palettes._ColorPalette):\n",
    "    if isinstance(palette, sns.palettes._ColorPalette):\n",
    "        display(palette)\n",
    "    else:\n",
    "        colors = list(palette.values())\n",
    "        display(sns.color_palette(colors))\n",
    "\n",
    "\n",
    "def filter_by_alpha(\n",
    "    keys: list[str],\n",
    "    ax,\n",
    "    palette: dict[str, Any] | None = None,\n",
    "    alpha=0.3,\n",
    "    highlight: str | list[str] | None = None,\n",
    "):\n",
    "    alphas = defaultdict(lambda: alpha)\n",
    "    if highlight is not None:\n",
    "        if isinstance(highlight, str):\n",
    "            highlight = [highlight]\n",
    "        for h in highlight:\n",
    "            alphas[h] = 1\n",
    "\n",
    "    if palette is None:\n",
    "        # look through all the palettes in PALETTE; find one whose keys match\n",
    "        # the keys passed in; if found, use that palette; otherwise, throw an error\n",
    "        for palette_name, palette_dict in PALETTES.items():\n",
    "            if all(k in palette_dict for k in keys):\n",
    "                palette = palette_dict\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"No matching palette found for keys: {keys}\")\n",
    "\n",
    "    face_colors = ax.collections[0].get_facecolors()\n",
    "    face_colors[:, 3] = alpha\n",
    "    for key in keys:\n",
    "        key_color = palette[key]\n",
    "\n",
    "        # Get indices of face_colors whose first 3 values match the model color\n",
    "        indices = [\n",
    "            i\n",
    "            for i, color in enumerate(face_colors)\n",
    "            if (color[0], color[1], color[2]) == key_color[:3]\n",
    "        ]\n",
    "        for i in indices:\n",
    "            face_colors[i][3] = alphas[key]\n",
    "    ax.collections[0].set_facecolor(face_colors)\n",
    "\n",
    "\n",
    "def legend_format(\n",
    "    ax: mpl.axes._axes.Axes | sns.FacetGrid,\n",
    "    keys: list[str] | None = None,\n",
    "    title: str | None = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    if isinstance(ax, sns.FacetGrid):\n",
    "        fg = ax\n",
    "        ax = fg.ax\n",
    "\n",
    "        _ = fg.legend.remove()\n",
    "\n",
    "    # Legend Formatting\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    if keys is not None:\n",
    "        if \"gpt-4.1\" in keys:\n",
    "            spacing_locs = [3, 6]\n",
    "        else:\n",
    "            ValueError(f\"Unsure how to space legend for {keys=}\")\n",
    "\n",
    "        spacer = mpatches.Patch(alpha=0, linewidth=0)\n",
    "        for sloc in spacing_locs:\n",
    "            handles.insert(sloc, spacer)\n",
    "            labels.insert(sloc, \"\")\n",
    "\n",
    "    _ = ax.legend(handles, labels)\n",
    "    _ = ax.get_legend().set_frame_on(False)\n",
    "\n",
    "    if title is not None:\n",
    "        _ = ax.get_legend().set_title(title)\n",
    "\n",
    "    if \"loc\" not in kwargs:\n",
    "        kwargs[\"loc\"] = \"upper left\"\n",
    "    if \"bbox_to_anchor\" not in kwargs:\n",
    "        kwargs[\"bbox_to_anchor\"] = (1, 1)\n",
    "    _ = sns.move_legend(ax, **kwargs)\n",
    "\n",
    "\n",
    "# MARK: Printing, experimentation, etc\n",
    "display(sns.color_palette(\"terrain\", n_colors=2, desat=0.8))\n",
    "display(darken(sns.color_palette(\"terrain\", n_colors=2, desat=0.8), by=0.2))\n",
    "\n",
    "display_palette(PALETTE_MODEL)\n",
    "display_palette(PALETTE_SAMPLE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_RE = re.compile(r\"[^a-zA-Z]*\\b(yes|no)\\b[^a-zA-Z]*\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def extract_content(choices_list: list) -> str:\n",
    "    try:\n",
    "        return choices_list[0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(choices_list)\n",
    "\n",
    "\n",
    "def extract_prediction(response: str) -> str:\n",
    "    # get a list of all matches to YES_RE in `response`; take the last match\n",
    "    # and check if it is a \"yes\" or \"no\" response\n",
    "\n",
    "    matches = YES_RE.findall(response)\n",
    "    if len(matches) == 0:\n",
    "        return \"unknown\"\n",
    "    else:\n",
    "        last_match = matches[-1]\n",
    "        if last_match.lower() == \"yes\":\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\"\n",
    "\n",
    "\n",
    "extract_prediction(\"```yes```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMARS_DIR = PROJECT_ROOT / \"data\" / \"grammars\"\n",
    "\n",
    "small_subset_file = PROJECT_ROOT / \"data\" / \"small_subset.txt\"\n",
    "large_subset_file = PROJECT_ROOT / \"data\" / \"large_subset.txt\"\n",
    "\n",
    "response_df_file = PROJECT_ROOT / \"data\" / \"response_df.feather\"\n",
    "\n",
    "# Check to see if the results_df_file exists\n",
    "response_df = None\n",
    "response_full_df = None\n",
    "new_response_df = None\n",
    "if os.path.exists(response_df_file):\n",
    "    print(f\"Loading `response_df` from {response_df_file}\")\n",
    "    response_df = pd.read_feather(\n",
    "        response_df_file,\n",
    "    )\n",
    "else:\n",
    "    print(f\"No store found at {response_df_file}\")\n",
    "\n",
    "print(\"Loading input and results files\")\n",
    "\n",
    "inputs_file_pattern = \"*_inputs.jsonl\"\n",
    "results_file_pattern = \"*_results.jsonl\"\n",
    "\n",
    "batch_id_re = re.compile(r\"^(batch_\\w+)_\")\n",
    "\n",
    "old_batches = []\n",
    "if response_df is not None:\n",
    "    old_batches = response_df[\"batch_id\"].unique()\n",
    "\n",
    "# find all input files in subdirectories of GRAMMARS_DIR\n",
    "input_files = list(GRAMMARS_DIR.rglob(inputs_file_pattern))\n",
    "results_files = list(GRAMMARS_DIR.rglob(results_file_pattern))\n",
    "\n",
    "# filter input_files and results_files to only include those which contain a directory\n",
    "# name that is present in the small_subset_file or large_subset_file\n",
    "with open(small_subset_file, \"r\") as f:\n",
    "    small_subset = set(f.read().strip().split(\"\\n\"))\n",
    "with open(large_subset_file, \"r\") as f:\n",
    "    large_subset = set(f.read().strip().split(\"\\n\"))\n",
    "\n",
    "keep_files = small_subset.union(large_subset)\n",
    "\n",
    "input_files = [\n",
    "    f\n",
    "    for f in input_files\n",
    "    if f.parent.name in keep_files\n",
    "    and batch_id_re.search(f.name)\n",
    "    and batch_id_re.search(f.name).group(1) not in old_batches\n",
    "]\n",
    "results_files = [\n",
    "    f\n",
    "    for f in results_files\n",
    "    if f.parent.name in keep_files\n",
    "    and batch_id_re.search(f.name)\n",
    "    and batch_id_re.search(f.name).group(1) not in old_batches\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Found {len(input_files)} new input files and {len(results_files)} new results files\"\n",
    ")\n",
    "\n",
    "if (len(input_files) > 0) and (len(results_files) > 0):\n",
    "    input_dfs = []\n",
    "\n",
    "    inputs_dfs = []\n",
    "    for f in input_files:\n",
    "        i_df = pd.read_json(f, lines=True)\n",
    "        i_json_struct = json.loads(i_df.to_json(orient=\"records\"))\n",
    "        i_flat_df = pd.json_normalize(i_json_struct)\n",
    "        batch_id = batch_id_re.search(f.name).group(1)\n",
    "        i_flat_df[\"batch_id\"] = batch_id\n",
    "        inputs_dfs.append(i_flat_df)\n",
    "    inputs_df = pd.concat(inputs_dfs, ignore_index=True)\n",
    "\n",
    "    del i_df, i_json_struct, i_flat_df, inputs_dfs\n",
    "\n",
    "    results_dfs = []\n",
    "    for f in results_files:\n",
    "        r_df = pd.read_json(f, lines=True)\n",
    "        r_json_struct = json.loads(r_df.to_json(orient=\"records\"))\n",
    "        r_flat_df = pd.json_normalize(r_json_struct)\n",
    "        batch_id = batch_id_re.search(f.name).group(1)\n",
    "        r_flat_df[\"batch_id\"] = batch_id\n",
    "        results_dfs.append(r_flat_df)\n",
    "    results_df = pd.concat(results_dfs, ignore_index=True)\n",
    "\n",
    "    del r_df, r_json_struct, r_flat_df, results_dfs\n",
    "\n",
    "    # Merge inputs and results on the the batch_id and custom_id\n",
    "    response_full_df = results_df.merge(\n",
    "        inputs_df[\n",
    "            [\n",
    "                \"custom_id\",\n",
    "                \"batch_id\",\n",
    "                \"body.metadata.sample_type\",  # ground-truth label for sample\n",
    "                \"body.metadata.sample\",  # the sample itself\n",
    "                \"body.metadata.grammar_file\",  # grammar file used\n",
    "                \"body.metadata.model\",  # model used\n",
    "                \"body.metadata.n_shots\",  # n_shots used\n",
    "            ]\n",
    "        ],\n",
    "        on=[\"batch_id\", \"custom_id\"],\n",
    "    )\n",
    "\n",
    "    del results_df, inputs_df\n",
    "\n",
    "    response_full_df = response_full_df.rename(\n",
    "        columns={\n",
    "            \"body.metadata.sample_type\": \"sample.type.ground_truth\",\n",
    "            \"body.metadata.sample\": \"sample\",\n",
    "            \"body.metadata.grammar_file\": \"grammar_file\",\n",
    "            \"body.metadata.model\": \"model\",\n",
    "            \"body.metadata.n_shots\": \"n_shots\",\n",
    "        }\n",
    "    )\n",
    "    response_full_df = response_full_df.rename(\n",
    "        columns={\n",
    "            \"body.metadata.sample_type\": \"sample.type.ground_truth\",\n",
    "            \"body.metadata.sample\": \"sample\",\n",
    "            \"body.metadata.grammar_file\": \"grammar_file\",\n",
    "            \"body.metadata.model\": \"model\",\n",
    "            \"body.metadata.n_shots\": \"n_shots\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "if (response_df is not None) and (response_full_df is not None):\n",
    "    response_full_df = response_full_df[\n",
    "        ~response_full_df[\"batch_id\"].isin(response_df[\"batch_id\"].unique())\n",
    "    ]\n",
    "    print(f\"Found {len(response_full_df)} new responses to add to response_df\")\n",
    "\n",
    "    new_batch_ids = response_full_df[\"batch_id\"].unique()\n",
    "\n",
    "if (response_full_df is not None) and (len(response_full_df) > 0):\n",
    "    print(\"Processing new responses\")\n",
    "    response_full_df[\"model_response\"] = response_full_df[\n",
    "        \"response.body.choices\"\n",
    "    ].apply(extract_content)\n",
    "\n",
    "    new_response_df = response_full_df[\n",
    "        [\n",
    "            \"sample\",\n",
    "            \"sample.type.ground_truth\",\n",
    "            \"model_response\",\n",
    "            \"grammar_file\",\n",
    "            \"model\",\n",
    "            \"n_shots\",\n",
    "            \"batch_id\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    del response_full_df\n",
    "\n",
    "    # drop columns with NA values\n",
    "    new_response_df = new_response_df.dropna(axis=1)\n",
    "\n",
    "    new_response_df[\"sample.type.predicted\"] = new_response_df[\"model_response\"].apply(\n",
    "        extract_prediction\n",
    "    )\n",
    "    new_response_df[\"sample.length\"] = new_response_df[\"sample\"].apply(\n",
    "        lambda s: len(str(s).split(\" \"))\n",
    "    )\n",
    "    new_response_df[\"correct\"] = (\n",
    "        new_response_df[\"sample.type.ground_truth\"]\n",
    "        == new_response_df[\"sample.type.predicted\"]\n",
    "    )\n",
    "    new_response_df = new_response_df.dropna()\n",
    "    new_response_df[\"n_shots\"] = pd.Categorical(\n",
    "        new_response_df[\"n_shots\"],\n",
    "        categories=[\"0\", \"2\", \"4\", \"8\", \"16\", \"32\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "    new_response_df[\"sample.type.ground_truth\"] = pd.Categorical(\n",
    "        new_response_df[\"sample.type.ground_truth\"],\n",
    "        categories=[\"positive\", \"negative\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "    new_response_df[\"sample.type.predicted\"] = pd.Categorical(\n",
    "        new_response_df[\"sample.type.predicted\"],\n",
    "        categories=[\"positive\", \"negative\", \"unknown\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    new_response_df[\"model\"] = new_response_df[\"model\"].str.replace(\n",
    "        \"_\", \"/\", regex=False\n",
    "    )\n",
    "\n",
    "    print(new_response_df[\"model\"].unique())\n",
    "\n",
    "    # Shorten gemma model names\n",
    "    new_response_df[\"model\"] = new_response_df[\"model\"].map(\n",
    "        {\n",
    "            \"gpt-4.1-nano\": \"gpt-4.1-nano\",\n",
    "            \"gpt-4.1-mini\": \"gpt-4.1-mini\",\n",
    "            \"gpt-4.1\": \"gpt-4.1\",\n",
    "            \"o4-mini\": \"o4-mini\",\n",
    "            \"o3\": \"o3\",\n",
    "            \"google/gemma-3-1b-it\": \"gemma-3-1b\",\n",
    "            \"google/gemma-3-4b-it\": \"gemma-3-4b\",\n",
    "            \"google/gemma-3-12b-it\": \"gemma-3-12b\",\n",
    "            \"google/gemma-3-27b-it\": \"gemma-3-27b\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    new_response_df[\"model_type\"] = pd.Categorical(\n",
    "        new_response_df[\"model\"].map(\n",
    "            {\n",
    "                \"gpt-4.1-nano\": \"regular\",\n",
    "                \"gpt-4.1-mini\": \"regular\",\n",
    "                \"gpt-4.1\": \"regular\",\n",
    "                \"o4-mini\": \"thinking\",\n",
    "                \"o3\": \"thinking\",\n",
    "                \"gemma-3-1b\": \"regular\",\n",
    "                \"gemma-3-4b\": \"regular\",\n",
    "                \"gemma-3-12b\": \"regular\",\n",
    "                \"gemma-3-27b\": \"regular\",\n",
    "            }\n",
    "        ),\n",
    "        categories=[\"regular\", \"thinking\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    # Add all the new data to the response_df\n",
    "    if response_df is None:\n",
    "        response_df = new_response_df.copy()\n",
    "    elif len(new_response_df) > 0:\n",
    "        response_df = pd.concat([response_df, new_response_df], ignore_index=True)\n",
    "\n",
    "    response_df[\"model\"] = pd.Categorical(\n",
    "        response_df[\"model\"],\n",
    "        categories=[\n",
    "            \"gpt-4.1-nano\",\n",
    "            \"gpt-4.1-mini\",\n",
    "            \"gpt-4.1\",\n",
    "            \"o4-mini\",\n",
    "            \"o3\",\n",
    "            \"gemma-3-1b\",\n",
    "            \"gemma-3-4b\",\n",
    "            \"gemma-3-12b\",\n",
    "            \"gemma-3-27b\",\n",
    "        ],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    print(\"Saving `response_df` to Feather store\")\n",
    "    response_df.dropna().to_feather(response_df_file)\n",
    "\n",
    "del new_response_df\n",
    "\n",
    "response_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_df[response_df[\"model\"] != \"gemma-3-4b\"].dropna().to_feather(response_df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any batch_ids which either have a results file with no inputs, or vice versa\n",
    "results_batch_ids = set(batch_id_re.search(f.name).group(1) for f in results_files)\n",
    "input_batch_ids = set(batch_id_re.search(f.name).group(1) for f in input_files)\n",
    "missing_results = input_batch_ids - results_batch_ids\n",
    "\n",
    "print(f\"Found {len(missing_results)} batches with inputs but no results\")\n",
    "\n",
    "for batch_id in missing_results:\n",
    "    # find the input file for this batch_id\n",
    "    input_file = [\n",
    "        f for f in input_files if batch_id_re.search(f.name).group(1) == batch_id\n",
    "    ]\n",
    "    if len(input_file) > 0:\n",
    "        print(f\"Found input file for batch_id {batch_id}: {input_file[0]}\")\n",
    "\n",
    "    results_file = [\n",
    "        f for f in results_files if batch_id_re.search(f.name).group(1) == batch_id\n",
    "    ]\n",
    "    if len(results_file) > 0:\n",
    "        print(f\"Found results file for batch_id {batch_id}: {results_file[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.groupby([\"grammar_file\", \"model\"], observed=False)[\"sample\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.groupby([\"model\"], observed=True)[\"grammar_file\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load grammar and sample statistics, and annotate the F1 scores with those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_stats_pattern = \"grammar_stats.json\"\n",
    "samples_stats_pattern = \"filtered_samples_stats.json\"\n",
    "\n",
    "grammar_stats_files = list(GRAMMARS_DIR.rglob(grammar_stats_pattern))\n",
    "samples_stats_files = list(GRAMMARS_DIR.rglob(samples_stats_pattern))\n",
    "\n",
    "grammar_stats_files = [f for f in grammar_stats_files if f.parent.name in keep_files]\n",
    "samples_stats_files = [f for f in samples_stats_files if f.parent.name in keep_files]\n",
    "\n",
    "grammar_stats_dicts = []\n",
    "for f in grammar_stats_files:\n",
    "    try:\n",
    "        g_dict = json.loads(f.read_text())\n",
    "        g_dict[\"grammar_file\"] = f.parent.name\n",
    "        grammar_stats_dicts.append(g_dict)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error reading {f}\")\n",
    "grammar_stats_df = pd.DataFrame(grammar_stats_dicts)\n",
    "\n",
    "samples_stats_dicts = []\n",
    "for f in samples_stats_files:\n",
    "    try:\n",
    "        s_dict = json.loads(f.read_text())\n",
    "        s_dict[\"grammar_file\"] = f.parent.name\n",
    "        samples_stats_dicts.append(s_dict)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error reading {f}\")\n",
    "samples_stats_df = pd.DataFrame(samples_stats_dicts)\n",
    "\n",
    "f1_df = (\n",
    "    response_df.groupby(\n",
    "        [\n",
    "            \"n_shots\",\n",
    "            \"model\",\n",
    "            \"model_type\",\n",
    "            \"grammar_file\",\n",
    "        ],\n",
    "        observed=False,\n",
    "    )\n",
    "    .apply(\n",
    "        lambda group: sk_metrics.f1_score(\n",
    "            group[\"sample.type.ground_truth\"],\n",
    "            group[\"sample.type.predicted\"],\n",
    "            average=\"weighted\",\n",
    "        ),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    .reset_index(name=\"weighted_f1_score\")\n",
    ")\n",
    "\n",
    "f1_df = f1_df.join(\n",
    "    grammar_stats_df.set_index(\"grammar_file\"),\n",
    "    on=\"grammar_file\",\n",
    ").join(\n",
    "    samples_stats_df.set_index(\"grammar_file\"),\n",
    "    on=\"grammar_file\",\n",
    ")\n",
    "\n",
    "f1_df = f1_df.dropna(axis=1)\n",
    "\n",
    "macro_f1_df = (\n",
    "    response_df.groupby(\n",
    "        [\n",
    "            \"n_shots\",\n",
    "            \"model\",\n",
    "            \"model_type\",\n",
    "            \"grammar_file\",\n",
    "        ],\n",
    "        observed=False,\n",
    "    )\n",
    "    .apply(\n",
    "        lambda group: sk_metrics.f1_score(\n",
    "            group[\"sample.type.ground_truth\"],\n",
    "            group[\"sample.type.predicted\"],\n",
    "            average=\"macro\",\n",
    "        ),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    .reset_index(name=\"macro_f1_score\")\n",
    ")\n",
    "\n",
    "micro_f1_df = (\n",
    "    response_df.groupby(\n",
    "        [\n",
    "            \"n_shots\",\n",
    "            \"model\",\n",
    "            \"model_type\",\n",
    "            \"grammar_file\",\n",
    "        ],\n",
    "        observed=False,\n",
    "    )\n",
    "    .apply(\n",
    "        lambda group: sk_metrics.f1_score(\n",
    "            group[\"sample.type.ground_truth\"],\n",
    "            group[\"sample.type.predicted\"],\n",
    "            average=\"micro\",\n",
    "        ),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    .reset_index(name=\"micro_f1_score\")\n",
    ")\n",
    "\n",
    "f1_df = f1_df.join(\n",
    "    macro_f1_df[\n",
    "        [\"macro_f1_score\", \"n_shots\", \"model\", \"model_type\", \"grammar_file\"]\n",
    "    ].set_index([\"grammar_file\", \"n_shots\", \"model\", \"model_type\"]),\n",
    "    on=[\"grammar_file\", \"n_shots\", \"model\", \"model_type\"],\n",
    ").join(\n",
    "    micro_f1_df[\n",
    "        [\"micro_f1_score\", \"n_shots\", \"model\", \"model_type\", \"grammar_file\"]\n",
    "    ].set_index([\"grammar_file\", \"n_shots\", \"model\", \"model_type\"]),\n",
    "    on=[\"grammar_file\", \"n_shots\", \"model\", \"model_type\"],\n",
    ")\n",
    "\n",
    "accuracy_df = response_df.join(\n",
    "    grammar_stats_df.set_index(\"grammar_file\"),\n",
    "    on=\"grammar_file\",\n",
    ").join(\n",
    "    samples_stats_df.set_index(\"grammar_file\"),\n",
    "    on=\"grammar_file\",\n",
    ")\n",
    "\n",
    "del grammar_stats_df, samples_stats_df, grammar_stats_dicts, samples_stats_dicts\n",
    "\n",
    "f1_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = f1_df[\n",
    "    [\n",
    "        \"macro_f1_score\",\n",
    "        \"weighted_f1_score\",\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "        \"compression_ratio\",\n",
    "        \"mean_positive_depth\",\n",
    "        \"median_positive_depth\",\n",
    "        \"coverage\",\n",
    "    ]\n",
    "].corr()\n",
    "\n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(\n",
    "    corr_mat,\n",
    "    cmap=CMAP_HEATMAP,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"grammar_stats_correlation_matrix.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_corr_mat = (\n",
    "    corr_mat.iloc[:, 0:2].sort_values(by=\"macro_f1_score\", ascending=False).iloc[2:]\n",
    ")\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    sliced_corr_mat,\n",
    "    cmap=CMAP_HEATMAP,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "\n",
    "for c in range(len(sliced_corr_mat.columns)):\n",
    "    for r in range(len(sliced_corr_mat)):\n",
    "        ax.text(\n",
    "            c + 0.5,\n",
    "            r + 0.5,\n",
    "            f\"${sliced_corr_mat.iloc[r, c]:.2f}$\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "        )\n",
    "\n",
    "_ = ax.set_title(\"Grammar Hyperparameter Correlations with F1 Scores\", y=1.05)\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"grammar_stats_correlation_matrix_sliced.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat_acc = accuracy_df[\n",
    "    [\n",
    "        \"correct\",\n",
    "        \"sample.length\",\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "        \"compression_ratio\",\n",
    "        \"mean_positive_depth\",\n",
    "        \"median_positive_depth\",\n",
    "        \"coverage\",\n",
    "    ]\n",
    "].corr()\n",
    "\n",
    "corr_mat_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_corr_mat_acc = (\n",
    "    corr_mat_acc.iloc[:, 0:1].sort_values(by=\"correct\", ascending=False).iloc[2:]\n",
    ")\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    sliced_corr_mat_acc,\n",
    "    cmap=CMAP_HEATMAP,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "\n",
    "for c in range(len(sliced_corr_mat_acc.columns)):\n",
    "    for r in range(len(sliced_corr_mat_acc)):\n",
    "        ax.text(\n",
    "            c + 0.5,\n",
    "            r + 0.5,\n",
    "            f\"${sliced_corr_mat_acc.iloc[r, c]:.2f}$\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "        )\n",
    "\n",
    "_ = ax.set_title(\"Grammar Hyperparameter Correlations with F1 Scores\", y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_stats_df = f1_df.copy().drop(\n",
    "    columns=[\n",
    "        \"total_possible_samples\",\n",
    "        \"uncompressed_size\",\n",
    "        \"compressed_size\",\n",
    "        \"grammar_file\",\n",
    "        \"grammar_name\",\n",
    "        \"n_shots\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "f1_stats_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endogenous_vars = [\n",
    "    \"model\",\n",
    "    # \"n_terminals\",\n",
    "    # \"n_nonterminals\",\n",
    "    # \"n_lexical_productions\",\n",
    "    \"n_nonlexical_productions\",\n",
    "    \"compression_ratio\",\n",
    "    \"mean_positive_depth\",\n",
    "    \"coverage\",\n",
    "]\n",
    "\n",
    "X = f1_stats_df[endogenous_vars].copy()\n",
    "X = pd.get_dummies(X, drop_first=True, dtype=int)  # one-hot encode categorical vars\n",
    "X = sm.add_constant(X)  # add a constant term to the model\n",
    "\n",
    "Y = f1_stats_df[\"weighted_f1_score\"]\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = f1_stats_df[\"macro_f1_score\"]\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_df = (\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"variable\": X.columns,\n",
    "            \"VIF\": [variance_inflation_factor(X.values, i) for i in range(X.shape[1])],\n",
    "        }\n",
    "    )\n",
    "    .drop(index=0)\n",
    "    .sort_values(by=\"VIF\", ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "vif_df[\"Rating\"] = vif_df[\"VIF\"].apply(\n",
    "    lambda x: \"High\" if x > 10 else \"Moderate\" if x > 5 else \"Low\"\n",
    ")\n",
    "\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Accuracy/Score by Model and Sample Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=accuracy_df,\n",
    "    kind=\"bar\",\n",
    "    x=\"model\",\n",
    "    y=\"correct\",\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    palette=PALETTE_SAMPLE_TYPE,\n",
    "    errorbar=\"se\",\n",
    "    height=3,\n",
    "    aspect=2.8,\n",
    ")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    for i, bar in enumerate(ax.containers):\n",
    "        for rect in bar:\n",
    "            height = rect.get_height()\n",
    "            x_coord = rect.get_x() + rect.get_width() / 2.0\n",
    "\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width() / 2,\n",
    "                height - 0.02,\n",
    "                f\"{height:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"top\",\n",
    "                fontsize=9,\n",
    "                color=\"white\",\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    for bar in ax.patches:\n",
    "        bar.set_edgecolor(BAR_EDGE_COLOR)\n",
    "        bar.set_linewidth(BAR_EDGE_WIDTH)\n",
    "\n",
    "n_counts = accuracy_df.groupby(\"model\", observed=False)[\"grammar_file\"].nunique()\n",
    "mean_accs = (\n",
    "    accuracy_df.groupby([\"model\", \"sample.type.ground_truth\"], observed=False)[\n",
    "        \"correct\"\n",
    "    ]\n",
    "    .mean()\n",
    "    .groupby(\"model\", observed=False)\n",
    "    .mean()\n",
    ")\n",
    "mean_errors = accuracy_df.groupby(\"model\", observed=False)[\"correct\"].sem()\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    for i, category in enumerate(n_counts.index):\n",
    "        try:\n",
    "            pos_height = ax.containers[0][i].get_height()\n",
    "            neg_height = ax.containers[1][i].get_height()\n",
    "            max_height = max(pos_height, neg_height)\n",
    "\n",
    "            count = n_counts[category]\n",
    "            mean_acc = mean_accs[category]\n",
    "            ax.text(i, -0.15, f\"n={count}\", ha=\"center\", va=\"top\")\n",
    "            ax.text(\n",
    "                i + (0.1 if pos_height > neg_height else -0.1),\n",
    "                mean_acc,\n",
    "                f\"{mean_acc:.2f}\",\n",
    "                ha=\"left\" if pos_height > neg_height else \"right\",\n",
    "                va=\"center\",\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "            # add  black diamond at mean accuracy\n",
    "            ax.plot(\n",
    "                i,\n",
    "                mean_acc,\n",
    "                marker=\"D\",\n",
    "                color=\"black\",\n",
    "                markersize=5,\n",
    "                linewidth=0,\n",
    "                label=\"mean (Â± sem)\" if i == 0 else \"_nolegend_\",\n",
    "            )\n",
    "\n",
    "            # add error bar\n",
    "            ax.errorbar(\n",
    "                i,\n",
    "                mean_acc,\n",
    "                yerr=mean_errors[category],\n",
    "                fmt=\"o\",\n",
    "                color=\"black\",\n",
    "                markersize=0,\n",
    "                capsize=5,\n",
    "                label=\"_nolegend_\",\n",
    "            )\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "_ = g.ax.axhline(\n",
    "    y=0.5, color=COLOR_AT_CHANCE, alpha=ALPHA_AT_CHANCE, linestyle=\"--\", zorder=0\n",
    ")\n",
    "\n",
    "legend_format(\n",
    "    ax=g,\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(0, 0.97),\n",
    "    columnspacing=0,\n",
    ")\n",
    "\n",
    "_ = g.ax.set_ylabel(\"Mean Accuracy\")\n",
    "_ = g.ax.set_xlabel(None)\n",
    "_ = g.ax.set_ylim(0, 1)\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"accuracy_by_model.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_melted_df = f1_df.melt(\n",
    "    id_vars=[\"n_shots\", \"model\", \"model_type\", \"grammar_file\"],\n",
    "    value_vars=[\"weighted_f1_score\", \"macro_f1_score\", \"micro_f1_score\"],\n",
    "    var_name=\"average\",\n",
    "    value_name=\"score\",\n",
    ")\n",
    "\n",
    "# map score_type to a more readable name\n",
    "f1_melted_df[\"average\"] = f1_melted_df[\"average\"].map(\n",
    "    {\n",
    "        \"weighted_f1_score\": \"Weighted F1\",\n",
    "        \"macro_f1_score\": \"Macro F1\",\n",
    "        \"micro_f1_score\": \"Micro F1\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Don't show micro f1 score\n",
    "f1_melted_df = f1_melted_df[f1_melted_df[\"average\"] != \"Micro F1\"]\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=f1_melted_df,\n",
    "    kind=\"bar\",\n",
    "    x=\"model\",\n",
    "    y=\"score\",\n",
    "    hue=\"average\",\n",
    "    palette=PALETTE_SCORE,\n",
    "    height=3,\n",
    "    aspect=3.5,\n",
    ")\n",
    "\n",
    "n_counts = accuracy_df.groupby(\"model\", observed=True)[\"grammar_file\"].nunique()\n",
    "\n",
    "_ = g.ax.axhline(\n",
    "    y=0.5, color=COLOR_AT_CHANCE, alpha=ALPHA_AT_CHANCE, linestyle=\"--\", zorder=0\n",
    ")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    for i, category in enumerate(n_counts.index):\n",
    "        count = n_counts[category]\n",
    "        ax.text(i, -0.15, f\"n={count}\", ha=\"center\", va=\"top\")\n",
    "\n",
    "# Add score labels to bars\n",
    "score_labels = f1_melted_df.groupby([\"model\", \"average\"], observed=True)[\"score\"].mean()\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    for bar in ax.containers:\n",
    "        for rect in bar:\n",
    "            height = rect.get_height()\n",
    "            x_coord = rect.get_x() + rect.get_width() / 2.0\n",
    "\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width() / 2,\n",
    "                height - 0.02,\n",
    "                f\"{height:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"top\",\n",
    "                fontsize=9,\n",
    "                color=\"white\",\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    for bar in ax.patches:\n",
    "        bar.set_edgecolor(BAR_EDGE_COLOR)\n",
    "        bar.set_linewidth(BAR_EDGE_WIDTH)\n",
    "\n",
    "legend_format(\n",
    "    ax=g,\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(0, 0.9),\n",
    "    ncol=1,\n",
    ")\n",
    "\n",
    "_ = g.ax.set_ylabel(\"F1 Score\")\n",
    "_ = g.ax.set_xlabel(None)\n",
    "_ = g.ax.set_ylim(0, 1)\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"f1_by_model.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro F1 Score by Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = ax.axhline(\n",
    "    y=0.5,\n",
    "    color=COLOR_AT_CHANCE,\n",
    "    alpha=ALPHA_AT_CHANCE,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=f1_df,\n",
    "    x=\"compression_ratio\",\n",
    "    y=\"macro_f1_score\",\n",
    "    style=\"model\",\n",
    "    hue=\"model\",\n",
    "    hue_order=f1_df[\"model\"].unique(),\n",
    "    style_order=f1_df[\"model\"].unique(),\n",
    "    palette=PALETTE_MODEL,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "legend_format(\n",
    "    keys=f1_df[\"model\"].unique(),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "_ = ax.set_xlabel(\"gzip Compression Ratio\")\n",
    "_ = ax.set_ylabel(\"Macro F1 Score\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"compression_ratio_vs_macro_f1.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = ax.axhline(\n",
    "    y=0.5,\n",
    "    color=COLOR_AT_CHANCE,\n",
    "    alpha=ALPHA_AT_CHANCE,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=f1_df,\n",
    "    x=\"n_terminals\",\n",
    "    y=\"macro_f1_score\",\n",
    "    style=\"model\",\n",
    "    hue=\"model\",\n",
    "    palette=PALETTE_MODEL,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "legend_format(\n",
    "    keys=f1_df[\"model\"].unique(),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xscale(\"log\")\n",
    "\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "_ = ax.set_xlabel(\"# of Terminals\")\n",
    "_ = ax.set_ylabel(\"Macro F1 Score\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"n_terminals_vs_macro_f1.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = ax.axhline(\n",
    "    y=0.5,\n",
    "    color=COLOR_AT_CHANCE,\n",
    "    alpha=ALPHA_AT_CHANCE,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=f1_df,\n",
    "    x=\"mean_positive_depth\",\n",
    "    y=\"macro_f1_score\",\n",
    "    style=\"model\",\n",
    "    hue=\"model\",\n",
    "    palette=PALETTES[\"model\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "legend_format(\n",
    "    keys=f1_df[\"model\"].unique(),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "_ = ax.set_xlabel(\"Mean Parse Depth\")\n",
    "_ = ax.set_ylabel(\"Macro F1 Score\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"mean_parse_depth_vs_macro_f1.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = ax.axhline(\n",
    "    y=0.5,\n",
    "    color=COLOR_AT_CHANCE,\n",
    "    alpha=ALPHA_AT_CHANCE,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=f1_df,\n",
    "    x=\"coverage\",\n",
    "    y=\"macro_f1_score\",\n",
    "    style=\"model\",\n",
    "    hue=\"model\",\n",
    "    palette=PALETTES[\"model\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "filter_by_alpha(\n",
    "    keys=f1_df[\"model\"].unique(),\n",
    "    highlight=[\"o3\", \"o4-mini\"],\n",
    "    alpha=0.2,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "legend_format(\n",
    "    keys=f1_df[\"model\"].unique(),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "_ = ax.set_xlabel(\"Coverage\")\n",
    "_ = ax.set_ylabel(\"Macro F1 Score\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"coverage_vs_macro_f1.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = ax.axhline(\n",
    "    y=0.5,\n",
    "    color=COLOR_AT_CHANCE,\n",
    "    alpha=ALPHA_AT_CHANCE,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=f1_df,\n",
    "    x=\"n_nonlexical_productions\",\n",
    "    y=\"macro_f1_score\",\n",
    "    style=\"model\",\n",
    "    hue=\"model\",\n",
    "    palette=PALETTES[\"model\"],\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "filter_by_alpha(\n",
    "    keys=f1_df[\"model\"].unique(),\n",
    "    highlight=[\"o3\", \"gemma-3-1b\"],\n",
    "    alpha=0.2,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "legend_format(\n",
    "    keys=f1_df[\"model\"].unique(),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_xscale(\"log\")\n",
    "\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "_ = ax.set_xlabel(\"# of Nonlexical Productions  [log scale]\")\n",
    "_ = ax.set_ylabel(\"Macro F1 Score\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"n_nonlexical_productions_vs_macro_f1.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    f1_df[f1_df.n_nonlexical_productions > 100]\n",
    "    .groupby(\"model\", observed=False)[\"macro_f1_score\"]\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (\n",
    "    sns.relplot(\n",
    "        data=accuracy_df,\n",
    "        kind=\"line\",\n",
    "        x=\"sample.length\",\n",
    "        y=\"correct\",\n",
    "        hue=\"sample.type.ground_truth\",\n",
    "        palette=PALETTES[\"sample_type\"],\n",
    "        errorbar=\"se\",\n",
    "        legend=None,\n",
    "        col=\"model\",\n",
    "        height=2,\n",
    "        aspect=0.7,\n",
    "    )\n",
    "    .set_titles(\"{col_name}\")\n",
    "    .set_axis_labels(\"\", \"Mean Accuracy\")\n",
    "    .tight_layout(w_pad=0)\n",
    ")\n",
    "\n",
    "g.set(xticks=[0, 25, 50])\n",
    "\n",
    "g.axes.flat[0].set_xlabel(\"Sample Length\")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    _ = ax.axhline(\n",
    "        y=0.5,\n",
    "        color=COLOR_AT_CHANCE,\n",
    "        alpha=ALPHA_AT_CHANCE,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "# Add \"positive\" and \"negative\" labels around the final xy coords in the first plot\n",
    "for ax in [g.axes.flat[0]]:\n",
    "    for i, label in enumerate([\"positive\", \"negative\"]):\n",
    "        x_coord = ax.lines[i].get_xdata()[-1]\n",
    "        y_coord = ax.lines[i].get_ydata()[-1] + (0.02 if label == \"positive\" else -0.15)\n",
    "        ax.text(\n",
    "            x_coord,\n",
    "            y_coord,\n",
    "            label,\n",
    "            ha=\"right\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=9,\n",
    "            color=darken(PALETTES[\"sample_type\"][label], by=0.2),\n",
    "        )\n",
    "\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"accuracy_by_sample_length.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = (\n",
    "    accuracy_df.groupby([\"model\", \"sample.length\"], observed=False)[\n",
    "        \"sample.type.predicted\"\n",
    "    ]\n",
    "    .value_counts(normalize=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "g = (\n",
    "    sns.relplot(\n",
    "        data=preds_df,\n",
    "        kind=\"line\",\n",
    "        x=\"sample.length\",\n",
    "        y=\"proportion\",\n",
    "        hue=\"sample.type.predicted\",\n",
    "        palette=PALETTES[\"sample_type\"],\n",
    "        errorbar=\"se\",\n",
    "        legend=None,\n",
    "        col=\"model\",\n",
    "        height=2,\n",
    "        aspect=0.7,\n",
    "    )\n",
    "    .set_titles(\"{col_name}\")\n",
    "    .set_axis_labels(\"\", \"Predicted Sample Type\")\n",
    "    .tight_layout(w_pad=0)\n",
    ")\n",
    "\n",
    "g.set(xticks=[0, 25, 50])\n",
    "\n",
    "g.axes.flat[0].set_xlabel(\"Sample Length\")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    _ = ax.axhline(\n",
    "        y=0.5,\n",
    "        color=COLOR_AT_CHANCE,\n",
    "        alpha=ALPHA_AT_CHANCE,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "# Add \"positive\" and \"negative\" labels around the final xy coords in the first plot\n",
    "for ax in [g.axes.flat[0]]:\n",
    "    for i, label in enumerate([\"positive\", \"negative\", \"unknown\"]):\n",
    "        x_coord = ax.lines[i].get_xdata()[-1]\n",
    "        y_coord = ax.lines[i].get_ydata()[-1] + 0.02\n",
    "        ax.text(\n",
    "            x_coord,\n",
    "            y_coord,\n",
    "            label,\n",
    "            ha=\"right\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=9,\n",
    "            color=darken(PALETTES[\"sample_type\"][label], by=0.2),\n",
    "        )\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"predicted_sample_type_by_sample_length.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = sns.lineplot(\n",
    "    data=accuracy_df,\n",
    "    x=\"sample.length\",\n",
    "    y=\"correct\",\n",
    "    hue=\"model\",\n",
    "    style=\"model\",\n",
    "    palette=PALETTE_MODEL,\n",
    "    errorbar=\"se\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.axhline(\n",
    "    y=0.5,\n",
    "    color=COLOR_AT_CHANCE,\n",
    "    alpha=ALPHA_AT_CHANCE,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "legend_format(\n",
    "    keys=f1_df[\"model\"].unique(),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylim(None, 1.02)\n",
    "_ = ax.set_xlabel(\"Sample Length\")\n",
    "_ = ax.set_ylabel(\"Mean Accuracy\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"accuracy_by_sample_length_by_model.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot entropy of model responses by sequence length. Calculate based on the \"correct\" column of the accuracy_df, which is a binary variable, when grouped by model and sample.length. This will give a distribution over 0's and 1's.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def binary_entropy(series) -> float:\n",
    "    counts = series.value_counts().values\n",
    "    total = counts.sum()\n",
    "    p_correct = counts[0] / total\n",
    "    if p_correct == 0 or p_correct == 1:\n",
    "        return 0\n",
    "    return -(p_correct * np.log2(p_correct) + (1 - p_correct) * np.log2(1 - p_correct))\n",
    "\n",
    "\n",
    "entropy_df = (\n",
    "    accuracy_df.groupby([\"model\", \"sample.length\"], observed=False)[\n",
    "        \"sample.type.predicted\"\n",
    "    ]\n",
    "    .apply(binary_entropy)\n",
    "    .reset_index(name=\"entropy\")\n",
    ")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "_ = sns.lineplot(\n",
    "    data=entropy_df,\n",
    "    x=\"sample.length\",\n",
    "    y=\"entropy\",\n",
    "    hue=\"model\",\n",
    "    style=\"model\",\n",
    "    palette=PALETTE_MODEL,\n",
    "    errorbar=\"se\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "legend_format(\n",
    "    keys=f1_df[\"model\"].unique(),\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# _ = ax.set_xscale(\"log\")\n",
    "\n",
    "_ = ax.set_xlabel(\"Sample Length\")\n",
    "_ = ax.set_ylabel(\"Entropy of Model Predictions\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"entropy_by_sample_length_by_model.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    accuracy_df[accuracy_df.model == \"gemma-3-1b\"]\n",
    "    .groupby(\"sample.length\", observed=False)[\"sample.type.predicted\"]\n",
    "    .value_counts(normalize=True)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of Sample Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "sns.histplot(\n",
    "    data=response_df,\n",
    "    x=\"sample.length\",\n",
    "    ax=ax,\n",
    "    binwidth=1,\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    palette=PALETTE_SAMPLE_TYPE,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "_ = ax.get_legend().set_title(\"Sample type\")\n",
    "_ = ax.set_xlabel(\"Sample length\")\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"sample_length_histogram.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts_df = (\n",
    "    response_df.groupby([\"sample.type.ground_truth\"], observed=False)[\n",
    "        \"sample.type.ground_truth\"\n",
    "    ]\n",
    "    .count()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "total_samples = sample_counts_df[\"count\"].sum()\n",
    "sample_counts_df[\"proportion\"] = sample_counts_df[\"count\"] / total_samples\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=sample_counts_df,\n",
    "    kind=\"bar\",\n",
    "    x=\"sample.type.ground_truth\",\n",
    "    y=\"proportion\",\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    palette=PALETTE_SAMPLE_TYPE,\n",
    "    height=3,\n",
    "    aspect=0.8,\n",
    ").set_axis_labels(\"\", \"Proportion of Samples\")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    for i, bar in enumerate(ax.containers):\n",
    "        for rect in bar:\n",
    "            height = rect.get_height()\n",
    "            x_coord = rect.get_x() + rect.get_width() / 2.0\n",
    "\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width() / 2,\n",
    "                height - 0.02,\n",
    "                f\"{height:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"top\",\n",
    "                fontsize=9,\n",
    "                color=\"white\",\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    for bar in ax.patches:\n",
    "        bar.set_edgecolor(BAR_EDGE_COLOR)\n",
    "        bar.set_linewidth(BAR_EDGE_WIDTH)\n",
    "\n",
    "_ = g.ax.axhline(\n",
    "    y=0.5,\n",
    "    color=COLOR_AT_CHANCE,\n",
    "    alpha=ALPHA_AT_CHANCE,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "_ = g.ax.set_ylim(0, 1)\n",
    "\n",
    "plt.savefig(\n",
    "    FIGURES_DIR / \"sample_type_proportions.pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_responses_df = response_df[(response_df.model == \"gemma-3-4b\")][\n",
    "    [\n",
    "        \"model_response\",\n",
    "        \"sample.type.predicted\",\n",
    "        \"sample.length\",\n",
    "        \"correct\",\n",
    "        \"grammar_file\",\n",
    "    ]\n",
    "].sort_values(by=\"grammar_file\")[\n",
    "    [\"model_response\", \"sample.type.predicted\", \"sample.length\", \"correct\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    gemma_responses_df[gemma_responses_df[\"sample.type.predicted\"] == \"unknown\"].iloc[\n",
    "        50\n",
    "    ][\"model_response\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with max_new_tokens=2048: np.float64(376.80102040816325)\n",
    "\n",
    "gemma_responses_df[\"model_response\"].apply(lambda x: len(x)).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
