{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for Prompting Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sk_metrics\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_RE = re.compile(r\"[^a-zA-Z]*\\b(yes|no)\\b[^a-zA-Z]*\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def extract_content(choices_list: list) -> str:\n",
    "    try:\n",
    "        return choices_list[0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(choices_list)\n",
    "\n",
    "\n",
    "def extract_prediction(response: str) -> str:\n",
    "    # get a list of all matches to YES_RE in `response`; take the last match\n",
    "    # and check if it is a \"yes\" or \"no\" response\n",
    "\n",
    "    matches = YES_RE.findall(response)\n",
    "    if len(matches) == 0:\n",
    "        return \"unknown\"\n",
    "    else:\n",
    "        last_match = matches[-1]\n",
    "        if last_match.lower() == \"yes\":\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")\n",
    "\n",
    "GRAMMARS_DIR = PROJECT_ROOT / \"data\" / \"grammars\"\n",
    "\n",
    "inputs_file_pattern = \"*_inputs.jsonl\"\n",
    "results_file_pattern = \"*_results.jsonl\"\n",
    "\n",
    "batch_id_re = re.compile(r\"^(batch_\\w+)_\")\n",
    "\n",
    "# find all input files in subdirectories of GRAMMARS_DIR\n",
    "input_files = list(GRAMMARS_DIR.rglob(inputs_file_pattern))\n",
    "results_files = list(GRAMMARS_DIR.rglob(results_file_pattern))\n",
    "\n",
    "input_dfs = []\n",
    "\n",
    "inputs_dfs = []\n",
    "for f in input_files:\n",
    "    i_df = pd.read_json(f, lines=True)\n",
    "    i_json_struct = json.loads(i_df.to_json(orient=\"records\"))\n",
    "    i_flat_df = pd.json_normalize(i_json_struct)\n",
    "    batch_id = batch_id_re.search(f.name).group(1)\n",
    "    i_flat_df[\"batch_id\"] = batch_id\n",
    "    inputs_dfs.append(i_flat_df)\n",
    "inputs_df = pd.concat(inputs_dfs, ignore_index=True)\n",
    "\n",
    "del i_df, i_json_struct, i_flat_df, inputs_dfs\n",
    "\n",
    "results_dfs = []\n",
    "for f in results_files:\n",
    "    r_df = pd.read_json(f, lines=True)\n",
    "    r_json_struct = json.loads(r_df.to_json(orient=\"records\"))\n",
    "    r_flat_df = pd.json_normalize(r_json_struct)\n",
    "    batch_id = batch_id_re.search(f.name).group(1)\n",
    "    r_flat_df[\"batch_id\"] = batch_id\n",
    "    results_dfs.append(r_flat_df)\n",
    "results_df = pd.concat(results_dfs, ignore_index=True)\n",
    "\n",
    "del r_df, r_json_struct, r_flat_df, results_dfs\n",
    "\n",
    "# Merge inputs and results on the the batch_id and custom_id\n",
    "response_full_df = results_df.merge(\n",
    "    inputs_df[\n",
    "        [\n",
    "            \"custom_id\",\n",
    "            \"batch_id\",\n",
    "            \"body.metadata.sample_type\",  # ground-truth label for sample\n",
    "            \"body.metadata.sample\",  # the sample itself\n",
    "            \"body.metadata.grammar_file\",  # grammar file used\n",
    "            \"body.metadata.model\",  # model used\n",
    "            \"body.metadata.n_shots\",  # n_shots used\n",
    "        ]\n",
    "    ],\n",
    "    on=[\"batch_id\", \"custom_id\"],\n",
    ")\n",
    "\n",
    "# del results_df, inputs_df\n",
    "\n",
    "response_full_df = response_full_df.rename(\n",
    "    columns={\n",
    "        \"body.metadata.sample_type\": \"sample.type.ground_truth\",\n",
    "        \"body.metadata.sample\": \"sample\",\n",
    "        \"body.metadata.grammar_file\": \"grammar_file\",\n",
    "        \"body.metadata.model\": \"model\",\n",
    "        \"body.metadata.n_shots\": \"n_shots\",\n",
    "    }\n",
    ")\n",
    "response_full_df = response_full_df.rename(\n",
    "    columns={\n",
    "        \"body.metadata.sample_type\": \"sample.type.ground_truth\",\n",
    "        \"body.metadata.sample\": \"sample\",\n",
    "        \"body.metadata.grammar_file\": \"grammar_file\",\n",
    "        \"body.metadata.model\": \"model\",\n",
    "        \"body.metadata.n_shots\": \"n_shots\",\n",
    "    }\n",
    ")\n",
    "response_full_df[\"model_response\"] = response_full_df[\"response.body.choices\"].apply(\n",
    "    extract_content\n",
    ")\n",
    "\n",
    "# Filter out batches with fewer than 500 samples\n",
    "response_full_df = response_full_df[\n",
    "    response_full_df.groupby(\"batch_id\")[\"sample\"].transform(\"count\") >= 500\n",
    "]\n",
    "\n",
    "response_df = response_full_df[\n",
    "    [\n",
    "        \"sample\",\n",
    "        \"sample.type.ground_truth\",\n",
    "        \"model_response\",\n",
    "        \"grammar_file\",\n",
    "        \"model\",\n",
    "        \"n_shots\",\n",
    "    ]\n",
    "].copy()\n",
    "# del response_full_df\n",
    "response_df[\"sample.type.predicted\"] = response_df[\"model_response\"].apply(\n",
    "    extract_prediction\n",
    ")\n",
    "response_df[\"sample.length\"] = response_df[\"sample\"].apply(\n",
    "    lambda s: len(str(s).split(\" \"))\n",
    ")\n",
    "response_df[\"correct\"] = (\n",
    "    response_df[\"sample.type.ground_truth\"] == response_df[\"sample.type.predicted\"]\n",
    ")\n",
    "response_df = response_df.dropna()\n",
    "response_df[\"n_shots\"] = pd.Categorical(\n",
    "    response_df[\"n_shots\"],\n",
    "    categories=[\"0\", \"2\", \"4\", \"8\", \"16\", \"32\"],\n",
    "    ordered=True,\n",
    ")\n",
    "response_df[\"sample.type.ground_truth\"] = pd.Categorical(\n",
    "    response_df[\"sample.type.ground_truth\"],\n",
    "    categories=[\"positive\", \"negative\"],\n",
    "    ordered=True,\n",
    ")\n",
    "response_df[\"sample.type.predicted\"] = pd.Categorical(\n",
    "    response_df[\"sample.type.predicted\"],\n",
    "    categories=[\"positive\", \"negative\", \"unknown\"],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "response_df[\"model\"] = response_df[\"model\"].str.replace(\"_\", \"/\", regex=False)\n",
    "response_df[\"model\"] = pd.Categorical(\n",
    "    response_df[\"model\"],\n",
    ")\n",
    "\n",
    "\n",
    "response_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter response_full_df to only include batch_id with at least 10 samples\n",
    "\n",
    "\n",
    "response_full_df.groupby(\"batch_id\")[\"sample\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.groupby([\"grammar_file\", \"model\"], observed=False)[\"sample\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load grammar and sample statistics, and annotate the F1 scores with those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_stats_pattern = \"grammar_stats.json\"\n",
    "samples_stats_pattern = \"filtered_samples_stats.json\"\n",
    "\n",
    "grammar_stats_files = list(GRAMMARS_DIR.rglob(grammar_stats_pattern))\n",
    "samples_stats_files = list(GRAMMARS_DIR.rglob(samples_stats_pattern))\n",
    "\n",
    "grammar_stats_dicts = []\n",
    "for f in grammar_stats_files:\n",
    "    try:\n",
    "        g_dict = json.loads(f.read_text())\n",
    "        g_dict[\"grammar_file\"] = f.parent.name\n",
    "        grammar_stats_dicts.append(g_dict)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error reading {f}\")\n",
    "grammar_stats_df = pd.DataFrame(grammar_stats_dicts)\n",
    "\n",
    "samples_stats_dicts = []\n",
    "for f in samples_stats_files:\n",
    "    try:\n",
    "        s_dict = json.loads(f.read_text())\n",
    "        s_dict[\"grammar_file\"] = f.parent.name\n",
    "        samples_stats_dicts.append(s_dict)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error reading {f}\")\n",
    "samples_stats_df = pd.DataFrame(samples_stats_dicts)\n",
    "\n",
    "f1_df = (\n",
    "    response_df.groupby([\"n_shots\", \"model\", \"grammar_file\"], observed=False)\n",
    "    .apply(\n",
    "        lambda group: sk_metrics.f1_score(\n",
    "            group[\"sample.type.ground_truth\"],\n",
    "            group[\"sample.type.predicted\"],\n",
    "            average=\"weighted\",\n",
    "        ),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    .reset_index(name=\"f1_score\")\n",
    ")\n",
    "\n",
    "f1_df = f1_df.join(\n",
    "    grammar_stats_df.set_index(\"grammar_file\"),\n",
    "    on=\"grammar_file\",\n",
    ").join(\n",
    "    samples_stats_df.set_index(\"grammar_file\"),\n",
    "    on=\"grammar_file\",\n",
    ")\n",
    "\n",
    "del grammar_stats_df, samples_stats_df, grammar_stats_dicts, samples_stats_dicts\n",
    "\n",
    "f1_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = f1_df[\n",
    "    [\n",
    "        \"f1_score\",\n",
    "        \"n_shots\",\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "        \"compression_ratio\",\n",
    "        \"mean_positive_parses\",\n",
    "        # \"median_positive_parses\",  # no variance\n",
    "        \"mean_positive_depth\",\n",
    "        \"median_positive_depth\",\n",
    "        \"total_samples\",\n",
    "    ]\n",
    "].corr()\n",
    "\n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.heatmap(\n",
    "    corr_mat,\n",
    "    cmap=\"vlag_r\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(\n",
    "    corr_mat.iloc[0].to_frame().sort_values(by=\"f1_score\", ascending=False),\n",
    "    cmap=\"vlag_r\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "\n",
    "_ = ax.set_title(\"Correlation with F1 Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corr_mat.iloc[0].to_frame().sort_values(by=\"f1_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_stats_df = f1_df.copy().drop(\n",
    "    columns=[\n",
    "        # \"coverage\",\n",
    "        # \"total_samples\",\n",
    "        \"total_possible_samples\",\n",
    "        \"uncompressed_size\",\n",
    "        \"compressed_size\",\n",
    "        \"grammar_file\",\n",
    "        \"grammar_name\",\n",
    "    ]\n",
    ")\n",
    "f1_stats_df[\"n_shots\"] = f1_stats_df[\"n_shots\"].astype(int)\n",
    "\n",
    "f1_stats_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = f1_stats_df[\n",
    "    [\n",
    "        \"model\",\n",
    "        \"n_shots\",\n",
    "        \"n_terminals\",\n",
    "        \"n_nonterminals\",\n",
    "        \"n_lexical_productions\",\n",
    "        \"n_nonlexical_productions\",\n",
    "        \"compression_ratio\",\n",
    "        \"mean_positive_parses\",\n",
    "        \"median_positive_parses\",\n",
    "        \"mean_positive_depth\",\n",
    "        \"median_positive_depth\",\n",
    "        \"coverage\",\n",
    "        \"total_samples\",\n",
    "    ]\n",
    "]\n",
    "X = pd.get_dummies(X, drop_first=True, dtype=int)  # one-hot encode categorical vars\n",
    "X = sm.add_constant(X)  # add a constant term to the model\n",
    "\n",
    "Y = f1_stats_df[\"f1_score\"]\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy by Model, Sample Type, and # of Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(\n",
    "    data=response_df,\n",
    "    x=\"n_shots\",\n",
    "    y=\"correct\",\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    palette={\"positive\": \"orange\", \"negative\": \"blue\"},\n",
    "    style=\"model\",\n",
    "    markers=True,\n",
    "    alpha=0.35,\n",
    "    err_kws={\"alpha\": 0.15},\n",
    "    markersize=8,\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=response_df,\n",
    "    x=\"n_shots\",\n",
    "    y=\"correct\",\n",
    "    style=\"model\",\n",
    "    color=\"black\",\n",
    "    linewidth=2,\n",
    "    markers=True,\n",
    "    ax=ax,\n",
    "    legend=False,\n",
    "    markersize=8,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylabel(\"Mean Accuracy\")\n",
    "_ = ax.set_xlabel(\"# of Shots  [log scale]\")\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "\n",
    "# baseline accuracy of guessing randomly is 50%\n",
    "_ = ax.axhline(y=0.5, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "for h in handles:\n",
    "    h.set_alpha(1)\n",
    "_ = ax.legend(handles, labels)\n",
    "\n",
    "_ = sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = ax.axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "sns.lineplot(\n",
    "    data=f1_df,\n",
    "    x=\"n_shots\",\n",
    "    y=\"f1_score\",\n",
    "    style=\"model\",\n",
    "    hue=\"model\",\n",
    "    markers=True,\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "_ = ax.set_xlabel(\"# of Shots [log scale]\")\n",
    "_ = ax.set_ylabel(\"F1 Score\")\n",
    "\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score by Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = ax.axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "sns.lineplot(\n",
    "    data=f1_df[f1_df[\"model\"] == \"gpt-4o-mini\"],\n",
    "    x=\"compression_ratio\",\n",
    "    y=\"f1_score\",\n",
    "    style=\"model\",\n",
    "    color=\"black\",\n",
    "    markers=True,\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "# _ = ax.set_xlim(0.98, None)\n",
    "# _ = ax.set_xscale(\"log\")\n",
    "_ = ax.set_xlabel(\"gzip Compression Ratio\")\n",
    "_ = ax.set_ylabel(\"F1 Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = ax.axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=f1_df[f1_df[\"model\"] == \"gpt-4o-mini\"],\n",
    "    x=\"n_terminals\",\n",
    "    y=\"f1_score\",\n",
    "    style=\"model\",\n",
    "    color=\"black\",\n",
    "    # markers=True,\n",
    "    # linewidth=2,\n",
    "    # markersize=8,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "_ = ax.set_xlabel(\"# of Terminals\")\n",
    "_ = ax.set_ylabel(\"F1 Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3.5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "_ = ax.axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=f1_df[f1_df[\"model\"] == \"gpt-4o-mini\"],\n",
    "    x=\"mean_positive_depth\",\n",
    "    y=\"f1_score\",\n",
    "    style=\"model\",\n",
    "    color=\"black\",\n",
    "    # markers=True,\n",
    "    # linewidth=2,\n",
    "    # markersize=8,\n",
    "    legend=False,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylim(-0.02, 1.02)\n",
    "_ = ax.set_xlabel(\"Mean Parse Depth\")\n",
    "_ = ax.set_ylabel(\"F1 Score\")\n",
    "_ = ax.set_title(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of Sample Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "sns.histplot(\n",
    "    data=response_df,\n",
    "    x=\"sample.length\",\n",
    "    ax=ax,\n",
    "    binwidth=1,\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    palette={\"positive\": \"orange\", \"negative\": \"blue\"},\n",
    ")\n",
    "\n",
    "_ = ax.get_legend().set_title(\"Sample type\")\n",
    "_ = ax.set_xlabel(\"Sample length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
