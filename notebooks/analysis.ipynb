{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sk_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data & model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_RE = re.compile(r\"yes\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def extract_content(choices_list: list) -> str:\n",
    "    return choices_list[0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def extract_prediction(response: str) -> str:\n",
    "    last_20_chars = response[-20:]\n",
    "    if YES_RE.search(last_20_chars):\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_path = PROJECT_ROOT / \"data\" / \"completions\"\n",
    "\n",
    "# list all files matching \".*_inputs.jsonl\" in the completions_path\n",
    "inputs_files = list(completions_path.glob(\"*_inputs.jsonl\"))\n",
    "results_files = list(completions_path.glob(\"*_results.jsonl\"))\n",
    "\n",
    "# Grab the batch_id from the filename\n",
    "batch_id_re = re.compile(r\"^(batch_\\w+)_\")\n",
    "\n",
    "inputs_dfs = []\n",
    "for f in inputs_files:\n",
    "    i_df = pd.read_json(f, lines=True)\n",
    "    i_json_struct = json.loads(i_df.to_json(orient=\"records\"))\n",
    "    i_flat_df = pd.json_normalize(i_json_struct)\n",
    "    batch_id = batch_id_re.search(f.name).group(1)\n",
    "    i_flat_df[\"batch_id\"] = batch_id\n",
    "    inputs_dfs.append(i_flat_df)\n",
    "inputs_df = pd.concat(inputs_dfs, ignore_index=True)\n",
    "\n",
    "results_dfs = []\n",
    "for f in results_files:\n",
    "    r_df = pd.read_json(f, lines=True)\n",
    "    r_json_struct = json.loads(r_df.to_json(orient=\"records\"))\n",
    "    r_flat_df = pd.json_normalize(r_json_struct)\n",
    "    batch_id = batch_id_re.search(f.name).group(1)\n",
    "    r_flat_df[\"batch_id\"] = batch_id\n",
    "    results_dfs.append(r_flat_df)\n",
    "results_df = pd.concat(results_dfs, ignore_index=True)\n",
    "\n",
    "# Merge inputs and results on the the batch_id and custom_id\n",
    "response_full_df = results_df.merge(\n",
    "    inputs_df[\n",
    "        [\n",
    "            \"custom_id\",\n",
    "            \"batch_id\",\n",
    "            \"body.metadata.sample_type\",  # ground-truth label for sample\n",
    "            \"body.metadata.sample\",  # the sample itself\n",
    "            \"body.metadata.grammar_file\",  # grammar file used\n",
    "            \"body.metadata.model\",  # model used\n",
    "            \"body.metadata.n_shots\",  # n_shots used\n",
    "        ]\n",
    "    ],\n",
    "    on=[\"batch_id\", \"custom_id\"],\n",
    ")\n",
    "\n",
    "response_full_df = response_full_df.rename(\n",
    "    columns={\n",
    "        \"body.metadata.sample_type\": \"sample.type.ground_truth\",\n",
    "        \"body.metadata.sample\": \"sample\",\n",
    "        \"body.metadata.grammar_file\": \"grammar_file\",\n",
    "        \"body.metadata.model\": \"model\",\n",
    "        \"body.metadata.n_shots\": \"n_shots\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response_full_df[\"model_response\"] = response_full_df[\"response.body.choices\"].apply(\n",
    "    extract_content\n",
    ")\n",
    "\n",
    "response_df = response_full_df[\n",
    "    [\n",
    "        \"sample\",\n",
    "        \"sample.type.ground_truth\",\n",
    "        \"model_response\",\n",
    "        \"grammar_file\",\n",
    "        \"model\",\n",
    "        \"n_shots\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "\n",
    "response_df[\"sample.type.predicted\"] = response_df[\"model_response\"].apply(\n",
    "    extract_prediction\n",
    ")\n",
    "\n",
    "response_df[\"sample.length\"] = response_df[\"sample\"].apply(\n",
    "    lambda s: len(str(s).split(\" \"))\n",
    ")\n",
    "\n",
    "response_df[\"correct\"] = (\n",
    "    response_df[\"sample.type.ground_truth\"] == response_df[\"sample.type.predicted\"]\n",
    ")\n",
    "\n",
    "response_df = response_df.dropna()\n",
    "\n",
    "response_df[\"n_shots\"] = pd.Categorical(\n",
    "    response_df[\"n_shots\"],\n",
    "    categories=[\"0\", \"2\", \"4\"],\n",
    "    ordered=True,\n",
    ")\n",
    "response_df[\"sample.type.ground_truth\"] = pd.Categorical(\n",
    "    response_df[\"sample.type.ground_truth\"],\n",
    "    categories=[\"positive\", \"negative\"],\n",
    "    ordered=True,\n",
    ")\n",
    "response_df[\"sample.type.predicted\"] = pd.Categorical(\n",
    "    response_df[\"sample.type.predicted\"],\n",
    "    categories=[\"positive\", \"negative\"],\n",
    "    ordered=True,\n",
    ")\n",
    "response_df[\"model\"] = pd.Categorical(\n",
    "    response_df[\"model\"],\n",
    ")\n",
    "\n",
    "unique_grammars = response_df[\"grammar_file\"].unique()\n",
    "g_map_dict = {g: f\"Grammar {i+1}\" for i, g in enumerate(unique_grammars)}\n",
    "response_df[\"grammar_file\"] = response_df[\"grammar_file\"].map(g_map_dict)\n",
    "response_df[\"grammar_file\"] = pd.Categorical(\n",
    "    response_df[\"grammar_file\"],\n",
    "    categories=list(g_map_dict.values()),\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "response_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    response_df.groupby([\"grammar_file\", \"n_shots\", \"sample.type.ground_truth\"])[\n",
    "        [\"sample\"]\n",
    "    ].count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sample-length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "sns.histplot(\n",
    "    data=response_df,\n",
    "    x=\"sample.length\",\n",
    "    ax=ax,\n",
    "    bins=25,\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    palette={\"positive\": \"orange\", \"negative\": \"purple\"},\n",
    ")\n",
    "\n",
    "ax.get_legend().set_title(\"Sample type\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Sample length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some of longer sample lengths only have a few samples, the variance on the \n",
    "accuracy will be really high. We solve this by throwing out any samples without at least\n",
    "10 samples in that length category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NUM_SAMPLES = 20\n",
    "\n",
    "samples_by_length = response_df.groupby(\"sample.length\")[\"sample\"].count()\n",
    "many_samples_lengths = samples_by_length[\n",
    "    samples_by_length > MIN_NUM_SAMPLES\n",
    "].index.values\n",
    "\n",
    "response_df = response_df[response_df[\"sample.length\"].isin(many_samples_lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = sk_metrics.accuracy_score(\n",
    "    response_df[\"sample.type.ground_truth\"], response_df[\"sample.type.predicted\"]\n",
    ")\n",
    "\n",
    "mean_cm = sk_metrics.confusion_matrix(\n",
    "    response_df[\"sample.type.ground_truth\"],\n",
    "    response_df[\"sample.type.predicted\"],\n",
    "    normalize=\"true\",\n",
    ")\n",
    "\n",
    "negative_sample_acc = mean_cm[0][0]\n",
    "positive_sample_acc = mean_cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "sns.heatmap(data=mean_cm, annot=True, ax=ax, vmin=0.0, vmax=1.0, cmap=\"coolwarm\")\n",
    "\n",
    "ax.set_xlabel(\"Predicted Label\")\n",
    "ax.set_xticklabels([\"Negative\", \"Positive\"])\n",
    "ax.set_ylabel(\"True Label\")\n",
    "ax.set_yticklabels([\"Negative\", \"Positive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy by sample length & type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 5))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 3])\n",
    "\n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax1 = plt.subplot(gs[1], sharex=ax0)\n",
    "\n",
    "n_bins = response_df[\"sample.length\"].nunique()\n",
    "\n",
    "sns.histplot(\n",
    "    data=response_df,\n",
    "    x=\"sample.length\",\n",
    "    ax=ax0,\n",
    "    bins=n_bins,\n",
    "    color=\"gray\",\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=response_df,\n",
    "    x=\"sample.length\",\n",
    "    y=\"correct\",\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    ax=ax1,\n",
    "    style=\"sample.type.ground_truth\",\n",
    "    palette={\"positive\": \"orange\", \"negative\": \"purple\"},\n",
    "    markers=[\"o\", \"o\"],\n",
    "    dashes=False,\n",
    "    alpha=0.5,\n",
    "    linewidth=2,\n",
    "    err_style=\"bars\",\n",
    ")\n",
    "\n",
    "ax0.set_yscale(\"log\")\n",
    "ax0.set_ylim(10, None)\n",
    "\n",
    "ax1.set_ylabel(\"Mean accuracy\")\n",
    "ax1.set_xlabel(\"Sample length\")\n",
    "\n",
    "# add horizontal lines for per-class accuracy\n",
    "ax1.axhline(positive_sample_acc, color=\"orange\", linestyle=\"--\", linewidth=2)\n",
    "ax1.axhline(negative_sample_acc, color=\"purple\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# add horizontal line for overall accuracy\n",
    "ax1.axhline(mean_accuracy, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# add text for accuracy values\n",
    "ax1.text(\n",
    "    x=0.97,\n",
    "    y=positive_sample_acc + 0.01,\n",
    "    s=f\"{positive_sample_acc:.2f}\",\n",
    "    color=\"orange\",\n",
    "    transform=ax1.transAxes,\n",
    "    horizontalalignment=\"right\",\n",
    "    fontdict={\"weight\": \"bold\"},\n",
    ")\n",
    "ax1.text(\n",
    "    x=0.97,\n",
    "    y=negative_sample_acc - 0.02,\n",
    "    s=f\"{negative_sample_acc:.2f}\",\n",
    "    color=\"purple\",\n",
    "    transform=ax1.transAxes,\n",
    "    horizontalalignment=\"right\",\n",
    "    verticalalignment=\"top\",\n",
    "    fontdict={\"weight\": \"bold\"},\n",
    ")\n",
    "ax1.text(\n",
    "    x=0.97,\n",
    "    y=mean_accuracy + 0.025,\n",
    "    s=f\"{mean_accuracy:.2f}\",\n",
    "    color=\"black\",\n",
    "    transform=ax1.transAxes,\n",
    "    horizontalalignment=\"right\",\n",
    "    fontdict={\"weight\": \"bold\"},\n",
    ")\n",
    "\n",
    "ax1.get_legend().set_title(\"Sample type\")\n",
    "\n",
    "# hide x-axis label and tick labels on the first subplot\n",
    "ax0.set_xlabel(\"\")\n",
    "ax0.tick_params(axis=\"x\", which=\"both\", bottom=True, top=False, labelbottom=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3, 3))\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "\n",
    "ax0 = plt.subplot(gs[0])\n",
    "\n",
    "sns.lineplot(\n",
    "    data=response_df,\n",
    "    x=\"n_shots\",\n",
    "    y=\"correct\",\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    ax=ax0,\n",
    "    style=\"grammar_file\",\n",
    "    palette={\"positive\": \"orange\", \"negative\": \"purple\"},\n",
    "    markers=True,\n",
    "    errorbar=None,\n",
    "    alpha=0.5,\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=response_df,\n",
    "    x=\"n_shots\",\n",
    "    y=\"correct\",\n",
    "    ax=ax0,\n",
    "    color=\"black\",\n",
    "    errorbar=None,\n",
    "    linewidth=3,\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "handles, labels = ax0.get_legend_handles_labels()\n",
    "labels[0] = \"Sample Type\"\n",
    "labels[3] = \"\"\n",
    "\n",
    "ax0.legend(\n",
    "    loc=\"upper left\",\n",
    "    bbox_to_anchor=(1, 1),\n",
    "    handles=handles,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "ax0.set_ylim(0, 1)\n",
    "ax0.set_ylabel(\"Mean Accuracy\")\n",
    "ax0.set_xlabel(\"# of Shots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shots_df = response_df[response_df[\"n_shots\"] == \"0\"]\n",
    "two_shots_df = response_df[response_df[\"n_shots\"] == \"2\"]\n",
    "four_shots_df = response_df[response_df[\"n_shots\"] == \"4\"]\n",
    "\n",
    "mean_cm_0 = sk_metrics.confusion_matrix(\n",
    "    zero_shots_df[\"sample.type.ground_truth\"],\n",
    "    zero_shots_df[\"sample.type.predicted\"],\n",
    "    normalize=\"true\",\n",
    ")\n",
    "\n",
    "mean_cm_2 = sk_metrics.confusion_matrix(\n",
    "    two_shots_df[\"sample.type.ground_truth\"],\n",
    "    two_shots_df[\"sample.type.predicted\"],\n",
    "    normalize=\"true\",\n",
    ")\n",
    "\n",
    "mean_cm_4 = sk_metrics.confusion_matrix(\n",
    "    four_shots_df[\"sample.type.ground_truth\"],\n",
    "    four_shots_df[\"sample.type.predicted\"],\n",
    "    normalize=\"true\",\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "gs = gridspec.GridSpec(1, 3)\n",
    "\n",
    "ax0 = plt.subplot(gs[0])\n",
    "ax1 = plt.subplot(gs[1], sharey=ax0)\n",
    "ax2 = plt.subplot(gs[2], sharey=ax0)\n",
    "\n",
    "sns.heatmap(\n",
    "    data=mean_cm_0, annot=True, ax=ax0, vmin=0.0, vmax=1.0, cmap=\"coolwarm\", cbar=False\n",
    ")\n",
    "sns.heatmap(\n",
    "    data=mean_cm_2, annot=True, ax=ax1, vmin=0.0, vmax=1.0, cmap=\"coolwarm\", cbar=False\n",
    ")\n",
    "sns.heatmap(\n",
    "    data=mean_cm_4, annot=True, ax=ax2, vmin=0.0, vmax=1.0, cmap=\"coolwarm\", cbar=False\n",
    ")\n",
    "\n",
    "\n",
    "ax0.set_ylabel(\"True Label\")\n",
    "ax0.set_yticklabels([\"Negative\", \"Positive\"])\n",
    "\n",
    "ax0.set_xlabel(\"Predicted Label\")\n",
    "ax0.set_xticklabels([\"Negative\", \"Positive\"])\n",
    "ax0.set_title(\"0 Shot\")\n",
    "ax1.set_xlabel(\"Predicted Label\")\n",
    "ax1.set_xticklabels([\"Negative\", \"Positive\"])\n",
    "ax1.set_title(\"2 Shot\")\n",
    "ax2.set_xlabel(\"Predicted Label\")\n",
    "ax2.set_xticklabels([\"Negative\", \"Positive\"])\n",
    "ax2.set_title(\"4 Shot\")\n",
    "\n",
    "ax1.tick_params(axis=\"y\", which=\"both\", left=True, right=False, labelleft=False)\n",
    "ax2.tick_params(axis=\"y\", which=\"both\", left=True, right=False, labelleft=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive sample proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_path = PROJECT_ROOT / \"data\" / \"partitions\"\n",
    "\n",
    "# open all .csv files in partitions_path\n",
    "partitions_files = list(partitions_path.glob(\"*.csv\"))\n",
    "\n",
    "# read all .csv files into a single dataframe\n",
    "partitions_dfs = []\n",
    "for f in partitions_files:\n",
    "    f_name = pathlib.Path(f).stem.split(\"_k=\")[0].split(\"counts_\")[1]\n",
    "    g_name = g_map_dict[f_name]\n",
    "    p_df = pd.read_csv(f)\n",
    "    p_df[\"grammar_file\"] = g_name\n",
    "    partitions_dfs.append(p_df)\n",
    "\n",
    "partitions_df = (\n",
    "    pd.concat(partitions_dfs, ignore_index=True)\n",
    "    .groupby([\"grammar_file\", \"sample.length\"])\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 4))\n",
    "gs = gridspec.GridSpec(1, 1)\n",
    "ax0 = plt.subplot(gs[0])\n",
    "\n",
    "sns.lineplot(\n",
    "    data=partitions_df,\n",
    "    x=\"sample.length\",\n",
    "    y=\"prop_positive_samples\",\n",
    "    hue=\"grammar_file\",\n",
    "    style=\"grammar_file\",\n",
    "    linewidth=2,\n",
    "    markers=True,\n",
    "    # color=\"orange\",\n",
    "    palette=\"Oranges\",\n",
    "    ax=ax0,\n",
    ")\n",
    "\n",
    "ax0.set_ylabel(\"Proportion of Strings in Grammar\")\n",
    "ax0.set_ylim(0, 1)\n",
    "ax0.get_legend().set_title(\"\")\n",
    "ax0.set_xlabel(\"Sample Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
