{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")\n",
    "GRAMMARS_PATH = PROJECT_ROOT / \"data\" / \"scfg_grammars\"\n",
    "\n",
    "results_files = list(GRAMMARS_PATH.rglob(\"batch_*.jsonl\"))\n",
    "inputs_files = list(GRAMMARS_PATH.rglob(\"scfg_*.jsonl\"))\n",
    "\n",
    "dotenv.load_dotenv(PROJECT_ROOT / \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dfs = []\n",
    "inputs_dfs = []\n",
    "\n",
    "for f in results_files:\n",
    "    df = pd.read_json(f, lines=True)\n",
    "    json_struct = json.loads(df.to_json(orient=\"records\"))\n",
    "    flat_df = pd.json_normalize(json_struct)\n",
    "\n",
    "    batch_id = f.name.split(\"_output.jsonl\")[0]\n",
    "\n",
    "    # extract response\n",
    "    flat_df[\"model_response\"] = flat_df[\"response.body.choices\"].apply(\n",
    "        lambda x: x[0][\"message\"][\"content\"]\n",
    "    )\n",
    "    flat_df[\"prompt_tokens\"] = flat_df[\"response.body.usage.prompt_tokens\"]\n",
    "    flat_df[\"completion_tokens\"] = flat_df[\"response.body.usage.completion_tokens\"]\n",
    "    flat_df[\"total_tokens\"] = flat_df[\"response.body.usage.total_tokens\"]\n",
    "    flat_df[\"model\"] = flat_df[\"response.body.model\"]\n",
    "    flat_df[\"batch_id\"] = batch_id\n",
    "\n",
    "    res_dfs.append(flat_df)\n",
    "res_df = pd.concat(res_dfs, ignore_index=True)\n",
    "batch_ids = res_df[\"batch_id\"].unique()\n",
    "\n",
    "for bid in batch_ids:\n",
    "    batch = openai_client.batches.retrieve(bid)\n",
    "    input_file = openai_client.files.retrieve(batch.input_file_id)\n",
    "    res_df.loc[res_df[\"batch_id\"] == bid, \"input_file\"] = input_file.filename\n",
    "\n",
    "    res_df[\"grammar_name\"] = res_df[\"input_file\"].apply(\n",
    "        lambda x: \"_\".join(str(x).split(\"_\")[0:2])\n",
    "    )\n",
    "\n",
    "inputs_dfs = []\n",
    "\n",
    "for f in inputs_files:\n",
    "    df = pd.read_json(f, lines=True)\n",
    "    json_struct = json.loads(df.to_json(orient=\"records\"))\n",
    "    flat_df = pd.json_normalize(json_struct)\n",
    "    flat_df[\"grammar_name\"] = flat_df[\"body.metadata.grammar_file\"]\n",
    "    flat_df[\"lhs\"] = flat_df[\"body.metadata.lhs\"]\n",
    "    flat_df[\"rhs\"] = flat_df[\"body.metadata.rhs\"]\n",
    "    inputs_dfs.append(flat_df)\n",
    "\n",
    "inputs_df = pd.concat(inputs_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# join res_df and inputs_df on \"grammar_name\", \"request_id\"\n",
    "res_df = pd.merge(\n",
    "    res_df,\n",
    "    inputs_df,\n",
    "    on=[\"grammar_name\", \"custom_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Extract answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_re = re.compile(r\"Final Answer: (.*?)(?:\\n|$)\", re.DOTALL)\n",
    "\n",
    "\n",
    "def extract_answer(model_response):\n",
    "    matches = answer_re.findall(model_response)\n",
    "    if matches:\n",
    "        last_match: str = matches[-1]\n",
    "        last_match = re.sub(r\"[^a-zA-Z\\s]\", \"\", last_match)\n",
    "        last_match = last_match.strip()\n",
    "        return last_match\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# metrics\n",
    "def exact_match(row) -> bool:\n",
    "    return row[\"model_answer\"] == row[\"rhs\"]\n",
    "\n",
    "\n",
    "def bow_match(row) -> bool:\n",
    "    return sorted(row[\"model_answer\"].split()) == sorted(row[\"rhs\"].split())\n",
    "\n",
    "\n",
    "def edit_distance(row) -> float:\n",
    "    from strsimpy.jaro_winkler import JaroWinkler\n",
    "\n",
    "    jw = JaroWinkler()\n",
    "    return jw.distance(row[\"model_answer\"], row[\"rhs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res_df.drop_duplicates(subset=[\"custom_id\", \"batch_id\"])\n",
    "res_df[\"model_answer\"] = res_df[\"model_response\"].apply(extract_answer)\n",
    "res_df = res_df.dropna(subset=[\"model_answer\", \"rhs\"]).reset_index(drop=True)\n",
    "res_df[\"exact_match\"] = res_df[\"model_answer\"] == res_df[\"rhs\"]\n",
    "res_df[\"bow_match\"] = res_df.apply(\n",
    "    lambda row: sorted(row[\"rhs\"].split()) == sorted(row[\"model_answer\"].split()),\n",
    "    axis=1,\n",
    ")\n",
    "res_df[\"edit_distance\"] = res_df.apply(edit_distance, axis=1)\n",
    "\n",
    "res_df[\"lhs_length\"] = res_df[\"lhs\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "# melt exact_match and bow_match\n",
    "metrics_df = res_df.melt(\n",
    "    id_vars=[\"model\", \"lhs_length\", \"custom_id\", \"model_answer\", \"rhs\"],\n",
    "    value_vars=[\"exact_match\", \"bow_match\", \"edit_distance\"],\n",
    "    var_name=\"match_type\",\n",
    "    value_name=\"match_value\",\n",
    ")\n",
    "\n",
    "# rename `exact_match` and `bow_match` in match_type column\n",
    "metrics_df[\"match_type\"] = metrics_df[\"match_type\"].replace(\n",
    "    {\n",
    "        \"exact_match\": \"Exact Match\",\n",
    "        \"bow_match\": \"Bag of Words\",\n",
    "        \"edit_distance\": \"Edit Distance\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# convert the `model` column to an ordered categorical type\n",
    "metrics_df[\"model_name\"] = metrics_df[\"model\"].apply(lambda x: x.split(\"-2\")[0])\n",
    "metrics_df[\"model_name\"] = pd.Categorical(\n",
    "    metrics_df[\"model_name\"],\n",
    "    categories=[\"gpt-4.1-nano\", \"gpt-4.1-mini\", \"o4-mini\"],\n",
    "    ordered=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[(metrics_df[\"model_name\"] == \"o4-mini\") & (metrics_df[\"lhs_length\"] == 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_row = res_df[\n",
    "    (res_df[\"edit_distance\"] > 0.2) & (res_df[\"model\"] != \"o4-mini-2025-04-16\")\n",
    "]\n",
    "\n",
    "test_row = test_row.iloc[5][\n",
    "    [\"model_response\", \"model_answer\", \"rhs\", \"edit_distance\", \"lhs_length\"]\n",
    "]\n",
    "\n",
    "print(test_row[\"model_response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 2.5), layout=\"constrained\")\n",
    "gs = fig.add_gridspec(1, 3)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "\n",
    "for i, model in enumerate(sorted(metrics_df[\"model_name\"].unique())):\n",
    "    model_df = metrics_df[metrics_df[\"model_name\"] == model]\n",
    "    sns.lineplot(\n",
    "        data=model_df,\n",
    "        x=\"lhs_length\",\n",
    "        y=\"match_value\",\n",
    "        hue=\"match_type\",\n",
    "        ax=axes[i],\n",
    "        marker=\"o\",\n",
    "    )\n",
    "\n",
    "    axes[i].set_ylim(-0.05, 1.05)\n",
    "\n",
    "    model_name = str(model)\n",
    "\n",
    "    # format y-axis ticks as percentages\n",
    "    axes[i].set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
    "    axes[i].set_xticks([10, 20, 30, 40, 50])\n",
    "    axes[i].set_title(model_name, fontsize=10)\n",
    "    axes[i].set_ylabel(\"Score\")\n",
    "    axes[i].set_xlabel(\"Sentence Length\", ha=\"left\", x=0)\n",
    "    # axes[i].set_xscale(\"log\")\n",
    "\n",
    "    if i > 0:\n",
    "        axes[i].get_legend().remove()\n",
    "        axes[i].set_ylabel(None)\n",
    "        axes[i].set_yticks([])\n",
    "        axes[i].set_xlabel(None)\n",
    "    else:\n",
    "        axes[i].legend(title=None, loc=\"upper left\", fontsize=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of rows by lhs_length\n",
    "fig = plt.figure(figsize=(6, 2.5), layout=\"constrained\")\n",
    "gs = fig.add_gridspec(1, 3)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "axes = [ax0, ax1, ax2]\n",
    "\n",
    "for i, model in enumerate(sorted(metrics_df[\"model_name\"].unique())):\n",
    "    print(model)\n",
    "    sns.histplot(\n",
    "        data=metrics_df[metrics_df[\"model_name\"] == model],\n",
    "        x=\"lhs_length\",\n",
    "        bins=20,\n",
    "        ax=axes[i],\n",
    "        # kde=True,\n",
    "        # stat=\"density\",\n",
    "        color=\"gray\",\n",
    "    )\n",
    "\n",
    "    axes[i].set_yscale(\"log\")\n",
    "    axes[i].set_title(model, fontsize=10)\n",
    "    # axes[i].set_ylabel(\"Accuracy\")\n",
    "    axes[i].set_xlabel(\"Sentence Length\", ha=\"left\", x=0)\n",
    "\n",
    "    # horizontal line at y=1\n",
    "    axes[i].axhline(y=1, color=\"red\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    if i > 0:\n",
    "        # axes[i].get_legend().remove()\n",
    "        axes[i].set_ylabel(None)\n",
    "        axes[i].set_yticks([])\n",
    "        axes[i].set_xlabel(None)\n",
    "    else:\n",
    "        pass\n",
    "        # axes[i].legend(title=None, loc=\"upper left\", fontsize=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=res_df,\n",
    "    x=\"lhs_length\",\n",
    "    y=\"bow_match\",\n",
    "    hue=\"model\",\n",
    "    marker=\"o\",\n",
    "    ci=None,\n",
    "    palette=\"tab10\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "formal-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
