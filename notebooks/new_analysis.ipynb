{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for Prompting Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyrootutils\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as sk_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YES_RE = re.compile(r\"[^a-zA-Z]*\\b(yes|no)\\b[^a-zA-Z]*\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def extract_content(choices_list: list) -> str:\n",
    "    return choices_list[0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def extract_prediction(response: str) -> str:\n",
    "    # get a list of all matches to YES_RE in `response`; take the last match\n",
    "    # and check if it is a \"yes\" or \"no\" response\n",
    "\n",
    "    matches = YES_RE.findall(response)\n",
    "    if len(matches) == 0:\n",
    "        return \"unknown\"\n",
    "    else:\n",
    "        last_match = matches[-1]\n",
    "        if last_match.lower() == \"yes\":\n",
    "            return \"positive\"\n",
    "        else:\n",
    "            return \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = pyrootutils.find_root(\n",
    "    search_from=os.path.abspath(\"\"), indicator=\".project-root\"\n",
    ")\n",
    "\n",
    "GRAMMARS_DIR = PROJECT_ROOT / \"data\" / \"grammars\"\n",
    "\n",
    "inputs_file_pattern = \"*_inputs.jsonl\"\n",
    "results_file_pattern = \"*_results.jsonl\"\n",
    "\n",
    "batch_id_re = re.compile(r\"^(batch_\\w+)_\")\n",
    "\n",
    "# find all input files in subdirectories of GRAMMARS_DIR\n",
    "input_files = list(GRAMMARS_DIR.rglob(inputs_file_pattern))\n",
    "results_files = list(GRAMMARS_DIR.rglob(results_file_pattern))\n",
    "\n",
    "input_dfs = []\n",
    "\n",
    "inputs_dfs = []\n",
    "for f in input_files:\n",
    "    i_df = pd.read_json(f, lines=True)\n",
    "    i_json_struct = json.loads(i_df.to_json(orient=\"records\"))\n",
    "    i_flat_df = pd.json_normalize(i_json_struct)\n",
    "    batch_id = batch_id_re.search(f.name).group(1)\n",
    "    i_flat_df[\"batch_id\"] = batch_id\n",
    "    inputs_dfs.append(i_flat_df)\n",
    "inputs_df = pd.concat(inputs_dfs, ignore_index=True)\n",
    "\n",
    "results_dfs = []\n",
    "for f in results_files:\n",
    "    r_df = pd.read_json(f, lines=True)\n",
    "    r_json_struct = json.loads(r_df.to_json(orient=\"records\"))\n",
    "    r_flat_df = pd.json_normalize(r_json_struct)\n",
    "    batch_id = batch_id_re.search(f.name).group(1)\n",
    "    r_flat_df[\"batch_id\"] = batch_id\n",
    "    results_dfs.append(r_flat_df)\n",
    "results_df = pd.concat(results_dfs, ignore_index=True)\n",
    "\n",
    "# Merge inputs and results on the the batch_id and custom_id\n",
    "response_full_df = results_df.merge(\n",
    "    inputs_df[\n",
    "        [\n",
    "            \"custom_id\",\n",
    "            \"batch_id\",\n",
    "            \"body.metadata.sample_type\",  # ground-truth label for sample\n",
    "            \"body.metadata.sample\",  # the sample itself\n",
    "            \"body.metadata.grammar_file\",  # grammar file used\n",
    "            \"body.metadata.model\",  # model used\n",
    "            \"body.metadata.n_shots\",  # n_shots used\n",
    "        ]\n",
    "    ],\n",
    "    on=[\"batch_id\", \"custom_id\"],\n",
    ")\n",
    "\n",
    "response_full_df = response_full_df.rename(\n",
    "    columns={\n",
    "        \"body.metadata.sample_type\": \"sample.type.ground_truth\",\n",
    "        \"body.metadata.sample\": \"sample\",\n",
    "        \"body.metadata.grammar_file\": \"grammar_file\",\n",
    "        \"body.metadata.model\": \"model\",\n",
    "        \"body.metadata.n_shots\": \"n_shots\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response_full_df = response_full_df.rename(\n",
    "    columns={\n",
    "        \"body.metadata.sample_type\": \"sample.type.ground_truth\",\n",
    "        \"body.metadata.sample\": \"sample\",\n",
    "        \"body.metadata.grammar_file\": \"grammar_file\",\n",
    "        \"body.metadata.model\": \"model\",\n",
    "        \"body.metadata.n_shots\": \"n_shots\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response_full_df[\"model_response\"] = response_full_df[\"response.body.choices\"].apply(\n",
    "    extract_content\n",
    ")\n",
    "\n",
    "response_df = response_full_df[\n",
    "    [\n",
    "        \"sample\",\n",
    "        \"sample.type.ground_truth\",\n",
    "        \"model_response\",\n",
    "        \"grammar_file\",\n",
    "        \"model\",\n",
    "        \"n_shots\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "\n",
    "response_df[\"sample.type.predicted\"] = response_df[\"model_response\"].apply(\n",
    "    extract_prediction\n",
    ")\n",
    "\n",
    "response_df[\"sample.length\"] = response_df[\"sample\"].apply(\n",
    "    lambda s: len(str(s).split(\" \"))\n",
    ")\n",
    "\n",
    "response_df[\"correct\"] = (\n",
    "    response_df[\"sample.type.ground_truth\"] == response_df[\"sample.type.predicted\"]\n",
    ")\n",
    "\n",
    "response_df = response_df.dropna()\n",
    "\n",
    "response_df[\"n_shots\"] = pd.Categorical(\n",
    "    response_df[\"n_shots\"],\n",
    "    categories=[\"0\", \"2\", \"4\", \"8\", \"16\", \"32\"],\n",
    "    ordered=True,\n",
    ")\n",
    "response_df[\"sample.type.ground_truth\"] = pd.Categorical(\n",
    "    response_df[\"sample.type.ground_truth\"],\n",
    "    categories=[\"positive\", \"negative\"],\n",
    "    ordered=True,\n",
    ")\n",
    "response_df[\"sample.type.predicted\"] = pd.Categorical(\n",
    "    response_df[\"sample.type.predicted\"],\n",
    "    categories=[\"positive\", \"negative\", \"unknown\"],\n",
    "    ordered=True,\n",
    ")\n",
    "response_df[\"model\"] = pd.Categorical(\n",
    "    response_df[\"model\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy by Model, Sample Type, and # of Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(\n",
    "    data=response_df,\n",
    "    x=\"n_shots\",\n",
    "    y=\"correct\",\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    palette={\"positive\": \"orange\", \"negative\": \"blue\"},\n",
    "    style=\"model\",\n",
    "    markers=True,\n",
    "    alpha=0.35,\n",
    "    err_kws={\"alpha\": 0.15},\n",
    "    markersize=8,\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=response_df,\n",
    "    x=\"n_shots\",\n",
    "    y=\"correct\",\n",
    "    style=\"model\",\n",
    "    color=\"black\",\n",
    "    linewidth=2,\n",
    "    markers=True,\n",
    "    ax=ax,\n",
    "    legend=False,\n",
    "    markersize=8,\n",
    ")\n",
    "\n",
    "_ = ax.set_ylabel(\"Mean Accuracy\")\n",
    "_ = ax.set_xlabel(\"# of Shots  [log scale]\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "new_labels = [\"Positive samples\", \"Negative samples\", None, \"gpt-4o-mini\", \"o3-mini\"]\n",
    "for h in handles[1:]:\n",
    "    h.set_alpha(1)\n",
    "_ = ax.legend(handles[1:], new_labels)\n",
    "\n",
    "_ = sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in handles:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of Sample Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "sns.histplot(\n",
    "    data=response_df,\n",
    "    x=\"sample.length\",\n",
    "    ax=ax,\n",
    "    binwidth=1,\n",
    "    hue=\"sample.type.ground_truth\",\n",
    "    palette={\"positive\": \"orange\", \"negative\": \"purple\"},\n",
    ")\n",
    "\n",
    "_ = ax.get_legend().set_title(\"Sample type\")\n",
    "\n",
    "_ = ax.set_yscale(\"log\")\n",
    "_ = ax.set_xlabel(\"Sample length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
